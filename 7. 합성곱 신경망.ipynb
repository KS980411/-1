{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec41599d",
   "metadata": {},
   "source": [
    "# 합성곱 신경망\n",
    "\n",
    "이번 장의 주제는 합성곱 신경망이다. CNN은 이미지 인식과 음성 인식 등 다양한 곳에서 사용한다.\n",
    "\n",
    "특히, 이미지 인식 분야에서 딥러닝을 활용한 기법은 거의 다 CNN을 기초로 한다. 이번 장에서는 CNN의 메커니즘을 자세히 설명하고 이를 파이썬으로 구현해볼 것이다.\n",
    "\n",
    "## 전체 구조\n",
    "\n",
    "우선 CNN의 네트워크 구조를 살펴보며 전체 틀을 이해해보기로 하자.\n",
    "\n",
    "CNN도 지금까지 본 신경망과 같이 **레고 블록처럼 계층을 조합**하여 만들 수 있다.\n",
    "\n",
    "다만, **합성곱 계층**과 **풀링 계층**이 새롭게 등장한다. 이에 대한 상세 내용은 나중에 설명할 것이다.\n",
    "\n",
    "여기에서는 이 계층들을 어떻게 조합하여 CNN을 만드는지를 먼저 볼 것이다.\n",
    "\n",
    "지금까지 본 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있었다. 이를 **완전연결**이라고 하며, 완전히 연결된 계층을 Affine 계층이라는 이름으로 구현했다. \n",
    "\n",
    "Affine 계층을 사용하면, 가령 층이 5개인 완전연결 신경망은 [그림 7-1]과 같이 구현할 수 있다.\n",
    "\n",
    "[229페이지 그림 7-1 참고]\n",
    "\n",
    "[그림 7-1]과 같이 완전연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층(혹은 Sigmoid 계층)이 이어진다.\n",
    "\n",
    "이 그림에서는 Affine-ReLU 조합이 4개가 쌓였고, 마지막 5번째 층은 Affine 계층에 이어 Softmax 계층에서 최종 결과(확률)을 출력한다.\n",
    "\n",
    "그럼 CNN의 구조는 어떻게 다른지 한 번 보자.\n",
    "\n",
    "[229페이지 그림 7-2 참고]\n",
    "\n",
    "CNN에서는 새로운 **'합성곱 계층'과 '풀링 계층'이 추가**된다.\n",
    "\n",
    "CNN에서는 'Conv - ReLU - (Pooling)' 흐름으로 연결된다(풀링 계층은 생략하기도 한다). 지금까지의 'Affine-ReLU' 연결이 'Conv-ReLU-(Pooling)'으로 바뀌었다고 생각할 수 있다.\n",
    "\n",
    "CNN에서 주목할 또 다른 점은 **출력에 가까운 층**에서는 지금까지의 'Affine-ReLU' 구성을 사용할 수 있다는 것이다. 또, 마지막 출력 계층에서는 'Affine-Softmax' 조합을 그대로 사용한다. 이상은 일반적인 CNN에서 흔히 볼 수 있는 구성이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a59de2c",
   "metadata": {},
   "source": [
    "## 합성곱 계층\n",
    "\n",
    "CNN에서는 패딩, 스트라이드 등 고유의 용어가 등장한다.\n",
    "\n",
    "또 각 계층 사이에서는 3차원 데이터같이 **입체적인 데이터가 흐른다**는 점에서 완전연결 신경망과 다르다.\n",
    "\n",
    "우선 이번 절에서는 CNN에서 사용하는 합성곱 계층의 구조를 차분히 살펴보도록 하자.\n",
    "\n",
    "\n",
    "### 완전연결 계층의 문제점\n",
    "\n",
    "지금까지 본 완전연결 신경망에는 완전연결 계층인 Affine 계층을 사용했다. 완전연결계층에서는 인접하는 계층의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있다.\n",
    "\n",
    "완전연결 계층의 문제점은 바로 **데이터의 형상이 무시**된다는 것이다. 입력 데이터가 이미지라고 하면, 이미지는 통상 세로, 가로, 채널(색상)으로 구성된 3차원 데이터이다. 그러나 완전연결 계층에 입력할 때에는 3차원 데이터를 **평평한 1차원 데이터로 평탄화해주어야 한다.** 지금까지 MNIST 데이터셋을 사용한 사례에서는 형상이 $(1,28,28)$인 이미지를 1줄로 세운 784개의 데이터를 첫 Affine 계층에 입력했다.\n",
    "\n",
    "이미지는 3차원 형상이며, 이 형상에는 공간적 정보가 담겨 있다.\n",
    "\n",
    "예를 들어 공간적으로 가까운 픽셀은 값이 비슷하거나, RGB의 각 채널은 서로 밀접하게 관련되어 있거나, 거리가 먼 픽셀끼리는 별 연관이 없는 등, **3차원 속에서 의미를 갖는 본질적인 패턴**이 숨어 있을 것이다. 그러나 완전연결 계층은 형상을 무시하고 모든 입력 데이터를 동등한 뉴런(같은 차원의 뉴런)으로 취급하여 형상에 담긴 정보를 살릴 수 없다.\n",
    "\n",
    "한편, 합성곱 계층은 **형상을 유지**한다. 이미지도 3차원 데이터로 입력받으며, 마찬가지로 다음 계층에도 3차원 데이터로 전달한다. 그래서 CNN에서는 이미지처럼 형상을 가진 데이터를 제대로 이해할 (가능성이 있는) 것이다.\n",
    "\n",
    "CNN에서는 합성곱 계층의 입출력 데이터를 **특징 맵**이라고도 한다. 합성곱 계층의 입력 데이터를 **입력 특징 맵**, 출력 데이터를 **출력 특징 맵**이라고 하는 식. 이 책에서는 '입출력 데이터'와 '특징 맵'을 같은 의미로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbbf0a",
   "metadata": {},
   "source": [
    "### 합성곱 연산\n",
    "\n",
    "합성곱 계층에서의 **합성곱 연산**을 처리한다. 합성곱 연산은 이미지 처리에서 말하는 **필터 연산**에 해당한다.\n",
    "\n",
    "<font color = blue> [231페이지 그림 7-3 참고] </font>\n",
    "\n",
    "합성곱 연산은 입력 데이터에 필터를 적용한다.\n",
    "\n",
    "위 그림의 예에서 입력 데이터는 세로 X 가로의 형상을 가졌고, 필터 역시 세로 X 가로의 차원을 갖는다. 데이터와 필터의 형상을 (높이, 너비)로 표현하며, 이 예에서는 입력은 $(4,4)$, 필터는 $(3,3)$, 출력은 $(2,2)$가 된다.\n",
    "\n",
    "합성곱 연산은 필터의 **윈도우**를 일정 간격으로 이동해가며 입력 데이터에 적용한다. 여기서 말하는 윈도우는 [그림 7-4]의 회색 3X3을 가리킨다. 이 그림에서 보듯, 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 구한다(이 계산을 **단일 곱셈-누산**이라고 한다). 그리고 그 출력을 해당 장소에 저장한다. 이 과정을 모든 장소에서 수행하면 합성곱 연산의 출력이 완성된다.\n",
    "\n",
    "<font color = blue> [232페이지 그림 7-4 참고] **중요!!!** </font>\n",
    "\n",
    "완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 **필터의 매개변수**가 그동안의 가중치에 해당한다. 그리고 CNN에서도 **편향이 존재**하는데, 편향까지 보여주면 [그림 7-5]와 같은 흐름이 된다.\n",
    "\n",
    "<font color = blue> [233페이지 그림 7-5 참고] **중요!!!** </font>\n",
    "\n",
    "위 그림과 같이 편향은 **필터를 적용한 후의 데이터에 더해진다**. 그리고 편향은 항상 하나(1X1)만 존재한다. 그 하나의 값을 필터를 적용한 모든 원소에 더하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90b1f06",
   "metadata": {},
   "source": [
    "### 패딩\n",
    "\n",
    "합성곱 연산을 수행하기 전에 입력 데이터 주변을 **특정 값(예컨대 0)** 으로 채우기도 한다. 이를 **패딩**이라고 하며, 합성곱 연산에서 자주 이용하는 기법이다. 예를 들어 [그림 7-6]은 $(4,4)$ 크기의 입력 데이터에 폭이 1인 패딩을 적용한 모습이다. 폭 1짜리 패딩이라 하면 **입력 데이터 사방 1픽셀을 특정 값으로 채우는 것** 이다.\n",
    "\n",
    "<font color = blue> [233페이지 그림 7-6 참고] </font>\n",
    "\n",
    "    합성곱 연산의 패딩 처리 : 입력 데이터 주위에 0을 채운다. 패딩은 점선으로 표시했으며 그 안의 값 '0'을 생략했다.\n",
    "    \n",
    "[그림 7-6]과 같이 처음에 크기가 $(4,4)$인 입력 데이터에 패딩이 추가되어 $(6,6)$이 된다.\n",
    "\n",
    "이 입력에 $(3,3)$ 크기의 필터를 걸면 $(4,4)$ 크기의 출력 데이터가 생성된다.\n",
    "\n",
    "이 예에서는 패딩을 1로 설정했지만, 2와 3 등 원하는 정수로 설정할 수 있다. 만약 [그림 7-5]에 패딩을 2로 설정하면 입력 데이터의 크기는 $(8,8)$이 되고, 3으로 설정하면 $(10, 10)$이 된다. (<font color = purple> 가로와 세로가 모두 늘어나므로! </font>)\n",
    "\n",
    "<font color = blue> *NOTE. 패딩은 주로 출력 크기를 조정할 목적으로 사용한다. 예를 들어 $(4,4)$ 입력 데이터에 $(3,3)$ 필터를 적용하면 출력은 $(2,2)$가 되어, 입력보다 2만큼 줄어든다. 이는 합성곱 연산을 몇 번이나 되풀이하는 심층 신경망에서는 문제가 될 수 있다. 합성곱 연산을 거칠 때마다 크기가 작아지면 어느 시점에서는 출력 크기가 1이 되어버린다. 더 이상 합성곱 연산을 적용할 수 없다는 것이다. 이러한 사태를 막기 위해 패딩을 사용한다. 앞의 예에서는 패딩의 폭을 1으로 설정해 입력에 대한 출력이 같은 크기인 $(4,4)$로 유지되었다. 입력 데이터의 공간적 크기를 고정한 채로 다음 계층에 전달할 수 있다는 것이다.* </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb599ef1",
   "metadata": {},
   "source": [
    "### 스트라이드\n",
    "\n",
    "필터가 적용하는 위치의 간격을 **스트라이드**라고 한다. 지금까지 본 예는 모두 스트라이드가 1이었지만, 예를 들어 스트라이드를 2로 하면 **필터를 적용하는 윈도우가 두 칸씩 이동** 한다.\n",
    "\n",
    "<font color = blue> [234페이지 그림 7-7 참고] </font>\n",
    "\n",
    "이철머 스트라이드를 키우면 **출력 크기는 작아진다**. 한편, 패딩을 크게 하면 **출력 크기가 커진다**. 이러한 관계를 수식화라면 어떻게 될까? 이어서 패딩, 스트라이드, 출력 크기를 어떻게 계산하는지 살펴보자.\n",
    "\n",
    "입력 크기를 $(H, W)$, 필터 크기를 $(FH, FW)$, 출력 크기를 $(OH, OW)$, 패딩을 $P$, 스트라이드를 $S$라 한다면, 출력 크기는 다음과 같은 식으로 계산한다.\n",
    "\n",
    "$$OH = \\frac{H + 2P - FH}{S} + 1 \\\\\n",
    "OW = \\frac{W + 2P - FW}{S} + 1$$\n",
    "\n",
    "이 식을 사용하여 연습을 해보자.\n",
    "\n",
    "예 1. [그림 7-6]의 예\n",
    "    입력 (4,4), 패딩 : 1, 스트라이드 : 1, 필터 : (3,3)\n",
    "    \n",
    "$$OH = \\frac{4 + 2 \\cdot 1 - 3}{1} + 1 = 4 \\\\\n",
    "OW = \\frac{4 + 2 \\cdot 1 - 3}{1} + 1 = 4$$\n",
    "\n",
    "\n",
    "예 2. [그림 7-7]의 예\n",
    "    입력 (7,7), 패딩 : 0, 스트라이드 : 2, 필터 : (3,3)\n",
    "    \n",
    "$$OH = \\frac{7 + 2 \\cdot 0 - 3}{2} + 1 = 3 \\\\\n",
    "OW = \\frac{7 + 2 \\cdot 0 - 3}{2} + 1 = 3$$\n",
    "\n",
    "예 3.\n",
    "    입력 : (28, 31), 패딩 : 2, 스트라이드 : 3, 필터 (5,5)\n",
    "\n",
    "$$OH = \\frac{28 + 2 \\cdot 2 - 5}{3} + 1 = 10 \\\\\n",
    "\\frac{31 + 2 \\cdot 2 - 5}{3} + 1 = 11$$\n",
    "\n",
    "이상의 예처럼 위의 식에 단순히 값을 대입하기만 하면 **출력 크기를 구할 수 있다**.\n",
    "\n",
    "단, $\\frac{W + 2P - FW}{S}, \\frac{H + 2P - FH}{S}$가 **정수로 나눠떨어지는 값이어야 한다는 점**에 주의하자.\n",
    "\n",
    "딥러닝 프레임워크 중에는 딱 나눠떨어지지 않을 때는 가장 가까운 정수롤 반올림 하는 등, 특별히 에러를 내지 않고 진행하도록 구현하는 경우도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9ecbc",
   "metadata": {},
   "source": [
    "### 3차원 데이터의 합성곱 연산\n",
    "\n",
    "지금까지 2차원 형상을 다루는 합성곱 연산을 살펴보았다.\n",
    "\n",
    "그러나 이미지만 해도 세로, 가로, 채널까지 고려한 3차원 데이터이다. 이번 절에는 조금 전과 같은 순서로 체널까지 고려한 3차원 데이터를 다루는 합성곱 연산을 살펴볼 것이다.\n",
    "\n",
    "[그림 7-8]은 3차원 데이터의 합성곱 연산 예이고, [그림 7-9]는 계산 순서이다. 2차원일 때와 비교하면, 길이 방향(채널 방향)으로 특징 맵이 늘어났다. 채널 쪽으로 특징 맵이 여러 개 있다면, 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고, 그 결과를 더해서 하나의 출력을 얻는다.\n",
    "\n",
    "<font color = blue> [236~237페이지 그림 7-8, 7-9 참고] **중요!!** </font>\n",
    "\n",
    "3차원의 합성곱 연산에서 주의할 점은 **입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다**는 것이다. 이 예에서는 모두 3개로 일치한다.\n",
    "\n",
    "필터 자체의 크기는 원하는 값으로 설정할 수 있는데, 모든 채널의 필터가 같은 크기여야 한다.\n",
    "\n",
    "이 예에서는 필터의 크기가 $(3,3)$이지만 원한다면 $(2,2), (1,1), (5,5)$ 등으로 설정해도 무방하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60e9fa",
   "metadata": {},
   "source": [
    "### 블록으로 생각하기\n",
    "\n",
    "3차원의 합성곱 연산을 데이터와 필터를 직육면체 블록이라고 생각하면 쉽다.\n",
    "\n",
    "블록은 [그림 7-10]과 같은 3차원 직육면체이다. 또, 3차원 데이터를 다차원 배열로 나타낼 대에는 (채널, 높이, 너비) 순서로 쓸 것이다. 필터도 같은 순서이다.\n",
    "\n",
    "<font color = blue> [238페이지 그림 7-10 참고] </font>\n",
    "\n",
    "이 예에서 출력 데이터는 한 장의 특징 맵이다. 한 장의 특징 맵을 다른 말로 하면 **채널이 1개인 특징 맵**이다.\n",
    "\n",
    "그럼 합성곱 연산의 출력으로 다수의 채널을 내보내려면 어떻게 해야 할가? 그 답은 필터(가중치)를 다수 사용하는 것이다. 그림으로는 [그림 7-11]처럼 된다.\n",
    "\n",
    "<font color = blue> [239페이지 그림 7-11 참고] </font>\n",
    "\n",
    "이 그림과 같이 필터를 $FN$개 적용하면 출력 맵도 $FN$개가 생성된다. 그리고 그 $FN$개의 맵을 모으면 형상이 $(FN, OH, OW)$인 블록이 완성된다. 이 완성된 블록을 다음 계층으로 넘기는 것이 CNN의 처리 흐름이다.\n",
    "\n",
    "이와 같이 합성곱 연산에서는 **필터의 수도 고려**해야 한다. 그런 이유로 필터의 가중치 데이터는 4차원 데이터이며 (출력 채널 수, 입력 채널 수, 높이, 너비) 순으로 쓴다. 예를 들어 채널 수 3, 크기 5X5인 필터가 20개 있다면 $(20, 3, 5, 5)$로 쓴다.\n",
    "\n",
    "합성곱 연산에도 편향이 쓰인다. [그림 7-12]은 [그림 7-11]에 편향을 더한 모습이다.\n",
    "\n",
    "<font color = blue> [239페이지 그림 7-12 참고] </font>\n",
    "\n",
    "======\n",
    "\n",
    "$(C, H, W) * (FN, C, FH, FW) \\rightarrow (FN, OH, OW) + (FN, 1, 1) \\rightarrow (FN, OH, OW)$\n",
    "\n",
    "$*$은 합성곱 연산\n",
    "\n",
    "======\n",
    "\n",
    "위 그림에서 보듯 편향은 채널 하나에 값 하나씩으로 구성된다. 이 예에서는 편향의 형상은 $(FN, 1, 1)$이고, 필터의 출력 결과의 형상은 $(FN, OH, OW)$이다. 이 두 블록을 더하면 편향의 각 값이 필터의 출력인 $(FN, OH, OW)$ 블록의 대응 채널의 **원소 모두에 더해진다.**\n",
    "\n",
    "형상이 다른 블록의 덧셈은 넘파이의 브로드캐스트 기능으로 쉽게 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef29a64",
   "metadata": {},
   "source": [
    "### 배치 처리\n",
    "\n",
    "신경망 처리에서는 입력 데이터를 한 덩어리로 묶어 배치로 처리했다. 완전연결 신경망을 구현하면서는 이 방식을 지원하여 처리 효율을 높이고, 미니배치 방식의 학습도 지원하도록 했다.\n",
    "\n",
    "합성곱 연산도 마찬가지로 배치 처리를 지원하고자 한다. 그래서 각 계층을 흐르는 데이터의 차원을 하나 늘려 **4차원 데이터로 저장**한다. 구체적으로는 데이터를 (데이터 수, 채널 수, 높이, 너비) 순으로 저장한다. 데이터가 N개 일 때, [그림 7-12]를 배치 처리한다면 데이터 형태가 [그림 7-13]처럼 된다.\n",
    "\n",
    "=======\n",
    "\n",
    "$(N, C, H, W) * (FN, C, FH, FW) \\rightarrow (N, FN, OH, OW) + (FN, 1, 1) \\rightarrow (N, FN, OH, OW)$\n",
    "\n",
    "$*$은 합성곱 연산, $(FN, 1, 1)$은 편향\n",
    "\n",
    "======\n",
    "\n",
    "배치 처리 시의 데이터 흐름을 나타낸 위 그림을 보면 각 데이터의 선두에 배치용 차원을 추가했다.\n",
    "\n",
    "이처럼 데이터는 4차원 형상을 가진 채 각 계층을 타고 흐른다. 여기에서 주의할 점으로는 신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 이뤄진다는 것이다. 즉, N회 분의 처리를 한 번에 수행하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c1d74",
   "metadata": {},
   "source": [
    "## 풀링 계층\n",
    "\n",
    "풀링은 세로, 가로 방향의 공간을 줄이는 연산이다. 예를 들어 [그림 7-14]와 같이 2X2 영역을 원소 하나로 집약하여 공간 크기를 줄인다.\n",
    "\n",
    "<font color = blue> [241페이지 그림 7-14 참고] </font>\n",
    "\n",
    "[그림 7-14]는 2X2 **최대 풀링**을 스트라이드 2로 처리하는 순서이다. 최대 풀링은 **최댓값을 구하는 연산**으로, '2X2'는 대상 영역의 크기를 말한다.\n",
    "\n",
    "따라서 2X2 최대 풀링은 **2X2 크기의 영역에서 가장 큰 원소 하나를 꺼내는 것** 이다.\n",
    "\n",
    "또, 스트라이드는 이 예에서는 2로 설정했으므로, 2X2 윈도우가 **원소 2칸 간격으로 이동**한다. 이 풀링의 윈도우 크기와 스트라이드는 **같은 값으로 설정** 하는 것이 보통이다. 예를 들어 윈도우 3X3이면 스트라이드는 3으로, 윈도우 4X4이면 스트라이드를 4로 설정한다.\n",
    "\n",
    "풀링은 최대 풀링 외에도 **평균 풀링**이 있다. 최대 풀링은 대상 영역에서 최댓값을 취하는 연산인 반면, 평균 풀링은 대상 영역의 평균을 계산한다. 이미지 인식 분야에서는 주로 최대 풀링을 사용한다.\n",
    "\n",
    "그러므로 여기에서 풀링 계층은 최대 풀링으로 통일하겠다.\n",
    "\n",
    "### 풀링 계층의 특징\n",
    "\n",
    "풀링 계층의 특징을 정리해보자.\n",
    "\n",
    "<font color = gray> **학습해야 할 매개변수가 없다** </font>\n",
    "    풀링 계층은 합성곱 계층과 달리 학습해야 할 매개변수가 없다. 풀링은 대상 영역에서 최댓값이나 평균을 취하는 명확한 처리이므로 특별히 학습할 것이 없다.\n",
    "\n",
    "<font color = gray> **채널 수가 변하지 않는다** </font>\n",
    "    풀링 연산은 입력 데이터의 채널 수 그대로 출력 데이터로 내보낸다. [그림 7-15]처럼 채널마다 독립적으로 계산한다.\n",
    "    \n",
    "<font color = blue> [242페이지 그림 7-15 참고] </font>\n",
    "\n",
    "<font color = gray> **입력의 변화에 영향을 적게 받는다(강건하다)** </font>\n",
    "    입력 데이터가 조금 변해도 풀링의 결과는 잘 변하지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0932b3",
   "metadata": {},
   "source": [
    "## 합성곱/풀링 계층 구현하기\n",
    "\n",
    "지금까지 합성곱 계층과 풀링 계층에 대해 자세히 설명했다. 이번 절에서는 이 두 계층을 파이썬으로 구현해볼 것이다.\n",
    "\n",
    "합성곱 계층과 풀링 계층은 복잡해 보이지만, 트릭을 사용하면 쉽게 구현할 수 있다.\n",
    "\n",
    "이번 절에서는 그 트릭을 활용해 문제를 간단히 하면서 합서옥ㅂ 계층을 구현해볼 것이다.\n",
    "\n",
    "### 4차원 배열\n",
    "\n",
    "CNN에서 계층 사이를 흐르는 데이터는 4차원이다.\n",
    "\n",
    "예를 들어, 데이터의 형상이 $(10, 1, 28, 28)$이라면, 이는 높이 28, 너비 28, 채널 1개인 데이터가 10개라는 이야기이다.\n",
    "\n",
    "이를 파이썬으로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8034a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(10, 1, 28, 28) # 무작위로 데이터 생성\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d3b2b",
   "metadata": {},
   "source": [
    "여기에서 10개 중 첫 번째 데이터에 접근하려면 단순히 $x[0]$이라고 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c6cb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7843026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621aee4a",
   "metadata": {},
   "source": [
    "또, 첫 번재 데이터의 첫 채널의 공간 데이터에 접근하려면 다음과 같이 적는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8d8ec02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "xp = x[0, 0] # 또는 x[0][0]\n",
    "xp.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ebc445",
   "metadata": {},
   "source": [
    "이처럼 CNN은 4차원 데이터를 다룬다. 그래서 합성곱 연산의 구현은 복잡해질 것 같지만, 다음 절에서 설명하는 im2col이라는 트릭이 문제를 단순하게 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258eaabc",
   "metadata": {},
   "source": [
    "### im2col로 데이터 전개하기\n",
    "\n",
    "합성곱 연산을 곧이곧대로 구현하려면 for 문을 겹겹이 써야할 것이다. \n",
    "\n",
    "하지만 넘파이에 for 문을 사용하면 성능이 떨어진다는 단점이 있다(넘파이에서는 원소에 접근할 때 for 문을 사용하지 않는 것이 바람직하다).\n",
    "\n",
    "이번 절에서는 for 문 대신 im2col이라는 편의 함수를 사용해 간단하게 구현해 볼 것이다.\n",
    "\n",
    "im2col은 입력 데이터를 필터링(가중치 계산)하게 좋게 전개하는(펼치는) 함수이다. \n",
    "\n",
    "[그림 7-17]과 같이 3차원 입력 데이터에 im2col을 적용하면 2차원 행렬로 바뀐다(정확히는 배치 안의 데이터 수까지 포함한 4차원 데이터를 2차원으로 변환한다). 즉, im2col은 필터 적용 영역을 앞에서 순서대로 1줄로 펼친다.\n",
    "\n",
    "im2col은 필터링하게 좋게 입력 데이터를 전개한다. 구체적으로는 [그림 7-18]과 같이 입력 데이터에서 **필터를 적용하는 영역(3차원 블록)을 한 줄로 늘어놓는다**. 이 전개를 필터를 적용하는 모든 영역에서 수행하는 것이 im2col이다.\n",
    "\n",
    "<font color = blue> [244페이지 그림 7-17, 그림 7-18 참고] </font>\n",
    "\n",
    "[그림 7-18]에서는 보기 좋게끔 스트라이드를 크게 잡아 필터의 적용 영역이 겹치지 않도록 했지만, 실제 상황에서는 영역이 겹치는 경우가 대부분이다.\n",
    "\n",
    "필터 적용 영역이 겹치게 되면 im2col로 전개한 후의 원소 수가 원래 블록의 원소 수보다 많아진다. 그래서 im2col을 사용해 구현하면 메모리르 더 많이 소비하는 단점이 있다.\n",
    "\n",
    "<font color = purple> 겹치는 원소를 중복해서 쓸수밖에 없으니, 블록의 원소 수보다 전개 후 원소 수가 많아질 수밖에 없다. </font>\n",
    "\n",
    "하지만 컴퓨터는 큰 행렬을 묶어서 계산하는 데 탁월하다.\n",
    "\n",
    "예를 들어 행렬 계산 라이브러리 등은 행렬 계산에 고도로 최적화되어 큰 행렬의 곱셈을 빠르게 꼐산할 수 있다. 그래서 문제를 행렬 계산으로 만들면 선형 대수 라이브러리를 활용해 효율을 높일 수 있다.\n",
    "\n",
    "<font color = blue> *NOTE. im2col은 'image to column', 즉 이미지에서 행렬로라는 뜻이다. Caffe나 Chainer 등의 딥러닝 프레임워크는 im2col이라는 이름의 함수를 만들어 합성곱 계층을 구현할 때 이용하고 있다.* </font>\n",
    "\n",
    "im2col로 입력 데이터를 전개한 다음에는 **합성곱 계층의 필터(가중치)를 1열로 전개**하고, 두 행렬의 곱을 계산하면 된다.\n",
    "\n",
    "이는 완전연결 계층의 Affine 계층에서 한 것과 거의 같다.\n",
    "\n",
    "<font color = blue> [245페이지 그림 7-19 참고] </font>\n",
    "\n",
    "[그림 7-19]와 같이 im2col 방식으로 출력한 결과는 2차원 행렬이다. CNN은 데이터를 4차원 배열로 저장하므로 출력 데이터를 4차원으로 변형한다. 이상이 합성곱 계층의 구현 흐름이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064d251",
   "metadata": {},
   "source": [
    "### 합성곱 계층 구현하기\n",
    "\n",
    "이 책에서는 im2col 함수를 미리 만들어 제공한다.\n",
    "\n",
    "    im2col 함수는 간단한 함수 10개 정도를 묶은 것이니, 궁금하면 common/util.py를 참고하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f611640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir(\"/Users/choeunsol/deep-learning-from-scratch-master/ch07\")\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac46e05",
   "metadata": {},
   "source": [
    "    im2col(input_data, filter_h, filter_w, stride = 1, pad = 0)\n",
    "    \n",
    "    - input_data : (데이터 수, 채널 수, 높이, 너비)의 4차원 배열로 이뤄진 입력 데이터\n",
    "    - filter_h : 필터의 높이\n",
    "    - filter_w : 필터의 너비\n",
    "    - stride : 스트라이드\n",
    "    - pad : 패딩\n",
    "    \n",
    "이 im2col은 '필터 크기', '스트라이드', '패딩'을 고려하여 입력 데이터를 2차원 배열로 전개한다.\n",
    "\n",
    "그렇다면 이 im2col을 실제로 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13ad0874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7) # (데이터 수, 채널, 높이, 너비)\n",
    "col1 = im2col(x1, 5, 5, stride = 1, pad = 0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc0c56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7) # 데이터 10개\n",
    "col2 = im2col(x2, 5, 5, stride = 1, pad = 0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbeea0",
   "metadata": {},
   "source": [
    "여기에서는 두 가지 예를 보여주고 있다.\n",
    "\n",
    "첫 번째는 배치 크기가 1(데이터 1개), 채널은 3개, 높이와 너비가 각각 7X7인 데이터이고,\n",
    "\n",
    "두 번째는 배치 크기만 10이고 나머지는 첫 번째와 같다.\n",
    "\n",
    "im2col 함수를 적용한 두 경우 모두 2번째 차원의 원소는 75개이다. 이 값은 필터의 원소 개수와 같다(채널 3개, 5X5데이터).\n",
    "\n",
    "또한, 배치 크기가 1일 때는 im2col의 결과의 크기가 $(9,75)$이고, 10일 때는 그 10배인 $(90, 75)$ 크기의 데이터가 저장된다.\n",
    "\n",
    "<font color = purple> 필터의 크기가 $(5,5)$이고 출력 특징 맵의 크기가 $(7,7)$이기 때문에, 한 채널 당 윈도우가 9번 이동한다(가로 3번, 세로 9번). 이 필터에 따른 합성곱에 필요한 요소들을 펼쳐놓은 것이 im2col의 열 개수인 75개. 따라서 5X5X3의 차원을 가지게 된다. </font>\n",
    "\n",
    "이제 이 im2col을 사용하여 합성곱 계층을 구현해보자. 여기에서는 합성곱 계층을 Convolution이라는 클래스로 구현할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3f1e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride = 1, pad = 0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape # 필터의 채널과 입력 데이터의 채널 수를 맞춰야 함에 주의!!\n",
    "        out_h = int(1+(H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride) \n",
    "        ## 2차원일 때의 출력 h, w을 계산하는 방법을 기억하자.\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) \n",
    "        ## im2col은 입력값을 필터에 맞게 전개한다.\n",
    "        col_W = self.W.reshape(FN, -1).T \n",
    "        ## 하나의 필터 당 계산을 해야하므로 필터의 개수만큼을 지정해놓고 펼쳐준다. \n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035b626",
   "metadata": {},
   "source": [
    "    합성곱 계층은 필터(가중치), 편향, 스트라이드, 패딩을 인수로 받아 초기화된다. 필터는 (FN, C, FH, FW)의 4차원 형상.\n",
    "    \n",
    "    col = im2col(...) ~ out = np.dot(...) + b 부분에서 입력 데이터를 im2col로 전개하고 필터도 reshape를 통해 2차원 배열로 전개한다. col과의 곱을 위해 행렬을 transpose 해준다. \n",
    "    \n",
    "    reshape(..., -1) 옵션은 행의 개수가 정해지고 데이터가 빠짐없이 들어가도록 열의 개수를 자동으로 정해준다. 필터의 개수만큼 행을 정의했으므로 한 필터의 해당 요소들을 열로 정의한다. 앞에서 정의한 im2col로 펼친 x의 2차원 배열과 호응하도록 한다. x의 단위가 필터 하나에 곱하는 원소들을 하나의 열로 정리했으므로, col_W도 마찬가지로 필터 하나를 모두 한 열에 집어넣는다.\n",
    "    \n",
    "    앞의 코드에서 (10, 3, 5, 5) 형상을 한 다차원 배열 W의 원소 개수는 총 750개이다. 이 배열에 reshape(10, -1)을 호출하면 750개의 원소를 10묶음인 (10, 75)인 배열로 만들어준다.\n",
    "    \n",
    "    forward 구현의 마지막에서는 출력 데이터를 적절한 형상으로 바꿔준다. 이 때 넘파이의 transpose 함수를 사용하는데, 이는 다차원 배열의 축 순서를 바꿔주는 함수이다. 즉, 원래 행렬이 (0,1,2,3)이라면 transpose(0,3,1,2)는 배열 순서를 인덱스 순서대로 바꿔준다.\n",
    "    \n",
    "<font color = blue> [248페이지 그림 7-20 참고] </font>\n",
    "\n",
    "이상이 합성곱 계층의 forward 구현이다. im2col로 전개한 덕분에 완전연결 계층의 Affine 계층과 거의 똑같이 구현할 수 있었다.\n",
    "\n",
    "합성곱 계층의 역전파는 Affine 계층의 구현과 유사하다. 주의할 것은, 합성곱 계층의 역전파에서는 im2col을 역으로 처리해야 한다. 이는 이 책에서 제공하는 col2im 함수를 사용하면 된다(col2im의 구현은 common/util.py에 있다). col2im을 사용한다는 점만 제외하면 합성곱 계층의 역전파는 Affine 계층과 똑같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f077af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.util import col2im\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        ## 정방향에서의 4차원 배열을 만들기 위해 결과물을 transpose해준다.\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffe878",
   "metadata": {},
   "source": [
    "   Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39359d50",
   "metadata": {},
   "source": [
    "### 풀링 계층 구현하기\n",
    "\n",
    "풀링 계층 구현도 합성곱 계층과 마찬가지로 im2col을 사용해 입력 데이터를 전개한다.\n",
    "\n",
    "단, 풀링의 경우에는 **채널 쪽이 독립적이라는 점**이 합성곱 계층 때와 다르다. 구체적으로는 [그림 7-21]과 같이 풀링 적용 영역을 채널마다 독립적으로 전개한다.\n",
    "\n",
    "<font color = blue> [249페이지 그림 7-21 참고] </font>\n",
    "\n",
    "입력 데이터에 풀링 적용 영역을 전개하고, 전개한 행렬에서 행별 최댓값을 구하는 식으로 풀링을 실행한다.\n",
    "\n",
    "이를 적절한 형상으로 변환하기만 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15c2994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride = 1, pad = 0):\n",
    "        ## 먼저 풀링의 범위를 정한다. pool_h와 pool_w은 풀링 범위이다. w*h 풀링을 의미한다.\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        ## self.pool_w * self.pool_h의 범위로 데이터를 펼친다. 2차원 배열\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        ## 이것을 다시 self.pool_w * self.pool_h 크기의 행으로 만들기 위해 재배열한다.\n",
    "        ## 풀링은 채널마다 독립적으로 계산되므로 im2col에서 채널을 합쳤담녀 반대로 reshape를 통해 분리해준다.\n",
    "        ## 이렇게 되면 데이터 수만큼 self.pool_w * self.pool_h 크기의 행들이 만들어진다.\n",
    "        \n",
    "        # 최댓값 (2)\n",
    "        arg_max = np.argmax(col, axis = 1) # np.argmax은 최댓값의 인덱스를 반환한다.\n",
    "        out = np.max(col, axis = 1)\n",
    "        ## 풀링 단위의 데이터가 행으로 펼쳐졌으므로 행에서 max value를 찾을 수 있도록 axis = 1으로 최댓값을 뽑아준다.\n",
    "        \n",
    "        # 성형 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        ## 펼쳐놓은 데이터를 다시 돌려놓는다.\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        ## 형상을 forward에서 빠져나오기 전으로 바꿔준다.\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a3f69",
   "metadata": {},
   "source": [
    "풀링 계층 구현은 다음의 세 단계로 진행된다.\n",
    "\n",
    "1. 입력 데이터를 전개한다.\n",
    "2. 행별 최댓값을 구한다.\n",
    "3. 적절한 모양으로 성형한다.\n",
    "\n",
    "앞의 코드에서와 같이 각 단계는 한두 줄 정도로 간단히 구현된다.\n",
    "\n",
    "이상이 풀링 계층의 forward 처리이다. 이 절에서 선택한 전략을 따라 입력 데이터를 풀링하기 쉬운 형태로 전개해버리면 그 후의 구현은 간단하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89f964",
   "metadata": {},
   "source": [
    "## CNN 구현하기\n",
    "\n",
    "합성곱 계층과 풀링 계층을 구현했으니, 이 계층을 조합하여 손글씨 숫자를 인식하는 CNN을 조립해보자.\n",
    "\n",
    "여기에서는 1층 (Conv - ReLU - Pooling) / 2층 (Affine - ReLU) / 3층 (Affine - Softmax)로 하는 CNN을 구현한다.\n",
    "\n",
    "이를 SimpleConvNet이라는 이름의 클래스로 구현한다.\n",
    "\n",
    "    우선 SimpleConvNet의 초기화 (__init__)을 살펴보자. 초기화할 때는 다음과 같은 인수를 받는다.\n",
    "    \n",
    "    input_dim : 입력 데이터(채널 수, 높이, 너비)의 차원\n",
    "    conv_param : 합성곱 계층의 하이퍼파라미터(딕셔너리). 딕셔너리의 키는 다음과 같다.\n",
    "        - filter_num : 필터 수\n",
    "        - filter_size : 필터 크기\n",
    "        - stride : 스트라이드\n",
    "        - pad : 패딩\n",
    "    hidden_size : 은닉층(완전연결)의 뉴런 수\n",
    "    output_size : 출력층(완전연결)의 뉴런 수\n",
    "    weight_init_std : 초기화 때의 가중치 표준편차\n",
    "    \n",
    "여기에서 합성곱 계층의 하이퍼파라미터는 딕셔너리 형태로 주어진다(conv_param).\n",
    "\n",
    "이것은 필요한 하이퍼파라미터의 값이 예컨대 ['fiter_num' : 30, 'filter_size' : 5, 'pad' : 0, 'stride' : 1] 처럼 저장된다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fa53bd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simple_convnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_mnist\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimple_convnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleConvNet\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleConvNet\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simple_convnet'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim = (1, 28, 28),\n",
    "                 conv_param = {'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size = 100, output_size = 10, weight_init_std = 0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['filter_pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1] # size로 행을 넣어준다.\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size / 2) * (conv_output_size / 2))\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'],\n",
    "                                          conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h = 2, pool_w = 2, stride = 2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49df8eb",
   "metadata": {},
   "source": [
    "우선, 초기화 인수로 주어진 함성곱 계층의 하이퍼파라미터를 딕셔너리에서 꺼낸다 (나중에 쓰기 쉽도록)\n",
    "\n",
    "    filter_num = conv_param['filter_num']\n",
    "    ---\n",
    "    filter_stride = conv_param['stride'] 가 이 부분이다.\n",
    "    \n",
    "그리고 합성층 계층의 출력 크기를 계산한다.\n",
    "\n",
    "학습에 필요한 매개변수는 1번째 층의 합성곱 계층과 나머지 두 완전연결 계층의 가중치와 편향이다.\n",
    "\n",
    "이 매개변수들을 인스턴스 변수 params 딕셔너리에 저장한다.\n",
    "\n",
    "    self.params = {}\n",
    "    self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "    self.params['b1'] = np.zeros(filter_num)\n",
    "    self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "    self.params['b2'] = np.zeros(hidden_size)\n",
    "    self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b3'] = np.zeros(output_size) 이 이 부분이다.\n",
    "    \n",
    "학습에 필요한 매개변수는 1번째 층의 합성곱 계층과 나머지 두 완전연결 계층의 가중치와 편향이다.\n",
    "\n",
    "이 매개변수를 parmas 딕셔너리에 저장한다. 1번째 층의 합성곱 계층의 가중치는 W1, 편향은 b1 , ... 이런 식으로 저장한다.\n",
    "\n",
    "마지막으로 CNN을 구성하는 계층을 생성한다.\n",
    "\n",
    "순서가 있는 딕셔너리인 OrdereDict()에 self.layers을 생성한 후 계층들을 차례대로 추가한다.\n",
    "\n",
    "마지막 SoftmaxWithLoss 계층은 last_layer라는 별도 변수에 저장한다.\n",
    "\n",
    "이렇게 초기화를 마친 다음에는 추론을 수행하는 predict 메서드와 손실 함수 값을 구하는 loss 메서드을 위와 같이 구현할 수 있다.\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    def loss(self, x):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t) 이 이 부분이다.\n",
    "        \n",
    "    이 코드에서 x는 입력 데이터, t는 정답 레이블이다.\n",
    "\n",
    "    추론을 수행하는 predict 메서드는 초기화 때 layers에 추가한 계층을 맨 앞에서부터 차례로 forward 메서드를 호출하며 그 결과를 다음 계층에 전달한다.\n",
    "    \n",
    "    손실 함수를 구하는 loss 메서드는 predict 메서드의 결과를 인수로 마지막 층의 forward 메서드를 호출한다.\n",
    "    \n",
    "    즉, 첫 계층부터 마지막 계층까지 forward를 처리한다.\n",
    "    \n",
    "이어서, 오차역전파법으로 기울기를 구한다.\n",
    "\n",
    "매개변수의 기울기는 오차역전파법으로 구하며, 이 과정은 순전파와 역전파를 반복한다.\n",
    "\n",
    "순전파와 역전파를 제대로 구현했다면 여기에서는 단지 그것들을 적절한 순서로 호출만 해주면 된다.\n",
    "\n",
    "마지막으로 grads라는 딕셔너리 변수에 각 가중치 매개변수들을 저장한다.\n",
    "\n",
    "이에 따라서 SimpleConvNet으로 MNIST 데이터셋을 학습해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54699667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2995592150934097\n",
      "=== epoch:1, train acc:0.093, test acc:0.107 ===\n",
      "train loss:2.2959484544564437\n",
      "train loss:2.2862635412385\n",
      "train loss:2.2822068972606955\n",
      "train loss:2.274773909288874\n",
      "train loss:2.263774387085139\n",
      "train loss:2.256503856385622\n",
      "train loss:2.2397328007809123\n",
      "train loss:2.233057599141265\n",
      "train loss:2.169100062417902\n",
      "train loss:2.1681295241619805\n",
      "train loss:2.122498706977855\n",
      "train loss:2.035566536783097\n",
      "train loss:2.033298648862059\n",
      "train loss:1.9447856747950585\n",
      "train loss:1.9305924626148385\n",
      "train loss:1.8942899003057163\n",
      "train loss:1.744578797549061\n",
      "train loss:1.6607158208782813\n",
      "train loss:1.6531666223433237\n",
      "train loss:1.5147638200259594\n",
      "train loss:1.5005980221095367\n",
      "train loss:1.332825081247106\n",
      "train loss:1.3214473477872253\n",
      "train loss:1.2669213930610816\n",
      "train loss:1.1099367690165882\n",
      "train loss:1.0403245521444646\n",
      "train loss:1.0956553624272083\n",
      "train loss:1.2025090925087807\n",
      "train loss:0.8605838408653771\n",
      "train loss:0.9408856962432042\n",
      "train loss:0.9273962539207271\n",
      "train loss:0.7638392444228533\n",
      "train loss:0.7838218103944324\n",
      "train loss:0.7183333135760605\n",
      "train loss:0.5390264641739729\n",
      "train loss:0.8046998531304475\n",
      "train loss:0.863279852621107\n",
      "train loss:0.6666220456547641\n",
      "train loss:0.8665452517188277\n",
      "train loss:0.7264219204877371\n",
      "train loss:0.6290477904647976\n",
      "train loss:0.7117236325718964\n",
      "train loss:0.47797801035194754\n",
      "train loss:0.6484579671832568\n",
      "train loss:0.7100135663627897\n",
      "train loss:0.7583558867401239\n",
      "train loss:0.6170134832317399\n",
      "train loss:0.44481838274554164\n",
      "train loss:0.44965723476527414\n",
      "train loss:0.4781202124765047\n",
      "train loss:0.7804854408183715\n",
      "train loss:0.5433730165210173\n",
      "train loss:0.7104812674467226\n",
      "train loss:0.5973571811562066\n",
      "train loss:0.39377471916411566\n",
      "train loss:0.5646851756302195\n",
      "train loss:0.5068338008751276\n",
      "train loss:0.4409856870594333\n",
      "train loss:0.6581757397463367\n",
      "train loss:0.37739002189380516\n",
      "train loss:0.6225535951012948\n",
      "train loss:0.5388336176442221\n",
      "train loss:0.42465476151725434\n",
      "train loss:0.5635759768987604\n",
      "train loss:0.4733781322927743\n",
      "train loss:0.4137617416716612\n",
      "train loss:0.5910929392119163\n",
      "train loss:0.3667596068219303\n",
      "train loss:0.5616062357689391\n",
      "train loss:0.4020116167713676\n",
      "train loss:0.5911883260211863\n",
      "train loss:0.4247150686046772\n",
      "train loss:0.40836935500210814\n",
      "train loss:0.503119384554968\n",
      "train loss:0.3844940052386654\n",
      "train loss:0.4691143406893673\n",
      "train loss:0.3750500548696193\n",
      "train loss:0.6706251825014032\n",
      "train loss:0.2836885875025341\n",
      "train loss:0.40651288573534977\n",
      "train loss:0.35244858821156394\n",
      "train loss:0.3841891263759929\n",
      "train loss:0.5040973141877957\n",
      "train loss:0.45056869321665616\n",
      "train loss:0.4398309004674582\n",
      "train loss:0.48481287287847347\n",
      "train loss:0.4136904600161831\n",
      "train loss:0.4956492954960462\n",
      "train loss:0.383003381820389\n",
      "train loss:0.5711939959961596\n",
      "train loss:0.376488766318809\n",
      "train loss:0.4839671747606662\n",
      "train loss:0.43750397590147727\n",
      "train loss:0.49315118817438824\n",
      "train loss:0.5750909612225925\n",
      "train loss:0.46844171362756826\n",
      "train loss:0.30416816918652584\n",
      "train loss:0.6672356400200252\n",
      "train loss:0.4865581823359234\n",
      "train loss:0.6271815599219205\n",
      "train loss:0.4234051615389178\n",
      "train loss:0.3274235968759401\n",
      "train loss:0.5347598035813868\n",
      "train loss:0.23405541917789388\n",
      "train loss:0.34864464028966524\n",
      "train loss:0.41898641905136247\n",
      "train loss:0.4733171695931924\n",
      "train loss:0.3964138711280669\n",
      "train loss:0.4679970698743003\n",
      "train loss:0.3362026455558736\n",
      "train loss:0.32052016432346275\n",
      "train loss:0.447851876630782\n",
      "train loss:0.3277088260048933\n",
      "train loss:0.3577220985022897\n",
      "train loss:0.36080780888355546\n",
      "train loss:0.43476802245366925\n",
      "train loss:0.5536111113890514\n",
      "train loss:0.3405245233024523\n",
      "train loss:0.30492082495209155\n",
      "train loss:0.3475986215515299\n",
      "train loss:0.34534374804481305\n",
      "train loss:0.6236650338364875\n",
      "train loss:0.3490912644075561\n",
      "train loss:0.32594677408539785\n",
      "train loss:0.19427907041689674\n",
      "train loss:0.5108047625326126\n",
      "train loss:0.3258246354040652\n",
      "train loss:0.3270066690772528\n",
      "train loss:0.5197958696387706\n",
      "train loss:0.27350074509047034\n",
      "train loss:0.21101187300551175\n",
      "train loss:0.36267079638298555\n",
      "train loss:0.2799116793455938\n",
      "train loss:0.2374360772927471\n",
      "train loss:0.36410013559739307\n",
      "train loss:0.5556988058658351\n",
      "train loss:0.3390826525697521\n",
      "train loss:0.39407869772282156\n",
      "train loss:0.28189966277668366\n",
      "train loss:0.44301678541652106\n",
      "train loss:0.41578939198594045\n",
      "train loss:0.361200675940301\n",
      "train loss:0.3260493671493312\n",
      "train loss:0.27160768274630537\n",
      "train loss:0.33006151515453114\n",
      "train loss:0.44313762033516857\n",
      "train loss:0.29562093893521835\n",
      "train loss:0.4339700070916377\n",
      "train loss:0.3461266560716992\n",
      "train loss:0.40188280293867223\n",
      "train loss:0.39030132204565904\n",
      "train loss:0.30648401114333856\n",
      "train loss:0.3953070439884561\n",
      "train loss:0.23042465397669573\n",
      "train loss:0.3896648580541228\n",
      "train loss:0.33247805216685916\n",
      "train loss:0.23737594311213736\n",
      "train loss:0.3014869387456623\n",
      "train loss:0.35085894345752955\n",
      "train loss:0.3792841803096536\n",
      "train loss:0.26619506473205073\n",
      "train loss:0.4055717469245186\n",
      "train loss:0.3088631539237061\n",
      "train loss:0.47441130492171907\n",
      "train loss:0.38865495752892004\n",
      "train loss:0.23025672436927913\n",
      "train loss:0.5127864969998945\n",
      "train loss:0.2633366576974671\n",
      "train loss:0.33431943897023836\n",
      "train loss:0.3629904498668126\n",
      "train loss:0.3323667192372834\n",
      "train loss:0.19326710670467148\n",
      "train loss:0.19154027768236048\n",
      "train loss:0.34666671924524584\n",
      "train loss:0.3574039548747945\n",
      "train loss:0.3481016240477021\n",
      "train loss:0.2847195133935704\n",
      "train loss:0.2623021163571551\n",
      "train loss:0.45027422288371155\n",
      "train loss:0.3953944672036925\n",
      "train loss:0.38909589489121615\n",
      "train loss:0.2745656991805539\n",
      "train loss:0.24513313266240366\n",
      "train loss:0.39679486425684024\n",
      "train loss:0.35763592924123266\n",
      "train loss:0.33215419241015803\n",
      "train loss:0.35820407455935743\n",
      "train loss:0.29436090219115135\n",
      "train loss:0.3604344941900541\n",
      "train loss:0.3834051857264234\n",
      "train loss:0.3562246887440089\n",
      "train loss:0.3774611822627949\n",
      "train loss:0.302531903595619\n",
      "train loss:0.3068813054632216\n",
      "train loss:0.22696111195575025\n",
      "train loss:0.3223315660299241\n",
      "train loss:0.3031846386776234\n",
      "train loss:0.33093217843590905\n",
      "train loss:0.38335256574125653\n",
      "train loss:0.2898260581834153\n",
      "train loss:0.3136821669680355\n",
      "train loss:0.31614424269936386\n",
      "train loss:0.25439102723544843\n",
      "train loss:0.23394516604479115\n",
      "train loss:0.20862686970129146\n",
      "train loss:0.24453919866298296\n",
      "train loss:0.2591172038096284\n",
      "train loss:0.33982340893844953\n",
      "train loss:0.24411386025712606\n",
      "train loss:0.2897924199767188\n",
      "train loss:0.30682718347010013\n",
      "train loss:0.2687716669817777\n",
      "train loss:0.3954014466608815\n",
      "train loss:0.4068776660641291\n",
      "train loss:0.32236788367498825\n",
      "train loss:0.23231868614150764\n",
      "train loss:0.2722479855471921\n",
      "train loss:0.31397991582193385\n",
      "train loss:0.1947634055304331\n",
      "train loss:0.25866299461935666\n",
      "train loss:0.3378334292291098\n",
      "train loss:0.3593466773199399\n",
      "train loss:0.16292427496073214\n",
      "train loss:0.24233766202885412\n",
      "train loss:0.34659405217992356\n",
      "train loss:0.2838154072199426\n",
      "train loss:0.37260762522954166\n",
      "train loss:0.3992028721009333\n",
      "train loss:0.3600881350457303\n",
      "train loss:0.36310441298593404\n",
      "train loss:0.158108827318546\n",
      "train loss:0.221753095366304\n",
      "train loss:0.20868981324270608\n",
      "train loss:0.32516310721294567\n",
      "train loss:0.20007840561073628\n",
      "train loss:0.13362256803478234\n",
      "train loss:0.28459630069568065\n",
      "train loss:0.28154864794065576\n",
      "train loss:0.24265434693131546\n",
      "train loss:0.3565884883602552\n",
      "train loss:0.3009317983987534\n",
      "train loss:0.24678483969715212\n",
      "train loss:0.3687944322505172\n",
      "train loss:0.38259413807337717\n",
      "train loss:0.24570556843652125\n",
      "train loss:0.3860860092153648\n",
      "train loss:0.3314901997209445\n",
      "train loss:0.2927006622087626\n",
      "train loss:0.3488667404188614\n",
      "train loss:0.25972141734023446\n",
      "train loss:0.282928309276268\n",
      "train loss:0.30162348244320186\n",
      "train loss:0.24848633250212265\n",
      "train loss:0.23258151816970343\n",
      "train loss:0.2689441483851839\n",
      "train loss:0.23364523908146173\n",
      "train loss:0.25194980881399087\n",
      "train loss:0.29015645572754833\n",
      "train loss:0.498167956987075\n",
      "train loss:0.21803771131239302\n",
      "train loss:0.177657962472818\n",
      "train loss:0.40332715297708366\n",
      "train loss:0.2727394945569331\n",
      "train loss:0.19758770792042468\n",
      "train loss:0.40521308651536186\n",
      "train loss:0.2324184649919177\n",
      "train loss:0.25604332842526195\n",
      "train loss:0.34601862075562584\n",
      "train loss:0.38841763862350376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.16480698896765453\n",
      "train loss:0.30902187584705226\n",
      "train loss:0.1598556013404212\n",
      "train loss:0.34056595888477753\n",
      "train loss:0.19822587679721404\n",
      "train loss:0.24620278334782777\n",
      "train loss:0.24008996496405216\n",
      "train loss:0.31461189712337356\n",
      "train loss:0.3301646736984456\n",
      "train loss:0.3719360231715085\n",
      "train loss:0.2792482763016386\n",
      "train loss:0.27414484387016824\n",
      "train loss:0.42754725978712166\n",
      "train loss:0.23188923142637005\n",
      "train loss:0.21968333664259443\n",
      "train loss:0.24356131594538616\n",
      "train loss:0.24647503685176544\n",
      "train loss:0.28688825225878406\n",
      "train loss:0.12931213675834166\n",
      "train loss:0.2526311913402214\n",
      "train loss:0.2697726298013642\n",
      "train loss:0.4579024325006674\n",
      "train loss:0.34179833468331255\n",
      "train loss:0.17499426701585175\n",
      "train loss:0.18424639498856143\n",
      "train loss:0.4556852130874029\n",
      "train loss:0.16892348624375836\n",
      "train loss:0.30384336648030535\n",
      "train loss:0.2508018364511508\n",
      "train loss:0.1972577333913775\n",
      "train loss:0.29733897889639455\n",
      "train loss:0.23776648308765339\n",
      "train loss:0.2833835096607766\n",
      "train loss:0.28264124408400315\n",
      "train loss:0.305147018935573\n",
      "train loss:0.19873895987962295\n",
      "train loss:0.17297498907671394\n",
      "train loss:0.27360799072338715\n",
      "train loss:0.2898037873832113\n",
      "train loss:0.26069564934658834\n",
      "train loss:0.29698750149643927\n",
      "train loss:0.2794819697195198\n",
      "train loss:0.18376618547938964\n",
      "train loss:0.3143884885545241\n",
      "train loss:0.26701046796900796\n",
      "train loss:0.3558520146816374\n",
      "train loss:0.38250855602401096\n",
      "train loss:0.3711263360951896\n",
      "train loss:0.28502379629635666\n",
      "train loss:0.22076216107220806\n",
      "train loss:0.38415167052475346\n",
      "train loss:0.18034605476920076\n",
      "train loss:0.19493293788795596\n",
      "train loss:0.30962907537329853\n",
      "train loss:0.24485403408426476\n",
      "train loss:0.24028771516437333\n",
      "train loss:0.20915224405760724\n",
      "train loss:0.291122655613891\n",
      "train loss:0.31948798991929755\n",
      "train loss:0.1618785592023012\n",
      "train loss:0.3693324439817968\n",
      "train loss:0.11793328532351613\n",
      "train loss:0.1785861929516804\n",
      "train loss:0.16352979852715252\n",
      "train loss:0.25432091493067693\n",
      "train loss:0.3344127163837902\n",
      "train loss:0.31899636835335393\n",
      "train loss:0.2516779499869173\n",
      "train loss:0.32191908668468955\n",
      "train loss:0.3112681488292511\n",
      "train loss:0.28446377755854113\n",
      "train loss:0.1849078556318481\n",
      "train loss:0.2899118232116523\n",
      "train loss:0.2565894396055777\n",
      "train loss:0.16710144566305213\n",
      "train loss:0.24043636181021866\n",
      "train loss:0.2966746268522023\n",
      "train loss:0.14536655879451663\n",
      "train loss:0.217401880423266\n",
      "train loss:0.2095298641950379\n",
      "train loss:0.11477030732760651\n",
      "train loss:0.247040880540309\n",
      "train loss:0.17591260788726884\n",
      "train loss:0.22355466800705556\n",
      "train loss:0.19036066645132949\n",
      "train loss:0.12464079477075901\n",
      "train loss:0.16966638672139606\n",
      "train loss:0.27891156744992\n",
      "train loss:0.1333510215133747\n",
      "train loss:0.16439417334509426\n",
      "train loss:0.24974492277116675\n",
      "train loss:0.35091747972567716\n",
      "train loss:0.1759680338813021\n",
      "train loss:0.15860190290945048\n",
      "train loss:0.23744249207341372\n",
      "train loss:0.21474457472482353\n",
      "train loss:0.30760265752298144\n",
      "train loss:0.25530844559726973\n",
      "train loss:0.20405775327925701\n",
      "train loss:0.28079869135827157\n",
      "train loss:0.30699880683969727\n",
      "train loss:0.11945443693634196\n",
      "train loss:0.24543501305077398\n",
      "train loss:0.20742645928024905\n",
      "train loss:0.2273334909827104\n",
      "train loss:0.22718232808217054\n",
      "train loss:0.16081232513713126\n",
      "train loss:0.08883065061585625\n",
      "train loss:0.18952574358790752\n",
      "train loss:0.17013645383964499\n",
      "train loss:0.4766325475293865\n",
      "train loss:0.27368777879309125\n",
      "train loss:0.2926983236416006\n",
      "train loss:0.22932473286589194\n",
      "train loss:0.2968822493406413\n",
      "train loss:0.38183738387811056\n",
      "train loss:0.17795610044259114\n",
      "train loss:0.1916410584285423\n",
      "train loss:0.1374478975272835\n",
      "train loss:0.15024282601467\n",
      "train loss:0.31469054684054865\n",
      "train loss:0.19190420283970472\n",
      "train loss:0.21739308562728812\n",
      "train loss:0.18472122745933325\n",
      "train loss:0.24914851135082572\n",
      "train loss:0.18578268262994427\n",
      "train loss:0.28395213377530043\n",
      "train loss:0.17057389034471238\n",
      "train loss:0.14143334995075832\n",
      "train loss:0.21855689574004017\n",
      "train loss:0.2621680332080091\n",
      "train loss:0.29145647045449885\n",
      "train loss:0.26016658611571747\n",
      "train loss:0.11802330072941043\n",
      "train loss:0.15135025879920763\n",
      "train loss:0.1462877143046325\n",
      "train loss:0.19998673234078285\n",
      "train loss:0.1831740014505682\n",
      "train loss:0.28027166722663216\n",
      "train loss:0.13956325025987965\n",
      "train loss:0.26065210464815114\n",
      "train loss:0.20995700919307894\n",
      "train loss:0.2504237763582915\n",
      "train loss:0.1483111813729601\n",
      "train loss:0.2084984692174371\n",
      "train loss:0.22857353554009102\n",
      "train loss:0.11118212048128105\n",
      "train loss:0.15186365808842262\n",
      "train loss:0.16993937214746604\n",
      "train loss:0.23747535066231354\n",
      "train loss:0.20657986779506532\n",
      "train loss:0.3329285965597449\n",
      "train loss:0.29230296776780923\n",
      "train loss:0.14729458046573773\n",
      "train loss:0.27333924795713554\n",
      "train loss:0.12188258588882833\n",
      "train loss:0.14753806182510337\n",
      "train loss:0.1685153669150562\n",
      "train loss:0.15663271044476057\n",
      "train loss:0.26400605975498814\n",
      "train loss:0.1802372984558417\n",
      "train loss:0.16884883396684144\n",
      "train loss:0.2103256552526606\n",
      "train loss:0.12976420446536097\n",
      "train loss:0.23619227970600107\n",
      "train loss:0.17105821223231266\n",
      "train loss:0.2778714503801236\n",
      "train loss:0.15650088368620696\n",
      "train loss:0.24078777012708286\n",
      "train loss:0.37272265493790896\n",
      "train loss:0.17608986247112188\n",
      "train loss:0.1510490445074376\n",
      "train loss:0.18128894553674232\n",
      "train loss:0.23235551331915125\n",
      "train loss:0.2098231398446651\n",
      "train loss:0.198683695611693\n",
      "train loss:0.2626238474135377\n",
      "train loss:0.16385598559843778\n",
      "train loss:0.16501733199350851\n",
      "train loss:0.2685615820457343\n",
      "train loss:0.19231256636822247\n",
      "train loss:0.14600579898786342\n",
      "train loss:0.2890063929199645\n",
      "train loss:0.2223372918298335\n",
      "train loss:0.3399259223301259\n",
      "train loss:0.24168894258849732\n",
      "train loss:0.2708143921773203\n",
      "train loss:0.16205424085573372\n",
      "train loss:0.10092662527675302\n",
      "train loss:0.20874544813828336\n",
      "train loss:0.16897511112952418\n",
      "train loss:0.12079756707219634\n",
      "train loss:0.2209454636240872\n",
      "train loss:0.20233244096741299\n",
      "train loss:0.16752905165981855\n",
      "train loss:0.16575227594603478\n",
      "train loss:0.38395113129754876\n",
      "train loss:0.19257923098047466\n",
      "train loss:0.18410322335980958\n",
      "train loss:0.26776539886446293\n",
      "train loss:0.1545759033188063\n",
      "train loss:0.14749194717522696\n",
      "train loss:0.12687277105429604\n",
      "train loss:0.22528479212186803\n",
      "train loss:0.193059419183486\n",
      "train loss:0.1231007425323373\n",
      "train loss:0.10670235580039764\n",
      "train loss:0.17094493795259832\n",
      "train loss:0.27814502945456976\n",
      "train loss:0.1876856329146936\n",
      "train loss:0.17040997667467664\n",
      "train loss:0.07721555324913452\n",
      "train loss:0.2093581628705095\n",
      "train loss:0.2041310962909709\n",
      "train loss:0.17653378249520904\n",
      "train loss:0.2823785484451344\n",
      "train loss:0.19379829143466215\n",
      "train loss:0.33003763610046166\n",
      "train loss:0.23535947863955123\n",
      "train loss:0.09620534307251072\n",
      "train loss:0.14364433714862856\n",
      "train loss:0.1407930169561606\n",
      "train loss:0.15038254178223118\n",
      "train loss:0.1549298824703917\n",
      "train loss:0.23917499442515514\n",
      "train loss:0.2331729775941281\n",
      "train loss:0.23215604236396156\n",
      "train loss:0.2260209725529769\n",
      "train loss:0.095273178612526\n",
      "train loss:0.14967761334909363\n",
      "train loss:0.1729942684801856\n",
      "train loss:0.10900169721121576\n",
      "train loss:0.31013385178712466\n",
      "train loss:0.13965189168565287\n",
      "train loss:0.21416260576430776\n",
      "train loss:0.30255188694479623\n",
      "train loss:0.159013749168753\n",
      "train loss:0.1970427288760098\n",
      "train loss:0.09806058308264318\n",
      "train loss:0.12166498832958807\n",
      "train loss:0.12964157846465066\n",
      "train loss:0.18405112051914052\n",
      "train loss:0.1746437466764296\n",
      "train loss:0.28149041113894535\n",
      "train loss:0.06947458501436021\n",
      "train loss:0.2120975897846625\n",
      "train loss:0.15987894670783898\n",
      "train loss:0.30844379204477784\n",
      "train loss:0.1964650093821579\n",
      "train loss:0.2428488182999889\n",
      "train loss:0.13996261527675447\n",
      "train loss:0.22712781928928696\n",
      "train loss:0.09898971550476514\n",
      "train loss:0.2254981636012057\n",
      "train loss:0.09431074525981313\n",
      "train loss:0.12062768322767267\n",
      "train loss:0.2260639968717722\n",
      "train loss:0.11401199362274815\n",
      "train loss:0.0369224049136143\n",
      "train loss:0.12089285914609497\n",
      "train loss:0.19459896619961645\n",
      "train loss:0.2437082934611417\n",
      "train loss:0.21133144608626428\n",
      "train loss:0.1184950966971627\n",
      "train loss:0.09856495646693243\n",
      "train loss:0.16671893388484085\n",
      "train loss:0.2600823092953472\n",
      "train loss:0.14071026325462593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1459246365930462\n",
      "train loss:0.2576323443819074\n",
      "train loss:0.11220348720365819\n",
      "train loss:0.10958162551581328\n",
      "train loss:0.11651627572854432\n",
      "train loss:0.15092053931291224\n",
      "train loss:0.08920292339113897\n",
      "train loss:0.10822796787511826\n",
      "train loss:0.12351021056577832\n",
      "train loss:0.07665928864936691\n",
      "train loss:0.12684289824189623\n",
      "train loss:0.1120037244131668\n",
      "train loss:0.09674921773231772\n",
      "train loss:0.15049603757553603\n",
      "train loss:0.2727483045887521\n",
      "train loss:0.19878402576290502\n",
      "train loss:0.17539096071033022\n",
      "train loss:0.17239263716962383\n",
      "train loss:0.17462177061326525\n",
      "train loss:0.1493209392196542\n",
      "train loss:0.10937897828397179\n",
      "train loss:0.0680006225174387\n",
      "train loss:0.1852232799505067\n",
      "train loss:0.13717438472117674\n",
      "train loss:0.05925244706948206\n",
      "train loss:0.16588561295352935\n",
      "train loss:0.1579719658758133\n",
      "train loss:0.14580987639396822\n",
      "train loss:0.0810091963378457\n",
      "train loss:0.10807447843055956\n",
      "train loss:0.09209336971256885\n",
      "train loss:0.16946697746525097\n",
      "train loss:0.31100643379857595\n",
      "train loss:0.15312690243106603\n",
      "train loss:0.12986438959333083\n",
      "train loss:0.13386200467764975\n",
      "train loss:0.16122701306882994\n",
      "train loss:0.08717567460260701\n",
      "train loss:0.09023975815857399\n",
      "train loss:0.16596343347684747\n",
      "train loss:0.22230982265615074\n",
      "train loss:0.10884686830650855\n",
      "train loss:0.0665653888320041\n",
      "train loss:0.2096860570559566\n",
      "train loss:0.11888292743005856\n",
      "train loss:0.15311894629905928\n",
      "train loss:0.1365407100926265\n",
      "train loss:0.07419151245284339\n",
      "train loss:0.08100785842306885\n",
      "train loss:0.14742936885694\n",
      "train loss:0.13311727733818338\n",
      "train loss:0.1564679554922804\n",
      "train loss:0.19523570732192133\n",
      "train loss:0.1315263021707859\n",
      "train loss:0.25964150573761263\n",
      "train loss:0.1162558646610577\n",
      "train loss:0.11434156580209212\n",
      "train loss:0.2297873557691837\n",
      "train loss:0.18911511565419376\n",
      "train loss:0.08129536651688841\n",
      "train loss:0.32406705424324217\n",
      "train loss:0.056103091200717194\n",
      "train loss:0.1447698210210788\n",
      "=== epoch:2, train acc:0.954, test acc:0.958 ===\n",
      "train loss:0.08398749936866169\n",
      "train loss:0.09180073870699954\n",
      "train loss:0.24663626914870754\n",
      "train loss:0.14222342291314016\n",
      "train loss:0.17178369977566924\n",
      "train loss:0.1451018040831337\n",
      "train loss:0.16937845537470822\n",
      "train loss:0.055838790776148006\n",
      "train loss:0.21711197365020707\n",
      "train loss:0.12016882179282588\n",
      "train loss:0.10064821959759546\n",
      "train loss:0.14111114062326935\n",
      "train loss:0.24655316425663654\n",
      "train loss:0.16539951282279922\n",
      "train loss:0.15567875847029924\n",
      "train loss:0.15552252286848625\n",
      "train loss:0.12227975930053418\n",
      "train loss:0.06530192950142989\n",
      "train loss:0.21131606937352426\n",
      "train loss:0.09998713469157425\n",
      "train loss:0.14637032833864563\n",
      "train loss:0.16352215085169117\n",
      "train loss:0.1734121048989417\n",
      "train loss:0.2107469129887229\n",
      "train loss:0.2507836124522205\n",
      "train loss:0.1152831727676595\n",
      "train loss:0.22275501835562708\n",
      "train loss:0.1414007638218557\n",
      "train loss:0.10067646466509701\n",
      "train loss:0.11906552289718668\n",
      "train loss:0.19659713319907024\n",
      "train loss:0.1387935120283676\n",
      "train loss:0.1496988995108906\n",
      "train loss:0.13886518155842353\n",
      "train loss:0.17919349310046662\n",
      "train loss:0.20229572281905348\n",
      "train loss:0.19684248123802114\n",
      "train loss:0.11460093808838469\n",
      "train loss:0.21184819584799888\n",
      "train loss:0.10306299195055953\n",
      "train loss:0.18912509757875434\n",
      "train loss:0.13055217334622035\n",
      "train loss:0.132734974057234\n",
      "train loss:0.10519695943055696\n",
      "train loss:0.10631208950645622\n",
      "train loss:0.3439177979999289\n",
      "train loss:0.06437244100019174\n",
      "train loss:0.1153088334009202\n",
      "train loss:0.07368012285855845\n",
      "train loss:0.13314504949325895\n",
      "train loss:0.06558899780679561\n",
      "train loss:0.13883442966279844\n",
      "train loss:0.3726790938803973\n",
      "train loss:0.20904527699996422\n",
      "train loss:0.10412902441762073\n",
      "train loss:0.09837442890666254\n",
      "train loss:0.06807079625427286\n",
      "train loss:0.11141120400073745\n",
      "train loss:0.20671827172538326\n",
      "train loss:0.08706241508300121\n",
      "train loss:0.1801874242482586\n",
      "train loss:0.17283392637974335\n",
      "train loss:0.17442026077780706\n",
      "train loss:0.13378317065308587\n",
      "train loss:0.15791102309569233\n",
      "train loss:0.22388092538482365\n",
      "train loss:0.1286069906962919\n",
      "train loss:0.1860631975975109\n",
      "train loss:0.09264374531160413\n",
      "train loss:0.15923416012144126\n",
      "train loss:0.14604618077420503\n",
      "train loss:0.12978540110515951\n",
      "train loss:0.16369421512114532\n",
      "train loss:0.11395693967000317\n",
      "train loss:0.07045424937181628\n",
      "train loss:0.06436955521696432\n",
      "train loss:0.20165775098645347\n",
      "train loss:0.2032145396454155\n",
      "train loss:0.10986758015470244\n",
      "train loss:0.16433387604591002\n",
      "train loss:0.07891523991479323\n",
      "train loss:0.0906693199031306\n",
      "train loss:0.11415430450107925\n",
      "train loss:0.07546295992850693\n",
      "train loss:0.13061190136549727\n",
      "train loss:0.12877796652509915\n",
      "train loss:0.153979377984773\n",
      "train loss:0.08964200054341852\n",
      "train loss:0.15483592878987038\n",
      "train loss:0.31573204891827744\n",
      "train loss:0.21087213726999224\n",
      "train loss:0.11697324662697327\n",
      "train loss:0.11990555545518225\n",
      "train loss:0.118222647091158\n",
      "train loss:0.09282822716645509\n",
      "train loss:0.06479908465908242\n",
      "train loss:0.12284355036031978\n",
      "train loss:0.1189438752207898\n",
      "train loss:0.09985645533199929\n",
      "train loss:0.09805936362403724\n",
      "train loss:0.1015053524731059\n",
      "train loss:0.1887904213006986\n",
      "train loss:0.19629245818682417\n",
      "train loss:0.08329302374299129\n",
      "train loss:0.2020249621996397\n",
      "train loss:0.07150708729107148\n",
      "train loss:0.20466733839564746\n",
      "train loss:0.07581681026347432\n",
      "train loss:0.14282571876057204\n",
      "train loss:0.1977186282355187\n",
      "train loss:0.10614267440892906\n",
      "train loss:0.0893344141655907\n",
      "train loss:0.13682001952894043\n",
      "train loss:0.06774368743276403\n",
      "train loss:0.07333207397022623\n",
      "train loss:0.24619718817959632\n",
      "train loss:0.08118287827089991\n",
      "train loss:0.1398257418888053\n",
      "train loss:0.10480057746248075\n",
      "train loss:0.19186638030066264\n",
      "train loss:0.11719233573431265\n",
      "train loss:0.10780784403019672\n",
      "train loss:0.08668132324054197\n",
      "train loss:0.1076017955819289\n",
      "train loss:0.10407479686742038\n",
      "train loss:0.15824420875002182\n",
      "train loss:0.26585431849295654\n",
      "train loss:0.1603042772028187\n",
      "train loss:0.23593054270183475\n",
      "train loss:0.09763405558972044\n",
      "train loss:0.08941281982189374\n",
      "train loss:0.18357917352387024\n",
      "train loss:0.08815736863048805\n",
      "train loss:0.16161616005996446\n",
      "train loss:0.12548282045106027\n",
      "train loss:0.1055529319607659\n",
      "train loss:0.10495865557255611\n",
      "train loss:0.18111601623185358\n",
      "train loss:0.18826904440645933\n",
      "train loss:0.2681672655431685\n",
      "train loss:0.22520597578217647\n",
      "train loss:0.0747668170114926\n",
      "train loss:0.11717611390844514\n",
      "train loss:0.11613854859508577\n",
      "train loss:0.22234012551589097\n",
      "train loss:0.14993775670560405\n",
      "train loss:0.14568767393555765\n",
      "train loss:0.07728828121833142\n",
      "train loss:0.1832347636079116\n",
      "train loss:0.18486441836102055\n",
      "train loss:0.18835865243342237\n",
      "train loss:0.1205644168214086\n",
      "train loss:0.06276036040922058\n",
      "train loss:0.06108592900107523\n",
      "train loss:0.1143463028162126\n",
      "train loss:0.17591333961402658\n",
      "train loss:0.15868772969645428\n",
      "train loss:0.14631473555003768\n",
      "train loss:0.0985222794868481\n",
      "train loss:0.1608616895112794\n",
      "train loss:0.11316808725882238\n",
      "train loss:0.0910987291692642\n",
      "train loss:0.242770413425763\n",
      "train loss:0.1750515527314355\n",
      "train loss:0.24335023724621763\n",
      "train loss:0.10490585912255282\n",
      "train loss:0.10601347505251439\n",
      "train loss:0.06589552096577352\n",
      "train loss:0.0745492770305232\n",
      "train loss:0.09416708991207155\n",
      "train loss:0.10733423700204846\n",
      "train loss:0.15785344078036953\n",
      "train loss:0.27240440777064673\n",
      "train loss:0.08577820488769429\n",
      "train loss:0.08118663763434526\n",
      "train loss:0.13108376413015543\n",
      "train loss:0.14638993506458012\n",
      "train loss:0.03423340913985076\n",
      "train loss:0.06822344516102989\n",
      "train loss:0.1062173886474775\n",
      "train loss:0.11185635170759807\n",
      "train loss:0.0377467088029436\n",
      "train loss:0.09692719016519125\n",
      "train loss:0.10748920428062463\n",
      "train loss:0.14030623924420657\n",
      "train loss:0.0992516203632867\n",
      "train loss:0.11896742536197248\n",
      "train loss:0.13328741492324125\n",
      "train loss:0.13714075780029014\n",
      "train loss:0.15853885304353918\n",
      "train loss:0.1403859865820646\n",
      "train loss:0.08836662272567825\n",
      "train loss:0.08474213055277006\n",
      "train loss:0.1325075838861279\n",
      "train loss:0.14364942013569756\n",
      "train loss:0.18848095174958182\n",
      "train loss:0.12275630279559176\n",
      "train loss:0.09939192776762211\n",
      "train loss:0.13299543662270324\n",
      "train loss:0.12353783223026385\n",
      "train loss:0.11435267706742075\n",
      "train loss:0.050450565307170916\n",
      "train loss:0.12808076893641243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.10505522259646279\n",
      "train loss:0.12369154602369521\n",
      "train loss:0.0860088764904873\n",
      "train loss:0.11440916611215003\n",
      "train loss:0.0491814546767791\n",
      "train loss:0.06450511138840785\n",
      "train loss:0.19042398351479592\n",
      "train loss:0.0697596948267052\n",
      "train loss:0.11805434672499013\n",
      "train loss:0.14610473013712605\n",
      "train loss:0.11705830814097812\n",
      "train loss:0.12530588057475417\n",
      "train loss:0.10331089387096792\n",
      "train loss:0.09078337286323061\n",
      "train loss:0.14754038332848507\n",
      "train loss:0.12917331498439094\n",
      "train loss:0.06316764883595699\n",
      "train loss:0.2131790856343057\n",
      "train loss:0.06589885802127216\n",
      "train loss:0.07738648455159242\n",
      "train loss:0.04233751801393125\n",
      "train loss:0.08574269840815614\n",
      "train loss:0.15781230713767525\n",
      "train loss:0.06118486243947957\n",
      "train loss:0.12384951794021153\n",
      "train loss:0.10800032395958656\n",
      "train loss:0.030213540555647768\n",
      "train loss:0.2149524185741861\n",
      "train loss:0.13306454251537583\n",
      "train loss:0.12490150231524605\n",
      "train loss:0.10892898829407338\n",
      "train loss:0.05965082072703914\n",
      "train loss:0.1705476149464105\n",
      "train loss:0.07159277752270378\n",
      "train loss:0.08043414712178572\n",
      "train loss:0.22871931765164144\n",
      "train loss:0.07372171433676895\n",
      "train loss:0.16887500595756694\n",
      "train loss:0.049943405323920394\n",
      "train loss:0.1675951501878639\n",
      "train loss:0.1731133035180579\n",
      "train loss:0.12056211295058678\n",
      "train loss:0.04749135416004738\n",
      "train loss:0.12929248853863531\n",
      "train loss:0.16299051630370237\n",
      "train loss:0.09665743052235844\n",
      "train loss:0.19133368137656417\n",
      "train loss:0.05107112619524875\n",
      "train loss:0.03801109308266872\n",
      "train loss:0.06835170300517475\n",
      "train loss:0.08206052446439405\n",
      "train loss:0.03220886031085979\n",
      "train loss:0.05118649508814425\n",
      "train loss:0.11052588676804033\n",
      "train loss:0.03627560839481324\n",
      "train loss:0.11542108283858411\n",
      "train loss:0.08376304587941664\n",
      "train loss:0.05324224530279062\n",
      "train loss:0.14842813970329552\n",
      "train loss:0.05458083665556813\n",
      "train loss:0.11449060647225401\n",
      "train loss:0.07507209394577236\n",
      "train loss:0.22565787663035686\n",
      "train loss:0.09054086990161755\n",
      "train loss:0.11144925347988871\n",
      "train loss:0.1800756030783975\n",
      "train loss:0.08254624530685199\n",
      "train loss:0.07863274274052447\n",
      "train loss:0.0368442931401099\n",
      "train loss:0.04458361179019076\n",
      "train loss:0.04821728643893372\n",
      "train loss:0.14696717419084768\n",
      "train loss:0.06856162054070558\n",
      "train loss:0.20081955137111926\n",
      "train loss:0.07193567167417236\n",
      "train loss:0.04754198894045862\n",
      "train loss:0.09012519766456607\n",
      "train loss:0.07752056884077402\n",
      "train loss:0.148399490844174\n",
      "train loss:0.0904108376835714\n",
      "train loss:0.05312619086445416\n",
      "train loss:0.06947899906288732\n",
      "train loss:0.14164039704466472\n",
      "train loss:0.03128745769244914\n",
      "train loss:0.07283968996615603\n",
      "train loss:0.25724352592524086\n",
      "train loss:0.0785359288955655\n",
      "train loss:0.10949239669469943\n",
      "train loss:0.1503987698769312\n",
      "train loss:0.13002629958302614\n",
      "train loss:0.1247894936569255\n",
      "train loss:0.15976139821031043\n",
      "train loss:0.0545608433547011\n",
      "train loss:0.09885039271484493\n",
      "train loss:0.09032140436702184\n",
      "train loss:0.056419547806360305\n",
      "train loss:0.08367512936376625\n",
      "train loss:0.039262253102465026\n",
      "train loss:0.08366810650099149\n",
      "train loss:0.14927268993505535\n",
      "train loss:0.0749744187830374\n",
      "train loss:0.039367712376829234\n",
      "train loss:0.06680916225502764\n",
      "train loss:0.24338754296851867\n",
      "train loss:0.29506269544067903\n",
      "train loss:0.09779194233896538\n",
      "train loss:0.10290434064030249\n",
      "train loss:0.06873950802065418\n",
      "train loss:0.20100118168849118\n",
      "train loss:0.07816473315335284\n",
      "train loss:0.0388077054232918\n",
      "train loss:0.11824387065133343\n",
      "train loss:0.07376935762690354\n",
      "train loss:0.2519627798459032\n",
      "train loss:0.07930055893877792\n",
      "train loss:0.14519426757169962\n",
      "train loss:0.1503075074772688\n",
      "train loss:0.08769713011282258\n",
      "train loss:0.061237445342304485\n",
      "train loss:0.13275990968131007\n",
      "train loss:0.08439430943264557\n",
      "train loss:0.12695925377970527\n",
      "train loss:0.05307452356461288\n",
      "train loss:0.02948217106007255\n",
      "train loss:0.05941025220813158\n",
      "train loss:0.17379154938145885\n",
      "train loss:0.06806025508561075\n",
      "train loss:0.07311829334195535\n",
      "train loss:0.08233757221476277\n",
      "train loss:0.058317125781240396\n",
      "train loss:0.11656729386783461\n",
      "train loss:0.06963612011949245\n",
      "train loss:0.06515380159491768\n",
      "train loss:0.15486317634440636\n",
      "train loss:0.05932255235112608\n",
      "train loss:0.0802369752684623\n",
      "train loss:0.11882215593542254\n",
      "train loss:0.03508190131578949\n",
      "train loss:0.12743199028735888\n",
      "train loss:0.044463168935768474\n",
      "train loss:0.09301055799111016\n",
      "train loss:0.031015453147096465\n",
      "train loss:0.08852476200466673\n",
      "train loss:0.2872128248524471\n",
      "train loss:0.09035464595528549\n",
      "train loss:0.10958451701691338\n",
      "train loss:0.07027181691147753\n",
      "train loss:0.06206094819112915\n",
      "train loss:0.04790943406823842\n",
      "train loss:0.15147761810167457\n",
      "train loss:0.09968330726702757\n",
      "train loss:0.13575500907144838\n",
      "train loss:0.0860805980327436\n",
      "train loss:0.07577770004749673\n",
      "train loss:0.17893736149142456\n",
      "train loss:0.07503929922161112\n",
      "train loss:0.058110682203889645\n",
      "train loss:0.04674959375695391\n",
      "train loss:0.04768528036638789\n",
      "train loss:0.07030597062210589\n",
      "train loss:0.114163759478952\n",
      "train loss:0.05739018771875503\n",
      "train loss:0.09178818282227469\n",
      "train loss:0.11450912428872226\n",
      "train loss:0.06523643510375998\n",
      "train loss:0.1513919674067552\n",
      "train loss:0.11816968634680486\n",
      "train loss:0.06299650538218446\n",
      "train loss:0.020574986581790782\n",
      "train loss:0.10791423108549299\n",
      "train loss:0.04788648354207562\n",
      "train loss:0.04350165088776141\n",
      "train loss:0.26651990489863203\n",
      "train loss:0.08948037109195128\n",
      "train loss:0.11096258335958568\n",
      "train loss:0.09216462983901895\n",
      "train loss:0.09878641418471142\n",
      "train loss:0.09461994506935854\n",
      "train loss:0.12996451127109715\n",
      "train loss:0.12538173602203262\n",
      "train loss:0.09448179521197195\n",
      "train loss:0.16668574962518234\n",
      "train loss:0.08279593470983256\n",
      "train loss:0.06085483448549174\n",
      "train loss:0.09679993195098534\n",
      "train loss:0.12527381636095689\n",
      "train loss:0.14538441660886842\n",
      "train loss:0.09122908315738203\n",
      "train loss:0.08852269016129168\n",
      "train loss:0.14954373320845207\n",
      "train loss:0.0900964164122804\n",
      "train loss:0.04049224468147373\n",
      "train loss:0.15059292171359717\n",
      "train loss:0.07257177797839752\n",
      "train loss:0.12703014608161697\n",
      "train loss:0.09303976840504426\n",
      "train loss:0.058543159389575654\n",
      "train loss:0.11471207861019\n",
      "train loss:0.0732263624339468\n",
      "train loss:0.06800485079083246\n",
      "train loss:0.09407995536315193\n",
      "train loss:0.07786652335683142\n",
      "train loss:0.11832324344360322\n",
      "train loss:0.09199313680516402\n",
      "train loss:0.1202486149382906\n",
      "train loss:0.034898932963448336\n",
      "train loss:0.11122374474582107\n",
      "train loss:0.06923820077400406\n",
      "train loss:0.14881739144150224\n",
      "train loss:0.07968933730912234\n",
      "train loss:0.0844592366713858\n",
      "train loss:0.11026781454362258\n",
      "train loss:0.12386398626494012\n",
      "train loss:0.07228810649730608\n",
      "train loss:0.1140051136101911\n",
      "train loss:0.05897693518992335\n",
      "train loss:0.08088793885466879\n",
      "train loss:0.02849706816956087\n",
      "train loss:0.0879545482811438\n",
      "train loss:0.03218618020054086\n",
      "train loss:0.06621995741882195\n",
      "train loss:0.11828236973631194\n",
      "train loss:0.05765755355586531\n",
      "train loss:0.10658601172113093\n",
      "train loss:0.10834907735130012\n",
      "train loss:0.08341903029141981\n",
      "train loss:0.06783657071440555\n",
      "train loss:0.05850803376785361\n",
      "train loss:0.12243953202666819\n",
      "train loss:0.051932506835470044\n",
      "train loss:0.044960903524925336\n",
      "train loss:0.036193566117243156\n",
      "train loss:0.049701590488053754\n",
      "train loss:0.09467002922046441\n",
      "train loss:0.10739675004623374\n",
      "train loss:0.03824610208394676\n",
      "train loss:0.051300432891800835\n",
      "train loss:0.20408291284332897\n",
      "train loss:0.05172056872007118\n",
      "train loss:0.05813309947061513\n",
      "train loss:0.04024158979233891\n",
      "train loss:0.05329622071807308\n",
      "train loss:0.07499766580376031\n",
      "train loss:0.16243251426293445\n",
      "train loss:0.06990859208637414\n",
      "train loss:0.05400398450750073\n",
      "train loss:0.06960427282495135\n",
      "train loss:0.14236278777198066\n",
      "train loss:0.03847608182254922\n",
      "train loss:0.04833496310447208\n",
      "train loss:0.025148846725157378\n",
      "train loss:0.09636430631440918\n",
      "train loss:0.03180832376362392\n",
      "train loss:0.015225850560234101\n",
      "train loss:0.17783840444118096\n",
      "train loss:0.028023969584793806\n",
      "train loss:0.1248907033521717\n",
      "train loss:0.15052194206023753\n",
      "train loss:0.09106232728709626\n",
      "train loss:0.04038119599921278\n",
      "train loss:0.06968367616864321\n",
      "train loss:0.09106012411628019\n",
      "train loss:0.0444450484312538\n",
      "train loss:0.04971389099760745\n",
      "train loss:0.09048946267292267\n",
      "train loss:0.09577496941229581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.040355372233922084\n",
      "train loss:0.02581111007698573\n",
      "train loss:0.11517962574151089\n",
      "train loss:0.07687249662513587\n",
      "train loss:0.07842233572282556\n",
      "train loss:0.09240540407491771\n",
      "train loss:0.06907350267686746\n",
      "train loss:0.08089839021069616\n",
      "train loss:0.11286219949885451\n",
      "train loss:0.05267909580914174\n",
      "train loss:0.12333953509653503\n",
      "train loss:0.04607776091594179\n",
      "train loss:0.07827944880189887\n",
      "train loss:0.07664351198301082\n",
      "train loss:0.0719855826774542\n",
      "train loss:0.12416380091792435\n",
      "train loss:0.12025556515490797\n",
      "train loss:0.044142035831415745\n",
      "train loss:0.07662213333483298\n",
      "train loss:0.10146678273708427\n",
      "train loss:0.06972780190590772\n",
      "train loss:0.05694187875177584\n",
      "train loss:0.1063281432775085\n",
      "train loss:0.15627643643699135\n",
      "train loss:0.04329793158258042\n",
      "train loss:0.046102643043377654\n",
      "train loss:0.11817576304593798\n",
      "train loss:0.10696058594839687\n",
      "train loss:0.09668098248291238\n",
      "train loss:0.12159152941427585\n",
      "train loss:0.10282058142296373\n",
      "train loss:0.034465573952419316\n",
      "train loss:0.06322014970803576\n",
      "train loss:0.04718302231349659\n",
      "train loss:0.11704271708059887\n",
      "train loss:0.11021338587084506\n",
      "train loss:0.03232493926459061\n",
      "train loss:0.016368094873093042\n",
      "train loss:0.11745021622546367\n",
      "train loss:0.05510579877239768\n",
      "train loss:0.08153715907838574\n",
      "train loss:0.0647581012357264\n",
      "train loss:0.04650478983257626\n",
      "train loss:0.08453233531650621\n",
      "train loss:0.09792760231658137\n",
      "train loss:0.04872693344430732\n",
      "train loss:0.13134399690170465\n",
      "train loss:0.07527442499230559\n",
      "train loss:0.034641315826543354\n",
      "train loss:0.09659555183999949\n",
      "train loss:0.04496059550044401\n",
      "train loss:0.032156150314951185\n",
      "train loss:0.07101389766371068\n",
      "train loss:0.02286509012143314\n",
      "train loss:0.10442315078521539\n",
      "train loss:0.07273854569890316\n",
      "train loss:0.05097796270676989\n",
      "train loss:0.3107647985746137\n",
      "train loss:0.10554704217124332\n",
      "train loss:0.19825722093855722\n",
      "train loss:0.12178747133872755\n",
      "train loss:0.12638446386331537\n",
      "train loss:0.06959764740674665\n",
      "train loss:0.11894945465602051\n",
      "train loss:0.09651357125965766\n",
      "train loss:0.1109549868066498\n",
      "train loss:0.08251305072858534\n",
      "train loss:0.2098581973759475\n",
      "train loss:0.05462724829722139\n",
      "train loss:0.07757973090436737\n",
      "train loss:0.09351200764623403\n",
      "train loss:0.07316448273397232\n",
      "train loss:0.07547160274026771\n",
      "train loss:0.042231897763396066\n",
      "train loss:0.07708903226966701\n",
      "train loss:0.08556564150981563\n",
      "train loss:0.09539362985557415\n",
      "train loss:0.052243911816948846\n",
      "train loss:0.23883814398205414\n",
      "train loss:0.13129581842976715\n",
      "train loss:0.05344148262585641\n",
      "train loss:0.09774264029600647\n",
      "train loss:0.07269597353958591\n",
      "train loss:0.050425011183255875\n",
      "train loss:0.0969436052542497\n",
      "train loss:0.12177852900923217\n",
      "train loss:0.06639602658084776\n",
      "train loss:0.08328475787623163\n",
      "train loss:0.09029804286655971\n",
      "train loss:0.029148650890014314\n",
      "train loss:0.049578839358176606\n",
      "train loss:0.08518038548631716\n",
      "train loss:0.12844052930137945\n",
      "train loss:0.07873472117579848\n",
      "train loss:0.0727953082964874\n",
      "train loss:0.05615749935406809\n",
      "train loss:0.04930633340297105\n",
      "train loss:0.061098356701900586\n",
      "train loss:0.0835025076944069\n",
      "train loss:0.02337189457904111\n",
      "train loss:0.0392679579468378\n",
      "train loss:0.10656697178963842\n",
      "train loss:0.07775521133658986\n",
      "train loss:0.05961387259753139\n",
      "train loss:0.06546638085490257\n",
      "train loss:0.08077074013395824\n",
      "train loss:0.06499474158671732\n",
      "train loss:0.09094386463224097\n",
      "train loss:0.05425230723010497\n",
      "train loss:0.039582851086126226\n",
      "train loss:0.052979622498271345\n",
      "train loss:0.07009349267112404\n",
      "train loss:0.041905081325049734\n",
      "train loss:0.09491902985273287\n",
      "train loss:0.08200842456126639\n",
      "train loss:0.08517520990424059\n",
      "train loss:0.06787480988603406\n",
      "train loss:0.03755115420087251\n",
      "train loss:0.04083132594217597\n",
      "train loss:0.020527691807740677\n",
      "train loss:0.04163387431484296\n",
      "train loss:0.045537060469590995\n",
      "train loss:0.026188350974914308\n",
      "train loss:0.07778273299939756\n",
      "train loss:0.08224170831959304\n",
      "train loss:0.050984444857380525\n",
      "train loss:0.08014484568870611\n",
      "train loss:0.09508728287619445\n",
      "train loss:0.10331860298860812\n",
      "train loss:0.029983426936245082\n",
      "train loss:0.08289871739608824\n",
      "=== epoch:3, train acc:0.977, test acc:0.977 ===\n",
      "train loss:0.04486025900022949\n",
      "train loss:0.06943542252741026\n",
      "train loss:0.13272709271773617\n",
      "train loss:0.059826939178887255\n",
      "train loss:0.08872540994815259\n",
      "train loss:0.12901030699004415\n",
      "train loss:0.07228541726507423\n",
      "train loss:0.10427444277621828\n",
      "train loss:0.03763903375950591\n",
      "train loss:0.0562461408300158\n",
      "train loss:0.14412868922514593\n",
      "train loss:0.12417254015414896\n",
      "train loss:0.027784661910451685\n",
      "train loss:0.09579906870737645\n",
      "train loss:0.11138915949970311\n",
      "train loss:0.07492873561324111\n",
      "train loss:0.06966532560444812\n",
      "train loss:0.09922607595559572\n",
      "train loss:0.05385902644838647\n",
      "train loss:0.1363302204329236\n",
      "train loss:0.06918910381405445\n",
      "train loss:0.09422678482641933\n",
      "train loss:0.09341747878768851\n",
      "train loss:0.1535204940273654\n",
      "train loss:0.043864685641219134\n",
      "train loss:0.0843717297572258\n",
      "train loss:0.0394890256692044\n",
      "train loss:0.09095834737598606\n",
      "train loss:0.10237139069532622\n",
      "train loss:0.06591069695553022\n",
      "train loss:0.0543160529535917\n",
      "train loss:0.09256665267173471\n",
      "train loss:0.0679277979326105\n",
      "train loss:0.05838134872447087\n",
      "train loss:0.050354810682927395\n",
      "train loss:0.061184952325866335\n",
      "train loss:0.16988158244191023\n",
      "train loss:0.0863149858280779\n",
      "train loss:0.03509638875353654\n",
      "train loss:0.059173720473050605\n",
      "train loss:0.024005056064467917\n",
      "train loss:0.040982387533792554\n",
      "train loss:0.08515705417221131\n",
      "train loss:0.02386536218282043\n",
      "train loss:0.09180633083876993\n",
      "train loss:0.020508905659423995\n",
      "train loss:0.041617864717310014\n",
      "train loss:0.09156351668654059\n",
      "train loss:0.34629160782923923\n",
      "train loss:0.06733400947365908\n",
      "train loss:0.012969506966915427\n",
      "train loss:0.09102457144130133\n",
      "train loss:0.022579229088962167\n",
      "train loss:0.05412542058571582\n",
      "train loss:0.0253783186331107\n",
      "train loss:0.05539692631581592\n",
      "train loss:0.01986957624874983\n",
      "train loss:0.1434488334369963\n",
      "train loss:0.029286838870332788\n",
      "train loss:0.030535005164112738\n",
      "train loss:0.036743042723370166\n",
      "train loss:0.1529653144955746\n",
      "train loss:0.04537588855837285\n",
      "train loss:0.05084347672588559\n",
      "train loss:0.08539248571461816\n",
      "train loss:0.045460749274208505\n",
      "train loss:0.08362752849292572\n",
      "train loss:0.08333488248322\n",
      "train loss:0.10185029173037895\n",
      "train loss:0.043564705455722864\n",
      "train loss:0.06266888492224387\n",
      "train loss:0.08733677919979162\n",
      "train loss:0.12456313659217491\n",
      "train loss:0.0523177362607539\n",
      "train loss:0.11383567257753816\n",
      "train loss:0.01535783043283787\n",
      "train loss:0.034331818477960366\n",
      "train loss:0.07966008858534164\n",
      "train loss:0.02498710992585853\n",
      "train loss:0.047171136135753675\n",
      "train loss:0.05322662098397757\n",
      "train loss:0.05479789579359195\n",
      "train loss:0.05792936526196765\n",
      "train loss:0.11409105785911146\n",
      "train loss:0.04833613150325815\n",
      "train loss:0.12508097567089677\n",
      "train loss:0.03341146901617934\n",
      "train loss:0.024493852568492475\n",
      "train loss:0.05800569410515233\n",
      "train loss:0.04317778554112509\n",
      "train loss:0.06014421756833619\n",
      "train loss:0.05132199717852825\n",
      "train loss:0.11499355910620744\n",
      "train loss:0.06667717924089843\n",
      "train loss:0.05331302707917812\n",
      "train loss:0.07178724359926889\n",
      "train loss:0.031601510603425205\n",
      "train loss:0.13280995117619662\n",
      "train loss:0.038063637905108076\n",
      "train loss:0.03823368465351602\n",
      "train loss:0.018955739760751055\n",
      "train loss:0.0727720329370015\n",
      "train loss:0.02050718009553308\n",
      "train loss:0.02516250652823564\n",
      "train loss:0.025321761087445615\n",
      "train loss:0.060153487519581786\n",
      "train loss:0.05065156926437192\n",
      "train loss:0.08419619122246927\n",
      "train loss:0.044857760796252526\n",
      "train loss:0.028760287636679382\n",
      "train loss:0.03832881599160252\n",
      "train loss:0.03933909213982238\n",
      "train loss:0.05984807366322792\n",
      "train loss:0.05636496516812156\n",
      "train loss:0.1378104686350607\n",
      "train loss:0.06383330436175315\n",
      "train loss:0.024626731795501743\n",
      "train loss:0.06331232787904995\n",
      "train loss:0.09385877492322342\n",
      "train loss:0.10513087639256431\n",
      "train loss:0.033838475949824504\n",
      "train loss:0.12480645737309948\n",
      "train loss:0.0692521660695927\n",
      "train loss:0.1315314397707723\n",
      "train loss:0.0443097014802776\n",
      "train loss:0.0280735861864859\n",
      "train loss:0.05954107187718399\n",
      "train loss:0.020784826818099435\n",
      "train loss:0.041984453116474016\n",
      "train loss:0.03763062892034334\n",
      "train loss:0.03640357786895164\n",
      "train loss:0.14670513560760812\n",
      "train loss:0.06880733805677462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.059291286607921316\n",
      "train loss:0.07593485961640878\n",
      "train loss:0.022421651116405917\n",
      "train loss:0.10150499943690622\n",
      "train loss:0.06070609813559007\n",
      "train loss:0.0591523776183071\n",
      "train loss:0.07551550101698379\n",
      "train loss:0.05405587797058722\n",
      "train loss:0.031371962296379015\n",
      "train loss:0.04262115948727565\n",
      "train loss:0.026170497130975984\n",
      "train loss:0.09829336687572597\n",
      "train loss:0.018646720767946337\n",
      "train loss:0.06700403185874727\n",
      "train loss:0.09807118171571623\n",
      "train loss:0.07116789801027144\n",
      "train loss:0.075952293697131\n",
      "train loss:0.0878340166855691\n",
      "train loss:0.0255375956966281\n",
      "train loss:0.03890185864430904\n",
      "train loss:0.011201081900147711\n",
      "train loss:0.02972000367759528\n",
      "train loss:0.08877585394525488\n",
      "train loss:0.07770714057586094\n",
      "train loss:0.05131402308556663\n",
      "train loss:0.027305838494803055\n",
      "train loss:0.0475781533681065\n",
      "train loss:0.040668117132825005\n",
      "train loss:0.057350164283561836\n",
      "train loss:0.06126669453877781\n",
      "train loss:0.04233251067025567\n",
      "train loss:0.0995542306878089\n",
      "train loss:0.13093232887725412\n",
      "train loss:0.018760314378183252\n",
      "train loss:0.09628771199467011\n",
      "train loss:0.08803976365756723\n",
      "train loss:0.07915241857246808\n",
      "train loss:0.08715255138220421\n",
      "train loss:0.0905835933677042\n",
      "train loss:0.05510663397135909\n",
      "train loss:0.06384233906370072\n",
      "train loss:0.06808574909585856\n",
      "train loss:0.17114830667740896\n",
      "train loss:0.03392002643616509\n",
      "train loss:0.039860756974605706\n",
      "train loss:0.08578943465640985\n",
      "train loss:0.19600673822437986\n",
      "train loss:0.08899335544583264\n",
      "train loss:0.09498304888031704\n",
      "train loss:0.08684005672123239\n",
      "train loss:0.01807052391289265\n",
      "train loss:0.07911898537389335\n",
      "train loss:0.026001579937939147\n",
      "train loss:0.07707012695574274\n",
      "train loss:0.07472912731746119\n",
      "train loss:0.08720428151240721\n",
      "train loss:0.021123111254263\n",
      "train loss:0.06649386899092985\n",
      "train loss:0.08396015105792991\n",
      "train loss:0.11686311369848776\n",
      "train loss:0.08541023417382673\n",
      "train loss:0.054908538249450904\n",
      "train loss:0.128396652304404\n",
      "train loss:0.06470818513826124\n",
      "train loss:0.019601712922114278\n",
      "train loss:0.03565751820486267\n",
      "train loss:0.09570089492045167\n",
      "train loss:0.04946620852535495\n",
      "train loss:0.042785165604408947\n",
      "train loss:0.07549701370598436\n",
      "train loss:0.04028288541509824\n",
      "train loss:0.0863616484165669\n",
      "train loss:0.04204441480550391\n",
      "train loss:0.057733620978247996\n",
      "train loss:0.15445252006172994\n",
      "train loss:0.020648718260939263\n",
      "train loss:0.04702415486106885\n",
      "train loss:0.031240880473909858\n",
      "train loss:0.12882124261758135\n",
      "train loss:0.08754753531142631\n",
      "train loss:0.07396217336962078\n",
      "train loss:0.0677393426650026\n",
      "train loss:0.07405837706571623\n",
      "train loss:0.0356700742830516\n",
      "train loss:0.0709462176063015\n",
      "train loss:0.052084168642143595\n",
      "train loss:0.06305120549550942\n",
      "train loss:0.0713871961294968\n",
      "train loss:0.03667640232415238\n",
      "train loss:0.024819001444590812\n",
      "train loss:0.020518653476776838\n",
      "train loss:0.03216987058997753\n",
      "train loss:0.0558638632996361\n",
      "train loss:0.04587585706824215\n",
      "train loss:0.04684588748184638\n",
      "train loss:0.07039458015462276\n",
      "train loss:0.037059586532910214\n",
      "train loss:0.07650439810391103\n",
      "train loss:0.08122039370612039\n",
      "train loss:0.09472624864680958\n",
      "train loss:0.04084722562056342\n",
      "train loss:0.03872444162911333\n",
      "train loss:0.07342601954024308\n",
      "train loss:0.055997636460029476\n",
      "train loss:0.16300122035725326\n",
      "train loss:0.04894106649925122\n",
      "train loss:0.07667816675026896\n",
      "train loss:0.062218485953944296\n",
      "train loss:0.06077694176779192\n",
      "train loss:0.0987701374941733\n",
      "train loss:0.017691169107264034\n",
      "train loss:0.033751183204667023\n",
      "train loss:0.10488308782533438\n",
      "train loss:0.028710320512668894\n",
      "train loss:0.05443424935706272\n",
      "train loss:0.031170308964821994\n",
      "train loss:0.025898153889681676\n",
      "train loss:0.06452162461474016\n",
      "train loss:0.040759873768006424\n",
      "train loss:0.035047306993524444\n",
      "train loss:0.07435739075192487\n",
      "train loss:0.08454526694627228\n",
      "train loss:0.04022163116290921\n",
      "train loss:0.015632271750758802\n",
      "train loss:0.07006224787285356\n",
      "train loss:0.0376576233601343\n",
      "train loss:0.030686211387747873\n",
      "train loss:0.03865150452476309\n",
      "train loss:0.08127981916524213\n",
      "train loss:0.08013733300992623\n",
      "train loss:0.023513950739113852\n",
      "train loss:0.04942100046877776\n",
      "train loss:0.010548145137873461\n",
      "train loss:0.0448480162953615\n",
      "train loss:0.03590227662670411\n",
      "train loss:0.12245197241853507\n",
      "train loss:0.07191276929426321\n",
      "train loss:0.04902574152373942\n",
      "train loss:0.06661512956669038\n",
      "train loss:0.1984696347426405\n",
      "train loss:0.03394971219276474\n",
      "train loss:0.05134913325869613\n",
      "train loss:0.07384944302485424\n",
      "train loss:0.02198385270222856\n",
      "train loss:0.04659184175908525\n",
      "train loss:0.03915855441151634\n",
      "train loss:0.1105330451324741\n",
      "train loss:0.05478620656216528\n",
      "train loss:0.07135851651959212\n",
      "train loss:0.07546025573988252\n",
      "train loss:0.07661399224682387\n",
      "train loss:0.08256840771667111\n",
      "train loss:0.1150272617255742\n",
      "train loss:0.0517022240504176\n",
      "train loss:0.029694504306611434\n",
      "train loss:0.13065889799719854\n",
      "train loss:0.033951522444042516\n",
      "train loss:0.032339290980801745\n",
      "train loss:0.06136698109366894\n",
      "train loss:0.053922400354919374\n",
      "train loss:0.058982267890566875\n",
      "train loss:0.10273863498107999\n",
      "train loss:0.04537163906472394\n",
      "train loss:0.07213216285046697\n",
      "train loss:0.06520710710876867\n",
      "train loss:0.02344092577141329\n",
      "train loss:0.027852632464568403\n",
      "train loss:0.061470070827883534\n",
      "train loss:0.050245376273174276\n",
      "train loss:0.14068624774114669\n",
      "train loss:0.03325773285298255\n",
      "train loss:0.053490810277182915\n",
      "train loss:0.03251781140470917\n",
      "train loss:0.04586073166035697\n",
      "train loss:0.06442896444750582\n",
      "train loss:0.033261740000661386\n",
      "train loss:0.02146515851635045\n",
      "train loss:0.03556611543392962\n",
      "train loss:0.0356512685939926\n",
      "train loss:0.04337634018414002\n",
      "train loss:0.05635861222566245\n",
      "train loss:0.03712506962194903\n",
      "train loss:0.0820747546137982\n",
      "train loss:0.047380675298796235\n",
      "train loss:0.028608517554248047\n",
      "train loss:0.05768075484572313\n",
      "train loss:0.06333870402583688\n",
      "train loss:0.07002939289105164\n",
      "train loss:0.03437672642371891\n",
      "train loss:0.03386110967719397\n",
      "train loss:0.058721397059465615\n",
      "train loss:0.025309150084036353\n",
      "train loss:0.02260624748096382\n",
      "train loss:0.044404562644304146\n",
      "train loss:0.032070335919012975\n",
      "train loss:0.08947042350850357\n",
      "train loss:0.005118551541771796\n",
      "train loss:0.0268601639089965\n",
      "train loss:0.04428856223761648\n",
      "train loss:0.03519929761830123\n",
      "train loss:0.05196868446910696\n",
      "train loss:0.11538591940246958\n",
      "train loss:0.02338857687897987\n",
      "train loss:0.04538014885497587\n",
      "train loss:0.01728037101314276\n",
      "train loss:0.0901380880670291\n",
      "train loss:0.141914170117168\n",
      "train loss:0.09189362887274095\n",
      "train loss:0.05517225114945557\n",
      "train loss:0.0218628489545021\n",
      "train loss:0.06783926558338454\n",
      "train loss:0.028731971910525088\n",
      "train loss:0.10194647163535457\n",
      "train loss:0.07175142318173011\n",
      "train loss:0.031628295326748795\n",
      "train loss:0.15550932345127552\n",
      "train loss:0.042574045793167335\n",
      "train loss:0.035775755410655335\n",
      "train loss:0.03728496587682939\n",
      "train loss:0.05384279467254071\n",
      "train loss:0.040508879450074795\n",
      "train loss:0.039692805882181124\n",
      "train loss:0.055533957041412424\n",
      "train loss:0.0374809851938359\n",
      "train loss:0.031752123466210286\n",
      "train loss:0.11940173601198271\n",
      "train loss:0.05119845579880611\n",
      "train loss:0.06372703538025745\n",
      "train loss:0.04211429297981379\n",
      "train loss:0.03417055554443806\n",
      "train loss:0.02429905182058359\n",
      "train loss:0.08776963242480285\n",
      "train loss:0.026615620809264197\n",
      "train loss:0.02037526507965667\n",
      "train loss:0.049678798197167244\n",
      "train loss:0.012285978664793234\n",
      "train loss:0.044286316530743226\n",
      "train loss:0.07855549654814302\n",
      "train loss:0.034121101823308846\n",
      "train loss:0.028645498166226174\n",
      "train loss:0.0459345714148919\n",
      "train loss:0.05870126556642316\n",
      "train loss:0.021128343836457848\n",
      "train loss:0.06465222473116443\n",
      "train loss:0.041026512492940245\n",
      "train loss:0.04799912723576929\n",
      "train loss:0.036031040002966204\n",
      "train loss:0.0706201785034577\n",
      "train loss:0.041920791600531374\n",
      "train loss:0.06800591166004828\n",
      "train loss:0.06382241720347843\n",
      "train loss:0.04967428385737134\n",
      "train loss:0.09406713332357088\n",
      "train loss:0.11242996306131539\n",
      "train loss:0.08307994555373374\n",
      "train loss:0.04917334123316965\n",
      "train loss:0.06999513555651173\n",
      "train loss:0.052384533045057254\n",
      "train loss:0.03663129514367392\n",
      "train loss:0.09291908866233926\n",
      "train loss:0.0251457801003291\n",
      "train loss:0.0473766847443744\n",
      "train loss:0.05438643076515631\n",
      "train loss:0.07411034903666515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.036187364923127255\n",
      "train loss:0.022108954339378758\n",
      "train loss:0.06326868259422917\n",
      "train loss:0.052416119065149515\n",
      "train loss:0.05460162057172249\n",
      "train loss:0.012891160133594998\n",
      "train loss:0.07874054469005078\n",
      "train loss:0.061448738754131114\n",
      "train loss:0.05749944228828117\n",
      "train loss:0.07177957346167729\n",
      "train loss:0.046410055509469854\n",
      "train loss:0.04047909589696535\n",
      "train loss:0.017823578949007948\n",
      "train loss:0.03511375701051233\n",
      "train loss:0.10088934165517824\n",
      "train loss:0.03860595051509213\n",
      "train loss:0.14000418728262873\n",
      "train loss:0.07280378822078987\n",
      "train loss:0.013039376904375842\n",
      "train loss:0.01645316758251548\n",
      "train loss:0.069853467580017\n",
      "train loss:0.031202365145933345\n",
      "train loss:0.047452345953056134\n",
      "train loss:0.0515005723566767\n",
      "train loss:0.018048597853322402\n",
      "train loss:0.022784043836230148\n",
      "train loss:0.04766728247141739\n",
      "train loss:0.04276098419837619\n",
      "train loss:0.023990344103312196\n",
      "train loss:0.012497704627407753\n",
      "train loss:0.07557695403018863\n",
      "train loss:0.04089505196002457\n",
      "train loss:0.04964123759765325\n",
      "train loss:0.11812456118783393\n",
      "train loss:0.011456758719286773\n",
      "train loss:0.0634869874938028\n",
      "train loss:0.03890162400235213\n",
      "train loss:0.04192693289388018\n",
      "train loss:0.053542562165098645\n",
      "train loss:0.07449799478290352\n",
      "train loss:0.1647808378702486\n",
      "train loss:0.08395200632202451\n",
      "train loss:0.020795814835290947\n",
      "train loss:0.0252548744987988\n",
      "train loss:0.08871995938819864\n",
      "train loss:0.02135110397132808\n",
      "train loss:0.0814037950005087\n",
      "train loss:0.04167701525227952\n",
      "train loss:0.028052042412698767\n",
      "train loss:0.0699042824723048\n",
      "train loss:0.017450136201648706\n",
      "train loss:0.10932994214675416\n",
      "train loss:0.004616011161813313\n",
      "train loss:0.013527797942550164\n",
      "train loss:0.07692117747612955\n",
      "train loss:0.018697079224383448\n",
      "train loss:0.13945196707295918\n",
      "train loss:0.02806483871222819\n",
      "train loss:0.05239608490595215\n",
      "train loss:0.09780860075036889\n",
      "train loss:0.057950788535152606\n",
      "train loss:0.04205776886069245\n",
      "train loss:0.030474246458998086\n",
      "train loss:0.019043777248061684\n",
      "train loss:0.04353764705962408\n",
      "train loss:0.06346941352171832\n",
      "train loss:0.0659998486623953\n",
      "train loss:0.06487820927111738\n",
      "train loss:0.05174087056402466\n",
      "train loss:0.14129488041292199\n",
      "train loss:0.04638730625832884\n",
      "train loss:0.0324095547012914\n",
      "train loss:0.03957561534107822\n",
      "train loss:0.06446462144342482\n",
      "train loss:0.04984902535615218\n",
      "train loss:0.01906873477197111\n",
      "train loss:0.04400030848575617\n",
      "train loss:0.04491337107439457\n",
      "train loss:0.036434145800551036\n",
      "train loss:0.0688300124676465\n",
      "train loss:0.09554577476230736\n",
      "train loss:0.04045474098699353\n",
      "train loss:0.1400944806910954\n",
      "train loss:0.021692568152741676\n",
      "train loss:0.027867424079231155\n",
      "train loss:0.0670021083410042\n",
      "train loss:0.017944483931832067\n",
      "train loss:0.0583471842056801\n",
      "train loss:0.013122057515431013\n",
      "train loss:0.05480890379920871\n",
      "train loss:0.03972387490549526\n",
      "train loss:0.019005354606292612\n",
      "train loss:0.0770630452738117\n",
      "train loss:0.03952829861039519\n",
      "train loss:0.0323517205521235\n",
      "train loss:0.039307128018191584\n",
      "train loss:0.03563608305454682\n",
      "train loss:0.043253190865959246\n",
      "train loss:0.024851622163875958\n",
      "train loss:0.04041404991667902\n",
      "train loss:0.05571261669942999\n",
      "train loss:0.020175670680075763\n",
      "train loss:0.08776251772074799\n",
      "train loss:0.02552907156669134\n",
      "train loss:0.0217883736707288\n",
      "train loss:0.03828060881845268\n",
      "train loss:0.06887138649710274\n",
      "train loss:0.03810689461600883\n",
      "train loss:0.03238696805467229\n",
      "train loss:0.08036612434083405\n",
      "train loss:0.044434556461765154\n",
      "train loss:0.09161972080933642\n",
      "train loss:0.05339952789474942\n",
      "train loss:0.08541324641999137\n",
      "train loss:0.07671768140517092\n",
      "train loss:0.018249863823990024\n",
      "train loss:0.06293431492927584\n",
      "train loss:0.01719653793459522\n",
      "train loss:0.018143238957828496\n",
      "train loss:0.03972766513516148\n",
      "train loss:0.016609322157222904\n",
      "train loss:0.05829048400795671\n",
      "train loss:0.12233221728367898\n",
      "train loss:0.08094934130202619\n",
      "train loss:0.03388009747726405\n",
      "train loss:0.011462714303882841\n",
      "train loss:0.03520573987275348\n",
      "train loss:0.020400311979983172\n",
      "train loss:0.01682820671275421\n",
      "train loss:0.0520479554916161\n",
      "train loss:0.050462091008414756\n",
      "train loss:0.007537039779353121\n",
      "train loss:0.028219594295172734\n",
      "train loss:0.01658914595336057\n",
      "train loss:0.09047617453457457\n",
      "train loss:0.021422281411619788\n",
      "train loss:0.06942961974382539\n",
      "train loss:0.027710575847526217\n",
      "train loss:0.08842516003739989\n",
      "train loss:0.0365259359267063\n",
      "train loss:0.022469476108645196\n",
      "train loss:0.034617248910283435\n",
      "train loss:0.07313218910459804\n",
      "train loss:0.1096894125477688\n",
      "train loss:0.02752757582511327\n",
      "train loss:0.05329122491695659\n",
      "train loss:0.12959159650505378\n",
      "train loss:0.12482995103585617\n",
      "train loss:0.028588187801699853\n",
      "train loss:0.034186983737319314\n",
      "train loss:0.03131264234817444\n",
      "train loss:0.09409809479753684\n",
      "train loss:0.06571645287430297\n",
      "train loss:0.09417181210019379\n",
      "train loss:0.04239093715012521\n",
      "train loss:0.045443844580778874\n",
      "train loss:0.04056417568232359\n",
      "train loss:0.044729941946586325\n",
      "train loss:0.04556204576507353\n",
      "train loss:0.02577843314014358\n",
      "train loss:0.03340152791421727\n",
      "train loss:0.04084829891814554\n",
      "train loss:0.05346441127190864\n",
      "train loss:0.08642923285660638\n",
      "train loss:0.031039970743870316\n",
      "train loss:0.05516654097019764\n",
      "train loss:0.021613405876122656\n",
      "train loss:0.05501952768390533\n",
      "train loss:0.08827071032636835\n",
      "train loss:0.029534069208391966\n",
      "train loss:0.028847344001298344\n",
      "train loss:0.04697808679190993\n",
      "train loss:0.05788309954024037\n",
      "train loss:0.045953211673238244\n",
      "train loss:0.022672306868354378\n",
      "train loss:0.062375249000308546\n",
      "train loss:0.049025316462238046\n",
      "train loss:0.052235268701442544\n",
      "train loss:0.029043964584602882\n",
      "train loss:0.020726069282719597\n",
      "train loss:0.00866372552674093\n",
      "train loss:0.019030970646365487\n",
      "train loss:0.07171953536206938\n",
      "train loss:0.01535204823012808\n",
      "train loss:0.04611153693382634\n",
      "train loss:0.060352263056698136\n",
      "train loss:0.08444671391410039\n",
      "train loss:0.014705305831272206\n",
      "train loss:0.09887692288880656\n",
      "train loss:0.02703811740017375\n",
      "train loss:0.05101447381560963\n",
      "train loss:0.01942469609806211\n",
      "train loss:0.04979543046149606\n",
      "train loss:0.05959607117776464\n",
      "train loss:0.042489496854284645\n",
      "train loss:0.010794382208499717\n",
      "train loss:0.09242487520671737\n",
      "train loss:0.01130714436659224\n",
      "train loss:0.06704954251479664\n",
      "train loss:0.016782187545555258\n",
      "train loss:0.07271557897065112\n",
      "train loss:0.07195162489771198\n",
      "train loss:0.03372947758149118\n",
      "=== epoch:4, train acc:0.982, test acc:0.988 ===\n",
      "train loss:0.04667118952362241\n",
      "train loss:0.0753268922293239\n",
      "train loss:0.0861167216830302\n",
      "train loss:0.05164356748352874\n",
      "train loss:0.03869073776757359\n",
      "train loss:0.025985784230779227\n",
      "train loss:0.08603041113570623\n",
      "train loss:0.047371780432235194\n",
      "train loss:0.06204382801595828\n",
      "train loss:0.03378531615123041\n",
      "train loss:0.043675748477665664\n",
      "train loss:0.03123075255230269\n",
      "train loss:0.0518472532187257\n",
      "train loss:0.08285844956546523\n",
      "train loss:0.04181498882968743\n",
      "train loss:0.07935853935888501\n",
      "train loss:0.01640880569956232\n",
      "train loss:0.08036329316512303\n",
      "train loss:0.03027565002515125\n",
      "train loss:0.021485096122789562\n",
      "train loss:0.04839250116915795\n",
      "train loss:0.048220639916670124\n",
      "train loss:0.010720666336530842\n",
      "train loss:0.02880963850865662\n",
      "train loss:0.031487061440494936\n",
      "train loss:0.03081430690043228\n",
      "train loss:0.022290444110035567\n",
      "train loss:0.09339540692295739\n",
      "train loss:0.036824274547225914\n",
      "train loss:0.04753745804142818\n",
      "train loss:0.02424767503350375\n",
      "train loss:0.09231461360859877\n",
      "train loss:0.01945056156016618\n",
      "train loss:0.056228620616710065\n",
      "train loss:0.040480341961897946\n",
      "train loss:0.032456584130175006\n",
      "train loss:0.030522930789569296\n",
      "train loss:0.04591212325386066\n",
      "train loss:0.03971562275992122\n",
      "train loss:0.046945541283536237\n",
      "train loss:0.021869359492835656\n",
      "train loss:0.05022680606218292\n",
      "train loss:0.0658885424937615\n",
      "train loss:0.05516463164063689\n",
      "train loss:0.06347416717879166\n",
      "train loss:0.12109060270102874\n",
      "train loss:0.02824030030364799\n",
      "train loss:0.05995591653882278\n",
      "train loss:0.01313910675523189\n",
      "train loss:0.030557194293964533\n",
      "train loss:0.015324836490296666\n",
      "train loss:0.05911180497088138\n",
      "train loss:0.0403000004818008\n",
      "train loss:0.008618682786069318\n",
      "train loss:0.025200026787105586\n",
      "train loss:0.020418904931830476\n",
      "train loss:0.05299962301155923\n",
      "train loss:0.08810840496113607\n",
      "train loss:0.03306673200430208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.05920494570327461\n",
      "train loss:0.05692990154419288\n",
      "train loss:0.0635583221137956\n",
      "train loss:0.033456192363875786\n",
      "train loss:0.02388289578086062\n",
      "train loss:0.020679786536623737\n",
      "train loss:0.092756373756381\n",
      "train loss:0.03819308009422792\n",
      "train loss:0.03894487952793381\n",
      "train loss:0.008480021773271725\n",
      "train loss:0.021365842991015592\n",
      "train loss:0.03774656644385329\n",
      "train loss:0.12784300186347192\n",
      "train loss:0.04510761662004107\n",
      "train loss:0.08699724381411704\n",
      "train loss:0.042436004113932564\n",
      "train loss:0.04096443865071192\n",
      "train loss:0.10216532456054103\n",
      "train loss:0.039788017332401476\n",
      "train loss:0.06419905915185664\n",
      "train loss:0.013319235721162002\n",
      "train loss:0.07352558044069282\n",
      "train loss:0.04216694354103674\n",
      "train loss:0.06552080926190901\n",
      "train loss:0.03905321117332844\n",
      "train loss:0.053572309731488826\n",
      "train loss:0.07235956405812505\n",
      "train loss:0.04627329560172524\n",
      "train loss:0.043246699449823026\n",
      "train loss:0.055427594150081465\n",
      "train loss:0.03982223705637876\n",
      "train loss:0.031405424639058765\n",
      "train loss:0.01100309851970069\n",
      "train loss:0.031368331734553105\n",
      "train loss:0.04427651047146064\n",
      "train loss:0.05539979336845823\n",
      "train loss:0.08320718419982523\n",
      "train loss:0.03542274175742021\n",
      "train loss:0.011762804099390449\n",
      "train loss:0.020565302614470437\n",
      "train loss:0.0706878628441349\n",
      "train loss:0.03306150853552759\n",
      "train loss:0.0669154353911847\n",
      "train loss:0.015142446064819714\n",
      "train loss:0.011941963760326867\n",
      "train loss:0.03373648826618188\n",
      "train loss:0.04548159788250186\n",
      "train loss:0.029339452520701116\n",
      "train loss:0.018360345771501748\n",
      "train loss:0.050858665232906955\n",
      "train loss:0.03448518064216692\n",
      "train loss:0.017750542888560948\n",
      "train loss:0.059384582883445274\n",
      "train loss:0.04152282774856499\n",
      "train loss:0.01881054906157091\n",
      "train loss:0.0870233625280369\n",
      "train loss:0.01222398898319067\n",
      "train loss:0.009279245264196597\n",
      "train loss:0.009271772370281149\n",
      "train loss:0.12635583201846523\n",
      "train loss:0.036280024302688105\n",
      "train loss:0.042194573192841006\n",
      "train loss:0.03990635255260404\n",
      "train loss:0.027654172013517685\n",
      "train loss:0.03166063684144426\n",
      "train loss:0.04195580821689172\n",
      "train loss:0.06137719576899184\n",
      "train loss:0.020913679420879645\n",
      "train loss:0.02999996175875452\n",
      "train loss:0.026231824159957853\n",
      "train loss:0.03773049061326396\n",
      "train loss:0.0539116205479695\n",
      "train loss:0.05834660963488158\n",
      "train loss:0.05913957991221073\n",
      "train loss:0.03042726939975049\n",
      "train loss:0.03778754856932902\n",
      "train loss:0.04262094227606322\n",
      "train loss:0.023700232680100585\n",
      "train loss:0.06636489156536836\n",
      "train loss:0.06084053112272838\n",
      "train loss:0.020078136597135025\n",
      "train loss:0.030025769272838163\n",
      "train loss:0.019033677510779456\n",
      "train loss:0.015852244251032342\n",
      "train loss:0.036855197905340455\n",
      "train loss:0.05083071780302835\n",
      "train loss:0.015380866198324508\n",
      "train loss:0.07411047135023723\n",
      "train loss:0.023551149336093133\n",
      "train loss:0.07341533100341607\n",
      "train loss:0.0459869237595108\n",
      "train loss:0.011983113904287688\n",
      "train loss:0.04267489109284993\n",
      "train loss:0.037928648669865644\n",
      "train loss:0.028481720713240895\n",
      "train loss:0.016256537259061694\n",
      "train loss:0.011524346469288717\n",
      "train loss:0.018743228025762507\n",
      "train loss:0.09754467270503626\n",
      "train loss:0.03409379468496266\n",
      "train loss:0.025019641823994944\n",
      "train loss:0.02098693926030959\n",
      "train loss:0.04408681394273736\n",
      "train loss:0.009081256648953791\n",
      "train loss:0.016608420929883117\n",
      "train loss:0.025349655153070198\n",
      "train loss:0.016109964345308654\n",
      "train loss:0.022113732781010805\n",
      "train loss:0.02063070030563806\n",
      "train loss:0.041253824679068025\n",
      "train loss:0.04050069259937722\n",
      "train loss:0.06624220709656084\n",
      "train loss:0.015647149523722512\n",
      "train loss:0.06357704224668376\n",
      "train loss:0.03055228397405256\n",
      "train loss:0.06651459336577531\n",
      "train loss:0.022543512597889434\n",
      "train loss:0.024361981657684207\n",
      "train loss:0.03118460655122897\n",
      "train loss:0.040838951384737567\n",
      "train loss:0.026144894869027574\n",
      "train loss:0.029850989673442872\n",
      "train loss:0.02714161388645932\n",
      "train loss:0.03176940788577262\n",
      "train loss:0.04960501066302567\n",
      "train loss:0.03539762700291321\n",
      "train loss:0.01818785728509401\n",
      "train loss:0.022158342374295717\n",
      "train loss:0.034146597865687786\n",
      "train loss:0.054433626663345225\n",
      "train loss:0.016091166877960906\n",
      "train loss:0.017621531676193868\n",
      "train loss:0.04068938387090599\n",
      "train loss:0.025637548512887468\n",
      "train loss:0.08769609749218399\n",
      "train loss:0.01481364163717926\n",
      "train loss:0.02406121562539643\n",
      "train loss:0.0361351175952044\n",
      "train loss:0.030949555146933178\n",
      "train loss:0.11381235396809047\n",
      "train loss:0.0297821163028538\n",
      "train loss:0.01320301713498966\n",
      "train loss:0.03497798003180656\n",
      "train loss:0.025001536184163443\n",
      "train loss:0.06387682663717248\n",
      "train loss:0.03893007084179003\n",
      "train loss:0.015779565163243105\n",
      "train loss:0.049303705835617426\n",
      "train loss:0.022205224821424983\n",
      "train loss:0.011971362312177356\n",
      "train loss:0.06378415974657024\n",
      "train loss:0.0443742178097363\n",
      "train loss:0.030194695717602395\n",
      "train loss:0.0963863110156663\n",
      "train loss:0.06155934410042698\n",
      "train loss:0.058266953839205445\n",
      "train loss:0.012002314978443028\n",
      "train loss:0.0071609563536671585\n",
      "train loss:0.09502693990382399\n",
      "train loss:0.060297141300990266\n",
      "train loss:0.03205511828922032\n",
      "train loss:0.09722919831952119\n",
      "train loss:0.029301954455275076\n",
      "train loss:0.049329162967679044\n",
      "train loss:0.07892573460811887\n",
      "train loss:0.1346861024471814\n",
      "train loss:0.1088236953588165\n",
      "train loss:0.03559715606303512\n",
      "train loss:0.0328710613426408\n",
      "train loss:0.026619216176502057\n",
      "train loss:0.03370441695691206\n",
      "train loss:0.025629718311267084\n",
      "train loss:0.011626690367888558\n",
      "train loss:0.03277284162078654\n",
      "train loss:0.009994373750034678\n",
      "train loss:0.0968063079268963\n",
      "train loss:0.05654152770482972\n",
      "train loss:0.05120403367243773\n",
      "train loss:0.08414601085224899\n",
      "train loss:0.06017322437639194\n",
      "train loss:0.015343934009215061\n",
      "train loss:0.03932447774526775\n",
      "train loss:0.15823259774771373\n",
      "train loss:0.03415873677458537\n",
      "train loss:0.0503675110082648\n",
      "train loss:0.07195937802938124\n",
      "train loss:0.042552514780421616\n",
      "train loss:0.045973724133442084\n",
      "train loss:0.02493115727833649\n",
      "train loss:0.03309658655132533\n",
      "train loss:0.014608201293671996\n",
      "train loss:0.03517404760552378\n",
      "train loss:0.03723546844902594\n",
      "train loss:0.03551875685731434\n",
      "train loss:0.05941301955734407\n",
      "train loss:0.03929258447932801\n",
      "train loss:0.048519087410512876\n",
      "train loss:0.02166184121619581\n",
      "train loss:0.009448486204879633\n",
      "train loss:0.06914152425672249\n",
      "train loss:0.056987694896110025\n",
      "train loss:0.045288448597960926\n",
      "train loss:0.05943656283452225\n",
      "train loss:0.10303150670369002\n",
      "train loss:0.0373953604174989\n",
      "train loss:0.0990567119240335\n",
      "train loss:0.02680429232596218\n",
      "train loss:0.023093346587151106\n",
      "train loss:0.05022740198442278\n",
      "train loss:0.062121615123784946\n",
      "train loss:0.0071293235651574164\n",
      "train loss:0.11117829485192793\n",
      "train loss:0.014896630795543358\n",
      "train loss:0.02955728946823802\n",
      "train loss:0.03687812999796329\n",
      "train loss:0.02847577217543808\n",
      "train loss:0.023731347878069086\n",
      "train loss:0.0458703478599724\n",
      "train loss:0.012002003872034374\n",
      "train loss:0.04022019950872007\n",
      "train loss:0.07151496978733322\n",
      "train loss:0.040415707493590525\n",
      "train loss:0.014608130718057784\n",
      "train loss:0.028328683392125333\n",
      "train loss:0.01359520675389838\n",
      "train loss:0.01189480805067097\n",
      "train loss:0.08566131215160205\n",
      "train loss:0.011950786727251276\n",
      "train loss:0.024647579421221307\n",
      "train loss:0.03480744267257146\n",
      "train loss:0.022922922454745417\n",
      "train loss:0.04950896811484157\n",
      "train loss:0.011889056737866327\n",
      "train loss:0.011899579370556657\n",
      "train loss:0.06701822861517942\n",
      "train loss:0.03331036568622872\n",
      "train loss:0.02187226081443718\n",
      "train loss:0.1285956437907692\n",
      "train loss:0.05330875857341768\n",
      "train loss:0.10762135753141341\n",
      "train loss:0.02181346558692914\n",
      "train loss:0.012301983601124715\n",
      "train loss:0.12231064194547038\n",
      "train loss:0.025276425734174407\n",
      "train loss:0.01167343756153079\n",
      "train loss:0.05450122558761752\n",
      "train loss:0.017643026390722117\n",
      "train loss:0.02842716799515423\n",
      "train loss:0.018204915317863623\n",
      "train loss:0.03863022010368513\n",
      "train loss:0.02438187058954691\n",
      "train loss:0.0492053151430949\n",
      "train loss:0.013375723023044388\n",
      "train loss:0.034880755798798485\n",
      "train loss:0.0969633682456232\n",
      "train loss:0.026380325517206798\n",
      "train loss:0.08417119115552796\n",
      "train loss:0.017113675152651324\n",
      "train loss:0.04820072104337531\n",
      "train loss:0.08115501947717357\n",
      "train loss:0.09027175646516854\n",
      "train loss:0.025594755007367218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06223402784030565\n",
      "train loss:0.03290480931562982\n",
      "train loss:0.05130508260683535\n",
      "train loss:0.020333553923452655\n",
      "train loss:0.012948886661122802\n",
      "train loss:0.030674782413477958\n",
      "train loss:0.05295957052137114\n",
      "train loss:0.01799402946644167\n",
      "train loss:0.0666041198885825\n",
      "train loss:0.06007711683843557\n",
      "train loss:0.01448235527948798\n",
      "train loss:0.07408607081535175\n",
      "train loss:0.05677908810102731\n",
      "train loss:0.07322672637366766\n",
      "train loss:0.023276974617863225\n",
      "train loss:0.027180581444585065\n",
      "train loss:0.047272627295883655\n",
      "train loss:0.020280136203363\n",
      "train loss:0.041661271127633986\n",
      "train loss:0.07985592418153349\n",
      "train loss:0.03873789780818502\n",
      "train loss:0.06528356456238261\n",
      "train loss:0.031332327277428955\n",
      "train loss:0.022247849839945367\n",
      "train loss:0.12061703354077238\n",
      "train loss:0.06374442941560193\n",
      "train loss:0.0451358397708196\n",
      "train loss:0.03761676893172089\n",
      "train loss:0.049273788858262065\n",
      "train loss:0.04890092623701933\n",
      "train loss:0.022439449773720064\n",
      "train loss:0.011078753300989478\n",
      "train loss:0.050283560248532894\n",
      "train loss:0.012023619149311158\n",
      "train loss:0.030420756493029572\n",
      "train loss:0.026692758496130987\n",
      "train loss:0.04053585510826029\n",
      "train loss:0.04499044512716479\n",
      "train loss:0.04950828807254096\n",
      "train loss:0.018233672057403054\n",
      "train loss:0.00617014769672516\n",
      "train loss:0.09334248306788527\n",
      "train loss:0.011053197537080948\n",
      "train loss:0.027294460109268486\n",
      "train loss:0.03877370310405975\n",
      "train loss:0.03308587406512422\n",
      "train loss:0.05543868812251345\n",
      "train loss:0.011728726675245011\n",
      "train loss:0.07989665699141411\n",
      "train loss:0.04788956540731614\n",
      "train loss:0.032318807611689794\n",
      "train loss:0.021261301515922326\n",
      "train loss:0.030558141005575905\n",
      "train loss:0.06820636216799711\n",
      "train loss:0.022467180476333747\n",
      "train loss:0.05634884994491942\n",
      "train loss:0.07794993888788214\n",
      "train loss:0.032280088264925524\n",
      "train loss:0.07631138511148107\n",
      "train loss:0.018794170343040918\n",
      "train loss:0.03553667995472472\n",
      "train loss:0.010627648560427265\n",
      "train loss:0.00924160881044072\n",
      "train loss:0.025183915156605605\n",
      "train loss:0.01700420679085897\n",
      "train loss:0.018333641601756934\n",
      "train loss:0.012946479650469123\n",
      "train loss:0.015939085588982046\n",
      "train loss:0.08015589672292689\n",
      "train loss:0.07005027702236273\n",
      "train loss:0.007804982406641953\n",
      "train loss:0.017210182589489486\n",
      "train loss:0.045943962378892096\n",
      "train loss:0.023069235227792424\n",
      "train loss:0.02062447672404111\n",
      "train loss:0.03757659208203721\n",
      "train loss:0.08950979048874652\n",
      "train loss:0.02731728103078561\n",
      "train loss:0.02589020939705338\n",
      "train loss:0.05436687251406778\n",
      "train loss:0.01974049356144313\n",
      "train loss:0.03938239041098871\n",
      "train loss:0.10584251006251487\n",
      "train loss:0.010568882799030219\n",
      "train loss:0.04964450928885587\n",
      "train loss:0.06762335740492652\n",
      "train loss:0.0037002680808400036\n",
      "train loss:0.024695060721097065\n",
      "train loss:0.0659687319143868\n",
      "train loss:0.10898607153586047\n",
      "train loss:0.08305906039472352\n",
      "train loss:0.05433749774626697\n",
      "train loss:0.020677783263114174\n",
      "train loss:0.0494892064915756\n",
      "train loss:0.009059432787203385\n",
      "train loss:0.09892394545942661\n",
      "train loss:0.06420493087103128\n",
      "train loss:0.05423901446304678\n",
      "train loss:0.023434579409478667\n",
      "train loss:0.02567821973206734\n",
      "train loss:0.01789196036526961\n",
      "train loss:0.03996220532590495\n",
      "train loss:0.06551675746414987\n",
      "train loss:0.015328856515893073\n",
      "train loss:0.13796458306056317\n",
      "train loss:0.02765470582135218\n",
      "train loss:0.050553165540704284\n",
      "train loss:0.04009326625112797\n",
      "train loss:0.02671628238047558\n",
      "train loss:0.0269933879163745\n",
      "train loss:0.041726518537908944\n",
      "train loss:0.07590898084663228\n",
      "train loss:0.13141700787215002\n",
      "train loss:0.03526047250215119\n",
      "train loss:0.060261195408914246\n",
      "train loss:0.02563894914559817\n",
      "train loss:0.024092696854138503\n",
      "train loss:0.036096404290769504\n",
      "train loss:0.03361777635825371\n",
      "train loss:0.04790227602705259\n",
      "train loss:0.04018759703609105\n",
      "train loss:0.04748889794412775\n",
      "train loss:0.061677698363968074\n",
      "train loss:0.03592177423779561\n",
      "train loss:0.03460854102628599\n",
      "train loss:0.05950113636068322\n",
      "train loss:0.012954066460891226\n",
      "train loss:0.0782718706547349\n",
      "train loss:0.02899823893565956\n",
      "train loss:0.03916135486212913\n",
      "train loss:0.03074156768824511\n",
      "train loss:0.0805459728373696\n",
      "train loss:0.04119905187944025\n",
      "train loss:0.04433145030617114\n",
      "train loss:0.017419497493150603\n",
      "train loss:0.051759656116085344\n",
      "train loss:0.041335578019259724\n",
      "train loss:0.04755804072499732\n",
      "train loss:0.05135380866394066\n",
      "train loss:0.02828312875753877\n",
      "train loss:0.0025660419000677185\n",
      "train loss:0.05242894491827881\n",
      "train loss:0.06915332866143335\n",
      "train loss:0.07254577947794835\n",
      "train loss:0.1176571452831698\n",
      "train loss:0.022418994146153764\n",
      "train loss:0.044638240153565\n",
      "train loss:0.024071384141053812\n",
      "train loss:0.01737496219396906\n",
      "train loss:0.0260916151936838\n",
      "train loss:0.029425130326680548\n",
      "train loss:0.02905292395683575\n",
      "train loss:0.04805702229231854\n",
      "train loss:0.04968754310146198\n",
      "train loss:0.018452052170076755\n",
      "train loss:0.08349068073150914\n",
      "train loss:0.031448935392122616\n",
      "train loss:0.01590485609909266\n",
      "train loss:0.03256944492523757\n",
      "train loss:0.021805373365076788\n",
      "train loss:0.08045143025646775\n",
      "train loss:0.07527928524891253\n",
      "train loss:0.022835399068684956\n",
      "train loss:0.0820293361575619\n",
      "train loss:0.15138484477621203\n",
      "train loss:0.03202936743969451\n",
      "train loss:0.018880392253172302\n",
      "train loss:0.012918464409053842\n",
      "train loss:0.011064335206998772\n",
      "train loss:0.050508559356224995\n",
      "train loss:0.04187614320363365\n",
      "train loss:0.014444707188867261\n",
      "train loss:0.019881510163449943\n",
      "train loss:0.007189972320605574\n",
      "train loss:0.013656308662206114\n",
      "train loss:0.0141361759987765\n",
      "train loss:0.042097078991942094\n",
      "train loss:0.03402226138112217\n",
      "train loss:0.016997452008836005\n",
      "train loss:0.04575588004931151\n",
      "train loss:0.012048574324140004\n",
      "train loss:0.023132924044750314\n",
      "train loss:0.02457256272364826\n",
      "train loss:0.017209444899565895\n",
      "train loss:0.011728446924827971\n",
      "train loss:0.04198478887341475\n",
      "train loss:0.09320398545940758\n",
      "train loss:0.07463906247350498\n",
      "train loss:0.04990223196344707\n",
      "train loss:0.02047233998934439\n",
      "train loss:0.06995534887575668\n",
      "train loss:0.011330720749395944\n",
      "train loss:0.01091206498322881\n",
      "train loss:0.01439233217769803\n",
      "train loss:0.014029639057079834\n",
      "train loss:0.031014661296781404\n",
      "train loss:0.05698321007303157\n",
      "train loss:0.00820629409772506\n",
      "train loss:0.06861390253053266\n",
      "train loss:0.029327628593995115\n",
      "train loss:0.02924165046824697\n",
      "train loss:0.10235888801243524\n",
      "train loss:0.05223470381169551\n",
      "train loss:0.06993711327328794\n",
      "train loss:0.016553737963125332\n",
      "train loss:0.047427850385226386\n",
      "train loss:0.01546996646191786\n",
      "train loss:0.02341926330592632\n",
      "train loss:0.08280958537852817\n",
      "train loss:0.01867924646232055\n",
      "train loss:0.02211433916685\n",
      "train loss:0.009089695905536058\n",
      "train loss:0.026314041943131358\n",
      "train loss:0.07507827387775748\n",
      "train loss:0.006929136697686677\n",
      "train loss:0.026791372940534287\n",
      "train loss:0.02647212071761465\n",
      "train loss:0.01678421971030705\n",
      "train loss:0.007463736297795663\n",
      "train loss:0.025424677252379516\n",
      "train loss:0.01684790544454393\n",
      "train loss:0.008208208595964295\n",
      "train loss:0.08396003858429885\n",
      "train loss:0.03387829316068814\n",
      "train loss:0.011274057695780508\n",
      "train loss:0.016556901124305547\n",
      "train loss:0.014704673412181866\n",
      "train loss:0.015413285170420168\n",
      "train loss:0.03022688100664261\n",
      "train loss:0.07029902716483315\n",
      "train loss:0.012423310589774078\n",
      "train loss:0.014089353017696503\n",
      "train loss:0.022876701737298458\n",
      "train loss:0.03369015821400489\n",
      "train loss:0.024620107634913756\n",
      "train loss:0.03934333850147018\n",
      "train loss:0.02143243281094088\n",
      "train loss:0.03826892181246553\n",
      "train loss:0.03353618185778068\n",
      "train loss:0.06733741261123657\n",
      "train loss:0.07206573337021008\n",
      "train loss:0.02232359698631075\n",
      "train loss:0.021332253018328032\n",
      "train loss:0.02542952785605899\n",
      "train loss:0.026662734455115578\n",
      "train loss:0.012623337108022341\n",
      "train loss:0.00860772150394213\n",
      "train loss:0.03138263909332652\n",
      "train loss:0.016642094083266713\n",
      "train loss:0.007057079020280683\n",
      "train loss:0.07629504885336574\n",
      "train loss:0.015709864309247853\n",
      "train loss:0.07560107161161288\n",
      "train loss:0.0673914905825262\n",
      "train loss:0.017407360836648737\n",
      "train loss:0.02075829699394078\n",
      "train loss:0.06331651239239097\n",
      "train loss:0.03067798224454243\n",
      "train loss:0.02363676680647\n",
      "train loss:0.06592762075679225\n",
      "train loss:0.1236986085341322\n",
      "train loss:0.023669486717212612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03872484978072306\n",
      "train loss:0.02348284446339783\n",
      "train loss:0.044182676955914664\n",
      "train loss:0.01723389398670644\n",
      "train loss:0.011743829291642471\n",
      "train loss:0.02873608773428317\n",
      "train loss:0.06681002695101905\n",
      "train loss:0.009613786161016373\n",
      "train loss:0.024686073108804565\n",
      "train loss:0.030555459154571744\n",
      "train loss:0.02018389491649496\n",
      "train loss:0.03336799946025233\n",
      "train loss:0.028785176674283392\n",
      "train loss:0.04369730363045545\n",
      "train loss:0.03260164037311115\n",
      "train loss:0.011431645250045547\n",
      "train loss:0.019488523889744813\n",
      "=== epoch:5, train acc:0.986, test acc:0.982 ===\n",
      "train loss:0.005743261717318692\n",
      "train loss:0.02563679487396573\n",
      "train loss:0.0882726660909147\n",
      "train loss:0.02650092762954829\n",
      "train loss:0.011003165926379503\n",
      "train loss:0.01311794797349896\n",
      "train loss:0.035425334785283244\n",
      "train loss:0.06062925735480199\n",
      "train loss:0.009413840377717596\n",
      "train loss:0.028377960760811928\n",
      "train loss:0.027661511400985325\n",
      "train loss:0.021019128146253185\n",
      "train loss:0.04530386273935938\n",
      "train loss:0.008457289555366348\n",
      "train loss:0.0352518857348594\n",
      "train loss:0.04486997364548844\n",
      "train loss:0.04833840437745204\n",
      "train loss:0.03811800395757184\n",
      "train loss:0.032134650098093645\n",
      "train loss:0.026199177525020788\n",
      "train loss:0.056453840612511276\n",
      "train loss:0.04249670482294087\n",
      "train loss:0.11164112379371488\n",
      "train loss:0.022237261594174544\n",
      "train loss:0.013989913055140685\n",
      "train loss:0.031105456144355198\n",
      "train loss:0.029185823335481788\n",
      "train loss:0.00932577851079683\n",
      "train loss:0.016850559284197777\n",
      "train loss:0.006223189693812792\n",
      "train loss:0.008410093620246603\n",
      "train loss:0.04137639030667249\n",
      "train loss:0.03166998057041163\n",
      "train loss:0.035419789341445254\n",
      "train loss:0.025274189123008987\n",
      "train loss:0.020962997591194993\n",
      "train loss:0.03990260290684115\n",
      "train loss:0.030312850710072104\n",
      "train loss:0.02317605538859629\n",
      "train loss:0.01830147849153699\n",
      "train loss:0.09746805738926573\n",
      "train loss:0.038651712671008774\n",
      "train loss:0.018623360946570067\n",
      "train loss:0.019801777789776622\n",
      "train loss:0.009082053154418922\n",
      "train loss:0.02208896152334578\n",
      "train loss:0.028172632301853587\n",
      "train loss:0.031743944286799095\n",
      "train loss:0.011057687466652597\n",
      "train loss:0.015546341141403096\n",
      "train loss:0.017093690569167222\n",
      "train loss:0.013432469641420666\n",
      "train loss:0.03268826503344706\n",
      "train loss:0.022611260462444327\n",
      "train loss:0.017244859396631444\n",
      "train loss:0.01529945527776453\n",
      "train loss:0.03641769027866353\n",
      "train loss:0.041319954904739935\n",
      "train loss:0.021174826664653982\n",
      "train loss:0.013084913025169223\n",
      "train loss:0.010201297448933309\n",
      "train loss:0.01618916380954736\n",
      "train loss:0.014103018327112755\n",
      "train loss:0.020835899624436157\n",
      "train loss:0.025863510592324182\n",
      "train loss:0.015051501598620745\n",
      "train loss:0.014144821565647616\n",
      "train loss:0.002458002089939727\n",
      "train loss:0.009089318208731842\n",
      "train loss:0.023984898829360987\n",
      "train loss:0.011915712298850465\n",
      "train loss:0.018854182305895667\n",
      "train loss:0.014365600930766609\n",
      "train loss:0.013970014030279816\n",
      "train loss:0.038728846379089445\n",
      "train loss:0.03576842662350435\n",
      "train loss:0.010801393751734558\n",
      "train loss:0.06482826643176326\n",
      "train loss:0.06440415232679957\n",
      "train loss:0.028907004055929483\n",
      "train loss:0.020962687950479123\n",
      "train loss:0.016611855686098336\n",
      "train loss:0.04851959972733818\n",
      "train loss:0.015433454819015265\n",
      "train loss:0.04202722014801801\n",
      "train loss:0.010873715307269844\n",
      "train loss:0.00978264412786219\n",
      "train loss:0.018484512185768628\n",
      "train loss:0.0072140813039241304\n",
      "train loss:0.01643365081767563\n",
      "train loss:0.027904852293261416\n",
      "train loss:0.05435923618669165\n",
      "train loss:0.07926644884220796\n",
      "train loss:0.021614680411790682\n",
      "train loss:0.042477719344100565\n",
      "train loss:0.008580309325941891\n",
      "train loss:0.05727082460614385\n",
      "train loss:0.04181150688745628\n",
      "train loss:0.026486093007391527\n",
      "train loss:0.004295605672256821\n",
      "train loss:0.024332728956768874\n",
      "train loss:0.021427064151955638\n",
      "train loss:0.0239988597740977\n",
      "train loss:0.05895991497297636\n",
      "train loss:0.022510376709580244\n",
      "train loss:0.012995644703512405\n",
      "train loss:0.030460941023346934\n",
      "train loss:0.028226743716429355\n",
      "train loss:0.03359902787614933\n",
      "train loss:0.05356109525560507\n",
      "train loss:0.03502337875735407\n",
      "train loss:0.007915762219146647\n",
      "train loss:0.02602419817234437\n",
      "train loss:0.007125898197085554\n",
      "train loss:0.010846627484252708\n",
      "train loss:0.032363181153004184\n",
      "train loss:0.032306913126334685\n",
      "train loss:0.04131823481770332\n",
      "train loss:0.008650275153900539\n",
      "train loss:0.020727134917019455\n",
      "train loss:0.01842162158636074\n",
      "train loss:0.04844409691015675\n",
      "train loss:0.015067283315261262\n",
      "train loss:0.01686267759192346\n",
      "train loss:0.031214554719293276\n",
      "train loss:0.03165620820698127\n",
      "train loss:0.009906758617734261\n",
      "train loss:0.021103142666971574\n",
      "train loss:0.12210676514943955\n",
      "train loss:0.020869976590301688\n",
      "train loss:0.010596956114190401\n",
      "train loss:0.021926731383633335\n",
      "train loss:0.06419374982647652\n",
      "train loss:0.003579643449107229\n",
      "train loss:0.02155208760096975\n",
      "train loss:0.008888146973907895\n",
      "train loss:0.06325238429177277\n",
      "train loss:0.023891680915783718\n",
      "train loss:0.006069058063632614\n",
      "train loss:0.07790633224434926\n",
      "train loss:0.009220288715462079\n",
      "train loss:0.029293951232781872\n",
      "train loss:0.010482005319456325\n",
      "train loss:0.014407810479644229\n",
      "train loss:0.04315099133481011\n",
      "train loss:0.05776999046447117\n",
      "train loss:0.04833632133562931\n",
      "train loss:0.0073411060605728775\n",
      "train loss:0.017353577413331604\n",
      "train loss:0.03561152945449809\n",
      "train loss:0.03629171026460263\n",
      "train loss:0.016957030468649978\n",
      "train loss:0.013506680083229044\n",
      "train loss:0.024021141339532922\n",
      "train loss:0.013332248093887939\n",
      "train loss:0.06266917289234814\n",
      "train loss:0.010219439191923817\n",
      "train loss:0.005535908277046654\n",
      "train loss:0.05447667341680622\n",
      "train loss:0.016467667891527864\n",
      "train loss:0.03187003913733503\n",
      "train loss:0.014812429885743114\n",
      "train loss:0.0068336419198607974\n",
      "train loss:0.02091380750780233\n",
      "train loss:0.020211117714775503\n",
      "train loss:0.023663270234878672\n",
      "train loss:0.03596217534343637\n",
      "train loss:0.020232706293992958\n",
      "train loss:0.03910072457510317\n",
      "train loss:0.009843912756960536\n",
      "train loss:0.029605273761368615\n",
      "train loss:0.042874229716685475\n",
      "train loss:0.04894375430386247\n",
      "train loss:0.02160236453809792\n",
      "train loss:0.019869483953976728\n",
      "train loss:0.0222423642583632\n",
      "train loss:0.07258801503409068\n",
      "train loss:0.05300793233374138\n",
      "train loss:0.07945863671725903\n",
      "train loss:0.0105103927278891\n",
      "train loss:0.0837297769880362\n",
      "train loss:0.03520911749875906\n",
      "train loss:0.011113540893310778\n",
      "train loss:0.023382844208382266\n",
      "train loss:0.033813508212592436\n",
      "train loss:0.05739811271369339\n",
      "train loss:0.017625282760434166\n",
      "train loss:0.0745768408912886\n",
      "train loss:0.02567718708862364\n",
      "train loss:0.019015426034856828\n",
      "train loss:0.03113202906285396\n",
      "train loss:0.005604957896396078\n",
      "train loss:0.03360566042425361\n",
      "train loss:0.02081855954659705\n",
      "train loss:0.009025856331757329\n",
      "train loss:0.07711593997328943\n",
      "train loss:0.01913132547255779\n",
      "train loss:0.02910030055436414\n",
      "train loss:0.00905800415392754\n",
      "train loss:0.041571869166911954\n",
      "train loss:0.010782988394665769\n",
      "train loss:0.017150288040410234\n",
      "train loss:0.038387408223171764\n",
      "train loss:0.018570222083718654\n",
      "train loss:0.05412076688391346\n",
      "train loss:0.012952522956295778\n",
      "train loss:0.010373405874877442\n",
      "train loss:0.048538397821807015\n",
      "train loss:0.009523947746778928\n",
      "train loss:0.0357337947342739\n",
      "train loss:0.018915165345757476\n",
      "train loss:0.03221761643218463\n",
      "train loss:0.009793301235523542\n",
      "train loss:0.0727669597856149\n",
      "train loss:0.013530945284177533\n",
      "train loss:0.021746924739947705\n",
      "train loss:0.0068082663612772445\n",
      "train loss:0.0411117023042174\n",
      "train loss:0.021437831598913503\n",
      "train loss:0.03302966178713739\n",
      "train loss:0.004472065654073534\n",
      "train loss:0.016724031561410363\n",
      "train loss:0.008314673840812276\n",
      "train loss:0.012440039073863986\n",
      "train loss:0.11802611615468397\n",
      "train loss:0.02302419262897735\n",
      "train loss:0.007971622231428893\n",
      "train loss:0.014729713917745958\n",
      "train loss:0.010921261396171517\n",
      "train loss:0.05406287035594636\n",
      "train loss:0.0029898741577081478\n",
      "train loss:0.009858347157701454\n",
      "train loss:0.020212838482624854\n",
      "train loss:0.02229628928456005\n",
      "train loss:0.042505456020967\n",
      "train loss:0.014679571846613018\n",
      "train loss:0.02182602696634626\n",
      "train loss:0.05270430068836003\n",
      "train loss:0.011092565545088447\n",
      "train loss:0.008244192915164266\n",
      "train loss:0.019962826994820014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03200139327209149\n",
      "train loss:0.0170417556900748\n",
      "train loss:0.00616601305907565\n",
      "train loss:0.021701997680631625\n",
      "train loss:0.02130143304782228\n",
      "train loss:0.02564532397069601\n",
      "train loss:0.011389235611720776\n",
      "train loss:0.006347857431188996\n",
      "train loss:0.010510397911490266\n",
      "train loss:0.045387127460564065\n",
      "train loss:0.020215888186622746\n",
      "train loss:0.018436049832572247\n",
      "train loss:0.009902147474925384\n",
      "train loss:0.02657680449079937\n",
      "train loss:0.02674142813249463\n",
      "train loss:0.021795047152298283\n",
      "train loss:0.014955482232247012\n",
      "train loss:0.014978760793785384\n",
      "train loss:0.014513486772124945\n",
      "train loss:0.030039879718241028\n",
      "train loss:0.014245978126769936\n",
      "train loss:0.04243959819946472\n",
      "train loss:0.009626036820102457\n",
      "train loss:0.013488957788899286\n",
      "train loss:0.07240046206878352\n",
      "train loss:0.013824607419303492\n",
      "train loss:0.01415785728556054\n",
      "train loss:0.011347497876510189\n",
      "train loss:0.015275299408197445\n",
      "train loss:0.04172735675677396\n",
      "train loss:0.0757813848545838\n",
      "train loss:0.014434254692328459\n",
      "train loss:0.06677432491565793\n",
      "train loss:0.009105358206752677\n",
      "train loss:0.00952722677410563\n",
      "train loss:0.020200430318088363\n",
      "train loss:0.030012971252775272\n",
      "train loss:0.04062692680898948\n",
      "train loss:0.0023548760928291945\n",
      "train loss:0.005927959135952552\n",
      "train loss:0.010146152928114407\n",
      "train loss:0.011703702853299405\n",
      "train loss:0.023672716145943806\n",
      "train loss:0.022120240219830212\n",
      "train loss:0.014637022271375377\n",
      "train loss:0.03014978352089225\n",
      "train loss:0.009636691796444623\n",
      "train loss:0.03188068823824371\n",
      "train loss:0.025311462022594634\n",
      "train loss:0.02104592894052981\n",
      "train loss:0.041332853755058664\n",
      "train loss:0.007218875176326522\n",
      "train loss:0.013928915598140044\n",
      "train loss:0.023696214372177532\n",
      "train loss:0.004652463234085625\n",
      "train loss:0.03597128461926149\n",
      "train loss:0.039664310331165444\n",
      "train loss:0.04323351743710024\n",
      "train loss:0.055478773007057086\n",
      "train loss:0.10123643992790703\n",
      "train loss:0.014509876729652043\n",
      "train loss:0.012107023247431035\n",
      "train loss:0.01841755586971212\n",
      "train loss:0.040882923564691274\n",
      "train loss:0.03172028359264864\n",
      "train loss:0.05237760662813022\n",
      "train loss:0.027687620117924836\n",
      "train loss:0.023058162363585325\n",
      "train loss:0.015304758237081571\n",
      "train loss:0.12968830387268887\n",
      "train loss:0.011659099269847584\n",
      "train loss:0.011064796617616309\n",
      "train loss:0.07274802786865353\n",
      "train loss:0.009489068127597002\n",
      "train loss:0.028585510348952802\n",
      "train loss:0.025135854730109517\n",
      "train loss:0.025860416660166093\n",
      "train loss:0.018361126604698786\n",
      "train loss:0.045033357957616114\n",
      "train loss:0.031002396129786978\n",
      "train loss:0.0284822809347633\n",
      "train loss:0.015244809972855893\n",
      "train loss:0.08431879973391083\n",
      "train loss:0.008757584486278759\n",
      "train loss:0.008930538820306342\n",
      "train loss:0.08898854862768399\n",
      "train loss:0.023023674948566036\n",
      "train loss:0.008477948619914579\n",
      "train loss:0.002958968648106922\n",
      "train loss:0.013820665159278172\n",
      "train loss:0.010867664348714293\n",
      "train loss:0.007492589665921723\n",
      "train loss:0.022867323398295357\n",
      "train loss:0.023270805245320755\n",
      "train loss:0.01659654309666017\n",
      "train loss:0.015054085010448644\n",
      "train loss:0.008328427436018805\n",
      "train loss:0.019942072683809386\n",
      "train loss:0.012499637338994021\n",
      "train loss:0.013695620914508155\n",
      "train loss:0.03133971424012103\n",
      "train loss:0.027814051359450486\n",
      "train loss:0.02687526281830841\n",
      "train loss:0.041461247695053924\n",
      "train loss:0.020045327607219613\n",
      "train loss:0.009150538464887286\n",
      "train loss:0.0209844318894022\n",
      "train loss:0.002832711563347095\n",
      "train loss:0.03628920650706262\n",
      "train loss:0.028117229087533762\n",
      "train loss:0.05167504596112437\n",
      "train loss:0.038997775549969\n",
      "train loss:0.011010283422723942\n",
      "train loss:0.024364264106882635\n",
      "train loss:0.014139579043526735\n",
      "train loss:0.04345463035219486\n",
      "train loss:0.009405049450008165\n",
      "train loss:0.022758275553280333\n",
      "train loss:0.018781103301832937\n",
      "train loss:0.019467191127483848\n",
      "train loss:0.013057469125112782\n",
      "train loss:0.008430333695731467\n",
      "train loss:0.007968275156826146\n",
      "train loss:0.00937049160515702\n",
      "train loss:0.025217210782990565\n",
      "train loss:0.005145880299747616\n",
      "train loss:0.009699502499172922\n",
      "train loss:0.012317708276218031\n",
      "train loss:0.006342940397122274\n",
      "train loss:0.02712034866202151\n",
      "train loss:0.09630429219076414\n",
      "train loss:0.04161568017964251\n",
      "train loss:0.0464426027305644\n",
      "train loss:0.0023205939667973563\n",
      "train loss:0.018775679146458377\n",
      "train loss:0.045572712032896694\n",
      "train loss:0.05886144310574751\n",
      "train loss:0.019760534848375838\n",
      "train loss:0.006853020641015927\n",
      "train loss:0.02850111445218703\n",
      "train loss:0.022411038732813957\n",
      "train loss:0.013589195573087536\n",
      "train loss:0.027529586603582675\n",
      "train loss:0.02530496438105973\n",
      "train loss:0.019177174127210733\n",
      "train loss:0.02117039765985002\n",
      "train loss:0.02081829161083017\n",
      "train loss:0.008110797570188712\n",
      "train loss:0.05232289613641925\n",
      "train loss:0.17207786356831054\n",
      "train loss:0.06051003607334037\n",
      "train loss:0.03525239387345905\n",
      "train loss:0.016023893453575345\n",
      "train loss:0.0066954290967965405\n",
      "train loss:0.02382019225454477\n",
      "train loss:0.01925012877864663\n",
      "train loss:0.013385988550913143\n",
      "train loss:0.027125801578106814\n",
      "train loss:0.021302150361599916\n",
      "train loss:0.015450380516043825\n",
      "train loss:0.002923560730890818\n",
      "train loss:0.034761679041098746\n",
      "train loss:0.02666083574345887\n",
      "train loss:0.037946635534883866\n",
      "train loss:0.0252304310704768\n",
      "train loss:0.011521776939442206\n",
      "train loss:0.026361906902048125\n",
      "train loss:0.01858569384532467\n",
      "train loss:0.05545290660088172\n",
      "train loss:0.009426462705489934\n",
      "train loss:0.05223330658796264\n",
      "train loss:0.02515643918927164\n",
      "train loss:0.04734993372612019\n",
      "train loss:0.060190493988572034\n",
      "train loss:0.012561664242434083\n",
      "train loss:0.03494480614959622\n",
      "train loss:0.013715970401825569\n",
      "train loss:0.03025654661823789\n",
      "train loss:0.008577192653569958\n",
      "train loss:0.10230574581149282\n",
      "train loss:0.015423576709223709\n",
      "train loss:0.0480089870875573\n",
      "train loss:0.01623179198388311\n",
      "train loss:0.02629950145927112\n",
      "train loss:0.014112064270178222\n",
      "train loss:0.026864833928263047\n",
      "train loss:0.04103986079162528\n",
      "train loss:0.04467180731029202\n",
      "train loss:0.009392730129828308\n",
      "train loss:0.012563903244025627\n",
      "train loss:0.007476826982920744\n",
      "train loss:0.026178486337984343\n",
      "train loss:0.016269198020227357\n",
      "train loss:0.020858680180883323\n",
      "train loss:0.05301485446889365\n",
      "train loss:0.04062360698144092\n",
      "train loss:0.007306686505515219\n",
      "train loss:0.06901237018674963\n",
      "train loss:0.030090929722997498\n",
      "train loss:0.005431148721974608\n",
      "train loss:0.008609830166666435\n",
      "train loss:0.025189720377805567\n",
      "train loss:0.026350220889870966\n",
      "train loss:0.006333597341394609\n",
      "train loss:0.007071824889446365\n",
      "train loss:0.009764335258237677\n",
      "train loss:0.0284838981416718\n",
      "train loss:0.09928115619007945\n",
      "train loss:0.0612730780680199\n",
      "train loss:0.01425007431845356\n",
      "train loss:0.0335952789079981\n",
      "train loss:0.01816548052487891\n",
      "train loss:0.03382712426751318\n",
      "train loss:0.0332546057308642\n",
      "train loss:0.02149755793639463\n",
      "train loss:0.007690392895026174\n",
      "train loss:0.010721289936668026\n",
      "train loss:0.016383754104159293\n",
      "train loss:0.04502199650310391\n",
      "train loss:0.009901569643021082\n",
      "train loss:0.019539882274546668\n",
      "train loss:0.020275686236842977\n",
      "train loss:0.009935706972487553\n",
      "train loss:0.013480080128856717\n",
      "train loss:0.012094953309349026\n",
      "train loss:0.015274461708547744\n",
      "train loss:0.037677459671535574\n",
      "train loss:0.012839832913983933\n",
      "train loss:0.03629521082716477\n",
      "train loss:0.03582238117936784\n",
      "train loss:0.008023365858152701\n",
      "train loss:0.02682082765393525\n",
      "train loss:0.007716673632729368\n",
      "train loss:0.008428677791263536\n",
      "train loss:0.026090308972446727\n",
      "train loss:0.022562884352590053\n",
      "train loss:0.012051036600943772\n",
      "train loss:0.062975220849756\n",
      "train loss:0.007648764327891149\n",
      "train loss:0.030771840498900013\n",
      "train loss:0.006180666354073343\n",
      "train loss:0.019640397684789763\n",
      "train loss:0.009855977699269356\n",
      "train loss:0.021322107456873837\n",
      "train loss:0.007266336831444108\n",
      "train loss:0.0030923610937543945\n",
      "train loss:0.01169052212901728\n",
      "train loss:0.029958295923634404\n",
      "train loss:0.011468376975174483\n",
      "train loss:0.028178993985642924\n",
      "train loss:0.024104859968753268\n",
      "train loss:0.006907239892395631\n",
      "train loss:0.01990603982180115\n",
      "train loss:0.025840951404873892\n",
      "train loss:0.009830629116361149\n",
      "train loss:0.010767579294637286\n",
      "train loss:0.04719606188565071\n",
      "train loss:0.04540713564878547\n",
      "train loss:0.002652775930895171\n",
      "train loss:0.06175503480928497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.004478507947111815\n",
      "train loss:0.04020013411683557\n",
      "train loss:0.004197902788314737\n",
      "train loss:0.009335214962264889\n",
      "train loss:0.022420950035814756\n",
      "train loss:0.009831921929109713\n",
      "train loss:0.010383159016798402\n",
      "train loss:0.0178337347334598\n",
      "train loss:0.023950818732044334\n",
      "train loss:0.010402700606815555\n",
      "train loss:0.014623566674699624\n",
      "train loss:0.006187516432054408\n",
      "train loss:0.003218119425400589\n",
      "train loss:0.014974789576052772\n",
      "train loss:0.021246365584577535\n",
      "train loss:0.048101759029751764\n",
      "train loss:0.05084323308766806\n",
      "train loss:0.01747232290128246\n",
      "train loss:0.002361087295254127\n",
      "train loss:0.009047617495384989\n",
      "train loss:0.029428852306080083\n",
      "train loss:0.0043396082763291965\n",
      "train loss:0.0403846449631049\n",
      "train loss:0.02270212383191782\n",
      "train loss:0.05126429773009773\n",
      "train loss:0.009109054118460207\n",
      "train loss:0.016079168632198083\n",
      "train loss:0.05466695104751235\n",
      "train loss:0.020390437504520905\n",
      "train loss:0.03528875411414395\n",
      "train loss:0.012261005869393111\n",
      "train loss:0.010045009563543521\n",
      "train loss:0.005403377585760609\n",
      "train loss:0.03922666942671547\n",
      "train loss:0.005427264134428203\n",
      "train loss:0.023130193974164825\n",
      "train loss:0.013852861812995878\n",
      "train loss:0.018181043584640632\n",
      "train loss:0.008242570087997924\n",
      "train loss:0.011398815825682342\n",
      "train loss:0.018468987830289264\n",
      "train loss:0.013175141722694881\n",
      "train loss:0.13434296381078445\n",
      "train loss:0.05703775974302795\n",
      "train loss:0.04476811396410667\n",
      "train loss:0.011478411517059063\n",
      "train loss:0.006529240691257094\n",
      "train loss:0.09486274274169633\n",
      "train loss:0.011767143620516436\n",
      "train loss:0.004402586396865655\n",
      "train loss:0.005978355814636035\n",
      "train loss:0.003700710376443676\n",
      "train loss:0.006779657217473348\n",
      "train loss:0.009008082615305134\n",
      "train loss:0.035969959632445744\n",
      "train loss:0.010018581134999531\n",
      "train loss:0.04942428301314183\n",
      "train loss:0.007676919048703432\n",
      "train loss:0.04840524316527083\n",
      "train loss:0.007606403704245789\n",
      "train loss:0.05301949802497376\n",
      "train loss:0.01260067614045377\n",
      "train loss:0.0537996090857924\n",
      "train loss:0.08649734845734514\n",
      "train loss:0.00821155694754033\n",
      "train loss:0.0381884708643455\n",
      "train loss:0.026197410458749508\n",
      "train loss:0.015040243008030202\n",
      "train loss:0.06631051748356738\n",
      "train loss:0.03572630465524678\n",
      "train loss:0.021539035607193103\n",
      "train loss:0.03700526314815963\n",
      "train loss:0.026809834419285257\n",
      "train loss:0.053428518459095235\n",
      "train loss:0.01560872014601388\n",
      "train loss:0.023144212910170325\n",
      "train loss:0.014967783756617779\n",
      "train loss:0.047140879034440065\n",
      "train loss:0.012337857334466898\n",
      "train loss:0.09450537983008857\n",
      "train loss:0.06860847021661422\n",
      "train loss:0.033532579852306905\n",
      "train loss:0.028886764513377333\n",
      "train loss:0.004773062440141235\n",
      "train loss:0.006928450775551891\n",
      "train loss:0.027510136012940625\n",
      "train loss:0.03873807454521412\n",
      "train loss:0.043454819514721396\n",
      "train loss:0.03269799373724909\n",
      "train loss:0.013606363689248318\n",
      "train loss:0.02928041441599257\n",
      "train loss:0.009238800417226299\n",
      "train loss:0.024621791012046864\n",
      "train loss:0.012511062160676815\n",
      "train loss:0.03339002781207885\n",
      "train loss:0.02914536803661928\n",
      "train loss:0.005846097648826183\n",
      "train loss:0.005579077153477091\n",
      "train loss:0.01693938605738105\n",
      "=== epoch:6, train acc:0.988, test acc:0.988 ===\n",
      "train loss:0.00451605560292943\n",
      "train loss:0.04293259180112886\n",
      "train loss:0.014087823399709407\n",
      "train loss:0.04551335158921392\n",
      "train loss:0.048627496945404254\n",
      "train loss:0.013238069831076293\n",
      "train loss:0.0094647390219257\n",
      "train loss:0.04570536358806689\n",
      "train loss:0.03884169034400683\n",
      "train loss:0.056299274489600835\n",
      "train loss:0.004211243248666707\n",
      "train loss:0.014985809161874676\n",
      "train loss:0.043902796338630395\n",
      "train loss:0.029287669196600308\n",
      "train loss:0.005641199642202182\n",
      "train loss:0.04340959181151038\n",
      "train loss:0.021767253905981335\n",
      "train loss:0.07682272924154171\n",
      "train loss:0.024273897238991333\n",
      "train loss:0.01706347221096763\n",
      "train loss:0.029106125351474207\n",
      "train loss:0.015901822531819553\n",
      "train loss:0.01301217400388206\n",
      "train loss:0.01204521666458913\n",
      "train loss:0.05759145245131824\n",
      "train loss:0.025058778159509635\n",
      "train loss:0.0035519639904089124\n",
      "train loss:0.010684975036280962\n",
      "train loss:0.01644487312245196\n",
      "train loss:0.008317880008742014\n",
      "train loss:0.03154843938079236\n",
      "train loss:0.008536406339807621\n",
      "train loss:0.012945444336436155\n",
      "train loss:0.022623680514592036\n",
      "train loss:0.012965565050991317\n",
      "train loss:0.030999396016035692\n",
      "train loss:0.029912042687802915\n",
      "train loss:0.01909667559788327\n",
      "train loss:0.006830335592231946\n",
      "train loss:0.026988823446523362\n",
      "train loss:0.025133631892926552\n",
      "train loss:0.0023178714361904353\n",
      "train loss:0.014116513630374606\n",
      "train loss:0.011796707270762296\n",
      "train loss:0.029097537032284862\n",
      "train loss:0.04293797208134965\n",
      "train loss:0.01710280496361954\n",
      "train loss:0.010917777328158242\n",
      "train loss:0.058017956365092625\n",
      "train loss:0.009088734328651092\n",
      "train loss:0.030071104202661084\n",
      "train loss:0.008825652146562241\n",
      "train loss:0.018194181041837537\n",
      "train loss:0.0047370857401410485\n",
      "train loss:0.03546672377901052\n",
      "train loss:0.02060164299228275\n",
      "train loss:0.010246981534308275\n",
      "train loss:0.010751043944983914\n",
      "train loss:0.017373579043086725\n",
      "train loss:0.010443752216648602\n",
      "train loss:0.007887671088307709\n",
      "train loss:0.0399516084439998\n",
      "train loss:0.016309300950193475\n",
      "train loss:0.012661874064219982\n",
      "train loss:0.0032730950365400333\n",
      "train loss:0.01917453127288161\n",
      "train loss:0.03486096130211766\n",
      "train loss:0.00710401357685151\n",
      "train loss:0.013061811029635453\n",
      "train loss:0.015508271288900184\n",
      "train loss:0.03318189939548532\n",
      "train loss:0.006688920507440696\n",
      "train loss:0.00912272655528236\n",
      "train loss:0.037093969324119944\n",
      "train loss:0.02277187453694859\n",
      "train loss:0.03123974364032633\n",
      "train loss:0.05942606323472509\n",
      "train loss:0.003064018981081575\n",
      "train loss:0.052098775528788444\n",
      "train loss:0.010575662997718886\n",
      "train loss:0.10044196039560654\n",
      "train loss:0.007998486947808315\n",
      "train loss:0.011072186240525639\n",
      "train loss:0.009643415894271938\n",
      "train loss:0.018487652269470627\n",
      "train loss:0.0062631094368257945\n",
      "train loss:0.016694768017046398\n",
      "train loss:0.0482240261495534\n",
      "train loss:0.02449008419540288\n",
      "train loss:0.007503397648444868\n",
      "train loss:0.059640021811569736\n",
      "train loss:0.006119703496913908\n",
      "train loss:0.019063190050693415\n",
      "train loss:0.02762508939541757\n",
      "train loss:0.02021297741980328\n",
      "train loss:0.009796859568861031\n",
      "train loss:0.1011795654065898\n",
      "train loss:0.007234889781182199\n",
      "train loss:0.020352843742787893\n",
      "train loss:0.04727382413963184\n",
      "train loss:0.02173862989556305\n",
      "train loss:0.033644826870240375\n",
      "train loss:0.022598997161485057\n",
      "train loss:0.002736099353504331\n",
      "train loss:0.004958208905649044\n",
      "train loss:0.02582290803725521\n",
      "train loss:0.021295373265991388\n",
      "train loss:0.02025112922365357\n",
      "train loss:0.03515881857625495\n",
      "train loss:0.011906095685632383\n",
      "train loss:0.029894492882459055\n",
      "train loss:0.005189697438730627\n",
      "train loss:0.00854993525875623\n",
      "train loss:0.008391907355661704\n",
      "train loss:0.20407373516413274\n",
      "train loss:0.015611505155755396\n",
      "train loss:0.03343057331699823\n",
      "train loss:0.011155213509424031\n",
      "train loss:0.03454818198831181\n",
      "train loss:0.03441633924164342\n",
      "train loss:0.005534336924578855\n",
      "train loss:0.029679941820601914\n",
      "train loss:0.07110146644350217\n",
      "train loss:0.029453961901041135\n",
      "train loss:0.011442586854678154\n",
      "train loss:0.010581910284062851\n",
      "train loss:0.01633200994004821\n",
      "train loss:0.005130559475530357\n",
      "train loss:0.042896035258221846\n",
      "train loss:0.0346437966744372\n",
      "train loss:0.03300242845613282\n",
      "train loss:0.013677397345242003\n",
      "train loss:0.004129835400134561\n",
      "train loss:0.02425006865440213\n",
      "train loss:0.011445167029671476\n",
      "train loss:0.005514257889506269\n",
      "train loss:0.024249989227026106\n",
      "train loss:0.011853313907413263\n",
      "train loss:0.022702140221905105\n",
      "train loss:0.014547323953141284\n",
      "train loss:0.014154175545076326\n",
      "train loss:0.011246407143135519\n",
      "train loss:0.010855153102260342\n",
      "train loss:0.01418265450716286\n",
      "train loss:0.06566107736322484\n",
      "train loss:0.06016004627165316\n",
      "train loss:0.010669137500584442\n",
      "train loss:0.007793367432252537\n",
      "train loss:0.024326603834690465\n",
      "train loss:0.0040811051827127805\n",
      "train loss:0.013163799027123967\n",
      "train loss:0.0042456751697265175\n",
      "train loss:0.014363452522622222\n",
      "train loss:0.019365224994419606\n",
      "train loss:0.02289367543315096\n",
      "train loss:0.00845053965054847\n",
      "train loss:0.010871137071442145\n",
      "train loss:0.024090363201422584\n",
      "train loss:0.029851134730742065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00964254322632577\n",
      "train loss:0.013333321647979407\n",
      "train loss:0.013435540195294901\n",
      "train loss:0.006878359733954176\n",
      "train loss:0.025606989298338864\n",
      "train loss:0.01597757548027746\n",
      "train loss:0.007674251452061971\n",
      "train loss:0.009923687469735891\n",
      "train loss:0.016080666484995444\n",
      "train loss:0.007893142290174304\n",
      "train loss:0.008591964921567195\n",
      "train loss:0.019434928912063956\n",
      "train loss:0.008696597220176887\n",
      "train loss:0.010104005603053034\n",
      "train loss:0.003900081517177082\n",
      "train loss:0.018461608600638283\n",
      "train loss:0.00454255378710957\n",
      "train loss:0.019270184496318784\n",
      "train loss:0.0037751108642573527\n",
      "train loss:0.01370546364143286\n",
      "train loss:0.009138253826831012\n",
      "train loss:0.008255645252071807\n",
      "train loss:0.035518545608072084\n",
      "train loss:0.011636309990739755\n",
      "train loss:0.007384346392230393\n",
      "train loss:0.010615266661056886\n",
      "train loss:0.054729069934074864\n",
      "train loss:0.005105611996098527\n",
      "train loss:0.0019102246762076596\n",
      "train loss:0.012167689863189122\n",
      "train loss:0.013807919895435214\n",
      "train loss:0.050276202535516526\n",
      "train loss:0.03195802832752058\n",
      "train loss:0.02219099595972516\n",
      "train loss:0.01492022458254584\n",
      "train loss:0.02480242092105939\n",
      "train loss:0.005680731275916597\n",
      "train loss:0.016409882964727788\n",
      "train loss:0.013604125317585584\n",
      "train loss:0.039911695615347846\n",
      "train loss:0.043417064850389125\n",
      "train loss:0.017987899450107373\n",
      "train loss:0.006000002027754358\n",
      "train loss:0.02700358843233657\n",
      "train loss:0.09321732037899903\n",
      "train loss:0.04168537418098896\n",
      "train loss:0.019248149701962543\n",
      "train loss:0.007071015097406517\n",
      "train loss:0.013431351013843615\n",
      "train loss:0.0415979203516575\n",
      "train loss:0.04367114403625323\n",
      "train loss:0.0055367340557717845\n",
      "train loss:0.02386544202294602\n",
      "train loss:0.0008248952327641105\n",
      "train loss:0.1406190650504002\n",
      "train loss:0.09566634952530187\n",
      "train loss:0.012442337772060106\n",
      "train loss:0.010287077682314436\n",
      "train loss:0.00859262946979082\n",
      "train loss:0.010993009958587147\n",
      "train loss:0.06698687846999633\n",
      "train loss:0.008688475879129205\n",
      "train loss:0.052488054475292804\n",
      "train loss:0.006139388134194365\n",
      "train loss:0.04254066337035769\n",
      "train loss:0.029875838213120013\n",
      "train loss:0.014865984327552116\n",
      "train loss:0.05631930167499026\n",
      "train loss:0.007883768104173966\n",
      "train loss:0.02363745439069668\n",
      "train loss:0.008644794616312219\n",
      "train loss:0.01550959784037673\n",
      "train loss:0.04673545222336152\n",
      "train loss:0.05184955928525233\n",
      "train loss:0.018145338242153836\n",
      "train loss:0.010794222093560544\n",
      "train loss:0.010950365418866919\n",
      "train loss:0.07810020878117975\n",
      "train loss:0.019102126376586384\n",
      "train loss:0.02963025634577514\n",
      "train loss:0.08091894854795513\n",
      "train loss:0.006043314374727563\n",
      "train loss:0.019894435631299404\n",
      "train loss:0.02456005384309186\n",
      "train loss:0.008657695663215002\n",
      "train loss:0.01471342000379785\n",
      "train loss:0.01001982921717599\n",
      "train loss:0.0068758264376875995\n",
      "train loss:0.06755182017828341\n",
      "train loss:0.03560776565774626\n",
      "train loss:0.01141477842333386\n",
      "train loss:0.04716229228997326\n",
      "train loss:0.04832534073984005\n",
      "train loss:0.07167888905332548\n",
      "train loss:0.005080580812967323\n",
      "train loss:0.014721001842806605\n",
      "train loss:0.06280721840250103\n",
      "train loss:0.019828339043022437\n",
      "train loss:0.031063684708009506\n",
      "train loss:0.01270268287189185\n",
      "train loss:0.028497840968031145\n",
      "train loss:0.014799308387367647\n",
      "train loss:0.016949002558642956\n",
      "train loss:0.018852928296684435\n",
      "train loss:0.004575051789424815\n",
      "train loss:0.03421673111208513\n",
      "train loss:0.021620439854304216\n",
      "train loss:0.0035483288558447253\n",
      "train loss:0.005446815011453953\n",
      "train loss:0.021416519341234955\n",
      "train loss:0.014965721711538857\n",
      "train loss:0.011266504524597981\n",
      "train loss:0.014962879065065627\n",
      "train loss:0.03095003978996732\n",
      "train loss:0.014539911788859067\n",
      "train loss:0.014147052912603533\n",
      "train loss:0.020943974099907336\n",
      "train loss:0.010569563857363164\n",
      "train loss:0.0427192577719223\n",
      "train loss:0.007757335395051717\n",
      "train loss:0.012329944126105375\n",
      "train loss:0.002914598233334495\n",
      "train loss:0.00835390568882961\n",
      "train loss:0.059628890230208835\n",
      "train loss:0.019444845723746534\n",
      "train loss:0.04227895733803908\n",
      "train loss:0.028430908560768273\n",
      "train loss:0.00836204244760725\n",
      "train loss:0.012048119028854465\n",
      "train loss:0.021337396010094833\n",
      "train loss:0.021264221574577444\n",
      "train loss:0.017419289008017168\n",
      "train loss:0.013573880214904941\n",
      "train loss:0.02950595404727664\n",
      "train loss:0.01102159518737833\n",
      "train loss:0.027367875917957196\n",
      "train loss:0.011248265166231224\n",
      "train loss:0.00821534954723625\n",
      "train loss:0.06574935091452479\n",
      "train loss:0.006223153983617287\n",
      "train loss:0.011081536584422173\n",
      "train loss:0.027866707535585306\n",
      "train loss:0.042173349403996294\n",
      "train loss:0.0026598691478564172\n",
      "train loss:0.026929018995891497\n",
      "train loss:0.010502045145646375\n",
      "train loss:0.03183963389140852\n",
      "train loss:0.0699000062922876\n",
      "train loss:0.11724792046481508\n",
      "train loss:0.006362240315489605\n",
      "train loss:0.01215031980062108\n",
      "train loss:0.04033353034735564\n",
      "train loss:0.0037830155530499316\n",
      "train loss:0.018668520937623757\n",
      "train loss:0.009006200473461477\n",
      "train loss:0.006992301797819873\n",
      "train loss:0.014204234120184314\n",
      "train loss:0.008984762837718499\n",
      "train loss:0.020913340555523338\n",
      "train loss:0.019617343068508823\n",
      "train loss:0.05269223924957228\n",
      "train loss:0.021446691397670258\n",
      "train loss:0.018547352877137664\n",
      "train loss:0.017842311469540083\n",
      "train loss:0.05301740777352545\n",
      "train loss:0.048812140838259396\n",
      "train loss:0.010229639132208406\n",
      "train loss:0.012491515386161311\n",
      "train loss:0.06343626559983144\n",
      "train loss:0.02146739373122018\n",
      "train loss:0.027176124781026087\n",
      "train loss:0.02668067127337458\n",
      "train loss:0.004898598562345629\n",
      "train loss:0.031192286851484362\n",
      "train loss:0.0565933125478643\n",
      "train loss:0.018067526192314307\n",
      "train loss:0.10093072717860804\n",
      "train loss:0.019702156183462143\n",
      "train loss:0.009400484959607706\n",
      "train loss:0.04458043275151149\n",
      "train loss:0.0674700001554878\n",
      "train loss:0.0061304334079957026\n",
      "train loss:0.013597711850680302\n",
      "train loss:0.011754187973775684\n",
      "train loss:0.030361341549561188\n",
      "train loss:0.012996957981780187\n",
      "train loss:0.003652408651442714\n",
      "train loss:0.05169818883547998\n",
      "train loss:0.01355864776771837\n",
      "train loss:0.020691351924715537\n",
      "train loss:0.010483064625074739\n",
      "train loss:0.005706146828469561\n",
      "train loss:0.008319813047034894\n",
      "train loss:0.014642639275566818\n",
      "train loss:0.011359345525868446\n",
      "train loss:0.013441202447105933\n",
      "train loss:0.024913470621028208\n",
      "train loss:0.013320039970414437\n",
      "train loss:0.007311670260398874\n",
      "train loss:0.005145352717500607\n",
      "train loss:0.029607570943778652\n",
      "train loss:0.016435073783992983\n",
      "train loss:0.013587171146494315\n",
      "train loss:0.013049515744565168\n",
      "train loss:0.03930484371196865\n",
      "train loss:0.010944962261931462\n",
      "train loss:0.002798349904400468\n",
      "train loss:0.03846005893288975\n",
      "train loss:0.002342691674521791\n",
      "train loss:0.01412879306483638\n",
      "train loss:0.044550200128884956\n",
      "train loss:0.01988653804222199\n",
      "train loss:0.017149774441808076\n",
      "train loss:0.056797309602049796\n",
      "train loss:0.04757380194948163\n",
      "train loss:0.002365971761871327\n",
      "train loss:0.00549278718215381\n",
      "train loss:0.023952982323684237\n",
      "train loss:0.02288445628260503\n",
      "train loss:0.04022116230212354\n",
      "train loss:0.054688742331313295\n",
      "train loss:0.06327876988590814\n",
      "train loss:0.0488906734100569\n",
      "train loss:0.0540863205249359\n",
      "train loss:0.03916147622739241\n",
      "train loss:0.05272938536074166\n",
      "train loss:0.025590843724438074\n",
      "train loss:0.006954884934166089\n",
      "train loss:0.011610131475870493\n",
      "train loss:0.009635547600619849\n",
      "train loss:0.05973622275148727\n",
      "train loss:0.03455200466317431\n",
      "train loss:0.004023902910903488\n",
      "train loss:0.007622503564995804\n",
      "train loss:0.014575533797428417\n",
      "train loss:0.008022198035930999\n",
      "train loss:0.037380969197364756\n",
      "train loss:0.012291496491376903\n",
      "train loss:0.011968135167493255\n",
      "train loss:0.0245607246494862\n",
      "train loss:0.022146308854844633\n",
      "train loss:0.08038792262882372\n",
      "train loss:0.06672913566901027\n",
      "train loss:0.04047100238765859\n",
      "train loss:0.027049840103409134\n",
      "train loss:0.014264605327018455\n",
      "train loss:0.026469538390546474\n",
      "train loss:0.0365439514638996\n",
      "train loss:0.052659542486409326\n",
      "train loss:0.009681852810243585\n",
      "train loss:0.026903898891091876\n",
      "train loss:0.004494245976740454\n",
      "train loss:0.054101204102806895\n",
      "train loss:0.11307289117135097\n",
      "train loss:0.003064100205183126\n",
      "train loss:0.02152353897814522\n",
      "train loss:0.009663699949666904\n",
      "train loss:0.010973446753801614\n",
      "train loss:0.028413447857286757\n",
      "train loss:0.011402405037337904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00891776772637444\n",
      "train loss:0.04295069195897328\n",
      "train loss:0.01800925291974232\n",
      "train loss:0.010098945156850067\n",
      "train loss:0.015306934426439534\n",
      "train loss:0.025666661638216636\n",
      "train loss:0.03294171731689523\n",
      "train loss:0.03943472407705994\n",
      "train loss:0.019901423473028375\n",
      "train loss:0.007640234330975805\n",
      "train loss:0.014659325824348918\n",
      "train loss:0.013332407107090912\n",
      "train loss:0.04579885376199069\n",
      "train loss:0.08095410173898969\n",
      "train loss:0.0039848092831173\n",
      "train loss:0.06837736990719036\n",
      "train loss:0.012338984956915383\n",
      "train loss:0.006458757606117708\n",
      "train loss:0.010017527725489462\n",
      "train loss:0.00980549585379904\n",
      "train loss:0.03443321053504874\n",
      "train loss:0.018157596498193013\n",
      "train loss:0.02176445676966805\n",
      "train loss:0.021188166626323248\n",
      "train loss:0.021133122490310643\n",
      "train loss:0.04197356327906289\n",
      "train loss:0.004417040802417054\n",
      "train loss:0.047525882383338984\n",
      "train loss:0.019950448623836527\n",
      "train loss:0.047266418823024396\n",
      "train loss:0.006290431659468783\n",
      "train loss:0.011112766520352639\n",
      "train loss:0.023662930645719144\n",
      "train loss:0.013509618540484415\n",
      "train loss:0.013917346134044044\n",
      "train loss:0.010918034372352795\n",
      "train loss:0.019809413147926253\n",
      "train loss:0.065654093124083\n",
      "train loss:0.030274556229273938\n",
      "train loss:0.0068665405103947195\n",
      "train loss:0.004223483434420106\n",
      "train loss:0.01770847019289776\n",
      "train loss:0.04086434614390762\n",
      "train loss:0.050082667754532365\n",
      "train loss:0.06543072796297966\n",
      "train loss:0.032572860493018975\n",
      "train loss:0.012608387153503962\n",
      "train loss:0.02085359418351123\n",
      "train loss:0.005478514473971261\n",
      "train loss:0.054010350572108796\n",
      "train loss:0.02863981117680286\n",
      "train loss:0.0145084858490624\n",
      "train loss:0.006386992141219252\n",
      "train loss:0.012198245662386\n",
      "train loss:0.010764485108433409\n",
      "train loss:0.010499752757704697\n",
      "train loss:0.09757953957016946\n",
      "train loss:0.02664387444845744\n",
      "train loss:0.006627269137487523\n",
      "train loss:0.015767090621436455\n",
      "train loss:0.005763869376373224\n",
      "train loss:0.005482844818830642\n",
      "train loss:0.025040449099834695\n",
      "train loss:0.004179452112908509\n",
      "train loss:0.010050845860778876\n",
      "train loss:0.009155823718629072\n",
      "train loss:0.01713981068775891\n",
      "train loss:0.011207271246164417\n",
      "train loss:0.005202347160914471\n",
      "train loss:0.01735745089541968\n",
      "train loss:0.02928301476124489\n",
      "train loss:0.01559950493704929\n",
      "train loss:0.014296849879656449\n",
      "train loss:0.007209468061720958\n",
      "train loss:0.006218290972493396\n",
      "train loss:0.01040009479683925\n",
      "train loss:0.04899471659473768\n",
      "train loss:0.03806023120463594\n",
      "train loss:0.022074835913160427\n",
      "train loss:0.0024264426843337112\n",
      "train loss:0.02747282169433926\n",
      "train loss:0.04737422576347469\n",
      "train loss:0.005364231928303895\n",
      "train loss:0.016947602054314254\n",
      "train loss:0.014808131666752837\n",
      "train loss:0.017525615339259268\n",
      "train loss:0.01764297715845658\n",
      "train loss:0.043476426559618774\n",
      "train loss:0.011491226605794045\n",
      "train loss:0.005994360752819874\n",
      "train loss:0.007750651574956323\n",
      "train loss:0.014849332546440547\n",
      "train loss:0.026844308783500855\n",
      "train loss:0.017570395216236708\n",
      "train loss:0.05303249723263182\n",
      "train loss:0.0054300454461962185\n",
      "train loss:0.01790773519848537\n",
      "train loss:0.015176546040906987\n",
      "train loss:0.015870341654292523\n",
      "train loss:0.0033945920053298924\n",
      "train loss:0.01384211129465606\n",
      "train loss:0.046087426381332584\n",
      "train loss:0.03444260150964713\n",
      "train loss:0.004638580675398463\n",
      "train loss:0.020719581667887805\n",
      "train loss:0.002573926651894869\n",
      "train loss:0.006771888065054442\n",
      "train loss:0.00285554603418504\n",
      "train loss:0.031019159346927852\n",
      "train loss:0.03711887865811586\n",
      "train loss:0.01317756658487992\n",
      "train loss:0.0045058584285968\n",
      "train loss:0.038323096063004415\n",
      "train loss:0.007234264100424659\n",
      "train loss:0.014187066243117316\n",
      "train loss:0.050233394739302134\n",
      "train loss:0.008542966126280178\n",
      "train loss:0.038390745859331846\n",
      "train loss:0.007516922000024469\n",
      "train loss:0.007484859085291765\n",
      "train loss:0.05278020333388647\n",
      "train loss:0.013187400367453523\n",
      "train loss:0.002290425294640949\n",
      "train loss:0.016591715447126894\n",
      "train loss:0.03208620600608423\n",
      "train loss:0.020022695933664934\n",
      "train loss:0.07519421130181626\n",
      "train loss:0.047589311109916946\n",
      "train loss:0.03320800942670309\n",
      "train loss:0.005445369063029297\n",
      "train loss:0.02718062429726514\n",
      "train loss:0.017068713114125562\n",
      "train loss:0.013418744655293373\n",
      "train loss:0.018232340743369567\n",
      "train loss:0.0381266980790188\n",
      "train loss:0.04857771575038179\n",
      "train loss:0.010664058517508337\n",
      "train loss:0.023892073514386724\n",
      "train loss:0.004200046594284055\n",
      "train loss:0.007401784827108332\n",
      "train loss:0.00357205825872006\n",
      "train loss:0.05498973078475573\n",
      "train loss:0.008643187674065586\n",
      "train loss:0.045232328969477494\n",
      "train loss:0.028127182182088747\n",
      "train loss:0.010203398662367373\n",
      "train loss:0.040191051304193974\n",
      "train loss:0.010923166054545514\n",
      "train loss:0.006271705855743836\n",
      "train loss:0.005313879376386995\n",
      "train loss:0.022287391264712526\n",
      "train loss:0.017018310504662248\n",
      "train loss:0.03181371148008745\n",
      "train loss:0.0042239242066741435\n",
      "train loss:0.013949883789376522\n",
      "train loss:0.008143264591403614\n",
      "train loss:0.014254644091973231\n",
      "train loss:0.019443415557343243\n",
      "train loss:0.02803043487653023\n",
      "train loss:0.03184043088011874\n",
      "train loss:0.01595792361139649\n",
      "train loss:0.08614996688449322\n",
      "train loss:0.00678566590021531\n",
      "train loss:0.021496148816857876\n",
      "train loss:0.034717476217645635\n",
      "train loss:0.013461228764129807\n",
      "train loss:0.03170659044038042\n",
      "train loss:0.01320266231064601\n",
      "train loss:0.05524615811863222\n",
      "train loss:0.005061718941927054\n",
      "train loss:0.014652849761910069\n",
      "train loss:0.032053003377201225\n",
      "train loss:0.006009217703274121\n",
      "train loss:0.010488734033890758\n",
      "train loss:0.006896067436297733\n",
      "train loss:0.021564542916976757\n",
      "train loss:0.007916352137674419\n",
      "train loss:0.036888075041051044\n",
      "train loss:0.006180206381793165\n",
      "train loss:0.014405803037641243\n",
      "train loss:0.01100041765699311\n",
      "=== epoch:7, train acc:0.988, test acc:0.984 ===\n",
      "train loss:0.032228720100986055\n",
      "train loss:0.005194701084461734\n",
      "train loss:0.01199872765827289\n",
      "train loss:0.003912892930807843\n",
      "train loss:0.04115374783350614\n",
      "train loss:0.00766747057127284\n",
      "train loss:0.023502206666516354\n",
      "train loss:0.007095377320217482\n",
      "train loss:0.005415016057618629\n",
      "train loss:0.021298286791607512\n",
      "train loss:0.03400573100165772\n",
      "train loss:0.013197418314317953\n",
      "train loss:0.03287768760505793\n",
      "train loss:0.019306677071495557\n",
      "train loss:0.032887831205222984\n",
      "train loss:0.019057537323603584\n",
      "train loss:0.008710613596446646\n",
      "train loss:0.04444474862749729\n",
      "train loss:0.033850379583578276\n",
      "train loss:0.0017865246984149453\n",
      "train loss:0.004375649333921441\n",
      "train loss:0.025985130034189927\n",
      "train loss:0.012160283679855756\n",
      "train loss:0.0077712838174238685\n",
      "train loss:0.00812134790024794\n",
      "train loss:0.015192873132546119\n",
      "train loss:0.1282538372610701\n",
      "train loss:0.008626041119115463\n",
      "train loss:0.07540153456357715\n",
      "train loss:0.03539495379198601\n",
      "train loss:0.0031924952287521345\n",
      "train loss:0.01685786684225771\n",
      "train loss:0.018235156411089245\n",
      "train loss:0.008194249292922295\n",
      "train loss:0.008028094712042773\n",
      "train loss:0.016984187603040265\n",
      "train loss:0.02170593052524368\n",
      "train loss:0.0025350392893005998\n",
      "train loss:0.006913456512728007\n",
      "train loss:0.024800079833397742\n",
      "train loss:0.018903054792313\n",
      "train loss:0.002834792850442672\n",
      "train loss:0.007911955930519143\n",
      "train loss:0.05914242540665083\n",
      "train loss:0.015238542240260944\n",
      "train loss:0.008740430926648408\n",
      "train loss:0.02972459059655574\n",
      "train loss:0.010757903139042733\n",
      "train loss:0.0018039945039739092\n",
      "train loss:0.008257260768587316\n",
      "train loss:0.08004213747693885\n",
      "train loss:0.018433510441872837\n",
      "train loss:0.007587998411395946\n",
      "train loss:0.0067927426585704005\n",
      "train loss:0.01942800457195014\n",
      "train loss:0.017368893377937508\n",
      "train loss:0.014467714985746656\n",
      "train loss:0.011198742704057582\n",
      "train loss:0.030513854848863792\n",
      "train loss:0.027762791574052818\n",
      "train loss:0.03195110463323875\n",
      "train loss:0.02939908449704271\n",
      "train loss:0.025885995358720458\n",
      "train loss:0.011047147909051203\n",
      "train loss:0.014377344969767813\n",
      "train loss:0.11601548489945897\n",
      "train loss:0.004225019905385678\n",
      "train loss:0.021702862320418948\n",
      "train loss:0.020892114409399646\n",
      "train loss:0.04647986780266557\n",
      "train loss:0.0028489246097654548\n",
      "train loss:0.005572927616561499\n",
      "train loss:0.017000006470826976\n",
      "train loss:0.014113872141683447\n",
      "train loss:0.008096096499478025\n",
      "train loss:0.013700660046941504\n",
      "train loss:0.03631669431179269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01510585373175327\n",
      "train loss:0.017869923562677848\n",
      "train loss:0.010940071801621914\n",
      "train loss:0.01993336651397859\n",
      "train loss:0.024202697416715484\n",
      "train loss:0.015089395599772779\n",
      "train loss:0.004030007650752313\n",
      "train loss:0.004544337871222428\n",
      "train loss:0.02113527212896079\n",
      "train loss:0.00728624748880173\n",
      "train loss:0.008891633888900779\n",
      "train loss:0.004489320380129357\n",
      "train loss:0.03624214228840757\n",
      "train loss:0.055491389004089564\n",
      "train loss:0.013673629123472858\n",
      "train loss:0.012057587096498974\n",
      "train loss:0.024279140186751976\n",
      "train loss:0.027834293497332643\n",
      "train loss:0.027584536524718307\n",
      "train loss:0.025290768530908544\n",
      "train loss:0.005570241153253406\n",
      "train loss:0.007219905922047599\n",
      "train loss:0.02038581809381274\n",
      "train loss:0.002166716585864907\n",
      "train loss:0.0031158964865459875\n",
      "train loss:0.018744048595689882\n",
      "train loss:0.02783668223505891\n",
      "train loss:0.023439109752117604\n",
      "train loss:0.02234002219473819\n",
      "train loss:0.03256407457983306\n",
      "train loss:0.013490707565564652\n",
      "train loss:0.05659720366884839\n",
      "train loss:0.04311021545245526\n",
      "train loss:0.012974633515763702\n",
      "train loss:0.020886716139001855\n",
      "train loss:0.01511865612921236\n",
      "train loss:0.010821258080302756\n",
      "train loss:0.028776660730641022\n",
      "train loss:0.06203636945927908\n",
      "train loss:0.05016792573206391\n",
      "train loss:0.03312249896738473\n",
      "train loss:0.007675022887879072\n",
      "train loss:0.007876594123905506\n",
      "train loss:0.0029049443611480525\n",
      "train loss:0.01659062412177165\n",
      "train loss:0.005501110302314174\n",
      "train loss:0.010716076118491822\n",
      "train loss:0.005477962938767115\n",
      "train loss:0.02825288215944485\n",
      "train loss:0.04613999112880708\n",
      "train loss:0.006287161472429717\n",
      "train loss:0.010176538480103454\n",
      "train loss:0.0022127593061296537\n",
      "train loss:0.009683078951405116\n",
      "train loss:0.06414835183736908\n",
      "train loss:0.0162551452409702\n",
      "train loss:0.01719306704074859\n",
      "train loss:0.00887863260819274\n",
      "train loss:0.016012215911725632\n",
      "train loss:0.006096860012292083\n",
      "train loss:0.021743998422184347\n",
      "train loss:0.005920376196280584\n",
      "train loss:0.01981508627288048\n",
      "train loss:0.03477266660864728\n",
      "train loss:0.020927913930771605\n",
      "train loss:0.034531618394804585\n",
      "train loss:0.018253124442725765\n",
      "train loss:0.013306268973594217\n",
      "train loss:0.03383592453619943\n",
      "train loss:0.01671880486345188\n",
      "train loss:0.009580298438462323\n",
      "train loss:0.006576733960417829\n",
      "train loss:0.011467223453937374\n",
      "train loss:0.04040758075823286\n",
      "train loss:0.019320621044495955\n",
      "train loss:0.02791486010116603\n",
      "train loss:0.01999272387366228\n",
      "train loss:0.005063389981792703\n",
      "train loss:0.014642243261853134\n",
      "train loss:0.012061582246794206\n",
      "train loss:0.03316914283639761\n",
      "train loss:0.00532121011154625\n",
      "train loss:0.013214659313272979\n",
      "train loss:0.015508836704697748\n",
      "train loss:0.031778538151070276\n",
      "train loss:0.014459606233879607\n",
      "train loss:0.028724269358143862\n",
      "train loss:0.00848979082504875\n",
      "train loss:0.011520322359557147\n",
      "train loss:0.03226194297247319\n",
      "train loss:0.005019864870432869\n",
      "train loss:0.034381589526505586\n",
      "train loss:0.004953004575238763\n",
      "train loss:0.012483700944088955\n",
      "train loss:0.0030769176589482265\n",
      "train loss:0.029184732252345323\n",
      "train loss:0.03625840522491368\n",
      "train loss:0.022072413777644014\n",
      "train loss:0.015784098422534417\n",
      "train loss:0.038524506666975517\n",
      "train loss:0.03684122817439792\n",
      "train loss:0.017922876024547917\n",
      "train loss:0.012471560566064983\n",
      "train loss:0.024986311577673913\n",
      "train loss:0.013030148766917105\n",
      "train loss:0.09430774249595097\n",
      "train loss:0.015761569900514857\n",
      "train loss:0.004053408666568903\n",
      "train loss:0.012252742993862835\n",
      "train loss:0.01973553840830922\n",
      "train loss:0.048894242335698654\n",
      "train loss:0.04238830261232038\n",
      "train loss:0.006186245947235786\n",
      "train loss:0.007824907105411451\n",
      "train loss:0.006091944647656134\n",
      "train loss:0.012266179449697615\n",
      "train loss:0.014784053214671675\n",
      "train loss:0.008238403707828239\n",
      "train loss:0.003162679083550472\n",
      "train loss:0.023226248712176507\n",
      "train loss:0.023639336838120372\n",
      "train loss:0.007697299504886079\n",
      "train loss:0.003027423075638986\n",
      "train loss:0.008084893723229512\n",
      "train loss:0.00855273861101285\n",
      "train loss:0.022747529920626296\n",
      "train loss:0.0034229563236131333\n",
      "train loss:0.021866254711164452\n",
      "train loss:0.00819174439364446\n",
      "train loss:0.01918172441263595\n",
      "train loss:0.02214698189415479\n",
      "train loss:0.046157435531212314\n",
      "train loss:0.012605903114634316\n",
      "train loss:0.004709914403197641\n",
      "train loss:0.01416828655691555\n",
      "train loss:0.0792949000431062\n",
      "train loss:0.01648204761916781\n",
      "train loss:0.006395606952571063\n",
      "train loss:0.06531634885108571\n",
      "train loss:0.0070816972816582805\n",
      "train loss:0.04762635888551481\n",
      "train loss:0.037720455563990896\n",
      "train loss:0.006086253862991702\n",
      "train loss:0.0021239655584577364\n",
      "train loss:0.004943305491119507\n",
      "train loss:0.020445327659573045\n",
      "train loss:0.01054998512914521\n",
      "train loss:0.0022423010754187727\n",
      "train loss:0.012965328624395724\n",
      "train loss:0.00394362330870314\n",
      "train loss:0.019456774636033247\n",
      "train loss:0.014287213998854098\n",
      "train loss:0.00766676964553891\n",
      "train loss:0.004179916764391024\n",
      "train loss:0.012632770776939589\n",
      "train loss:0.024236004913827762\n",
      "train loss:0.056671318066375716\n",
      "train loss:0.00446106150443003\n",
      "train loss:0.024761106015955908\n",
      "train loss:0.011956840020942676\n",
      "train loss:0.004439026304194294\n",
      "train loss:0.015010876619900793\n",
      "train loss:0.007763342455579887\n",
      "train loss:0.02844437983796071\n",
      "train loss:0.015102369192667265\n",
      "train loss:0.007009995619055642\n",
      "train loss:0.009879054174805125\n",
      "train loss:0.0049035593676085\n",
      "train loss:0.024399081592293665\n",
      "train loss:0.043106685095287076\n",
      "train loss:0.03443616029295715\n",
      "train loss:0.0076038755381341635\n",
      "train loss:0.009473625450327032\n",
      "train loss:0.006283078691849091\n",
      "train loss:0.0052912359187575805\n",
      "train loss:0.0124410948763143\n",
      "train loss:0.0021446262862941257\n",
      "train loss:0.0071011245141610335\n",
      "train loss:0.024432312927257102\n",
      "train loss:0.01824414479798845\n",
      "train loss:0.0068615234640719645\n",
      "train loss:0.024736191813074165\n",
      "train loss:0.013410028893485658\n",
      "train loss:0.06485949023045043\n",
      "train loss:0.05321110384370492\n",
      "train loss:0.01751073254065389\n",
      "train loss:0.01621567912601527\n",
      "train loss:0.010421260675670603\n",
      "train loss:0.028500376407275783\n",
      "train loss:0.006907378484620149\n",
      "train loss:0.00935815907266622\n",
      "train loss:0.011191685520790086\n",
      "train loss:0.005700676961283211\n",
      "train loss:0.015005810754292572\n",
      "train loss:0.009104670541782756\n",
      "train loss:0.008201471675507086\n",
      "train loss:0.003330716121026708\n",
      "train loss:0.01963627933448541\n",
      "train loss:0.014349264380410042\n",
      "train loss:0.03737631987262241\n",
      "train loss:0.015019826765428783\n",
      "train loss:0.006178037361233874\n",
      "train loss:0.010347703174253606\n",
      "train loss:0.004967041449966302\n",
      "train loss:0.01993173882710155\n",
      "train loss:0.019712238916688163\n",
      "train loss:0.0036665901757317425\n",
      "train loss:0.0015036355006822263\n",
      "train loss:0.007470877820080935\n",
      "train loss:0.02103243475360861\n",
      "train loss:0.009769197290983751\n",
      "train loss:0.03266667368703285\n",
      "train loss:0.010575281619042025\n",
      "train loss:0.006565486409003441\n",
      "train loss:0.006184913683369684\n",
      "train loss:0.015793385508173906\n",
      "train loss:0.0044189064998262605\n",
      "train loss:0.030971585413024024\n",
      "train loss:0.004172910164012245\n",
      "train loss:0.012270943277993648\n",
      "train loss:0.013228507171620585\n",
      "train loss:0.004220645898319631\n",
      "train loss:0.010887326111483419\n",
      "train loss:0.004780341742790257\n",
      "train loss:0.008177243672212427\n",
      "train loss:0.01785079645215279\n",
      "train loss:0.02401682198965175\n",
      "train loss:0.012143804675477772\n",
      "train loss:0.08640734047321504\n",
      "train loss:0.00751736754723259\n",
      "train loss:0.026488820241170618\n",
      "train loss:0.03979619201848972\n",
      "train loss:0.011652444227406413\n",
      "train loss:0.006288920624056507\n",
      "train loss:0.0060988039118148344\n",
      "train loss:0.018141882776836104\n",
      "train loss:0.027951961706703315\n",
      "train loss:0.012162938341028407\n",
      "train loss:0.014898966610341782\n",
      "train loss:0.0069486476539612875\n",
      "train loss:0.04226139811254633\n",
      "train loss:0.017236131663134903\n",
      "train loss:0.008275473984772914\n",
      "train loss:0.020558892926382175\n",
      "train loss:0.03493594195156533\n",
      "train loss:0.008335164773265666\n",
      "train loss:0.0054893897968010955\n",
      "train loss:0.016241493231753144\n",
      "train loss:0.01821452775654863\n",
      "train loss:0.012063355123781927\n",
      "train loss:0.013121641211377719\n",
      "train loss:0.015779671565889313\n",
      "train loss:0.04328507636738675\n",
      "train loss:0.025001534960339813\n",
      "train loss:0.020574260273816047\n",
      "train loss:0.08167168834983869\n",
      "train loss:0.006975343036654515\n",
      "train loss:0.02045211958820235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01852436030816436\n",
      "train loss:0.013518697690824064\n",
      "train loss:0.0026999757552581094\n",
      "train loss:0.009657116094206669\n",
      "train loss:0.02228084581489054\n",
      "train loss:0.004659406025998844\n",
      "train loss:0.09210981774534108\n",
      "train loss:0.010627869618102716\n",
      "train loss:0.018285085594693325\n",
      "train loss:0.024503639420797283\n",
      "train loss:0.04109260267973852\n",
      "train loss:0.004431715982198175\n",
      "train loss:0.01224696340450726\n",
      "train loss:0.020537067528582403\n",
      "train loss:0.01801853621536567\n",
      "train loss:0.06092916436164609\n",
      "train loss:0.017273928243268614\n",
      "train loss:0.02311217092472333\n",
      "train loss:0.01986813398760088\n",
      "train loss:0.024713251726721116\n",
      "train loss:0.0035275313289884077\n",
      "train loss:0.03517918544494944\n",
      "train loss:0.00782961933083206\n",
      "train loss:0.042564859278194735\n",
      "train loss:0.006306236743507966\n",
      "train loss:0.06096821583445653\n",
      "train loss:0.007989896564939494\n",
      "train loss:0.0038359862448235026\n",
      "train loss:0.03650580679560691\n",
      "train loss:0.03808312607017486\n",
      "train loss:0.036460676504698836\n",
      "train loss:0.020834394491686083\n",
      "train loss:0.016707430186707978\n",
      "train loss:0.05368654848208229\n",
      "train loss:0.011617064132395353\n",
      "train loss:0.00146153010934048\n",
      "train loss:0.030348044551959674\n",
      "train loss:0.008483876939414381\n",
      "train loss:0.008541190793788586\n",
      "train loss:0.036353607427837054\n",
      "train loss:0.06063503647547266\n",
      "train loss:0.004578215235793231\n",
      "train loss:0.016749456229268322\n",
      "train loss:0.046713063548030204\n",
      "train loss:0.02471964606036982\n",
      "train loss:0.04428266412159579\n",
      "train loss:0.012945456876840414\n",
      "train loss:0.004387673179183219\n",
      "train loss:0.028099940290356545\n",
      "train loss:0.019799550166734407\n",
      "train loss:0.004465065219185298\n",
      "train loss:0.014359190990542748\n",
      "train loss:0.01662662616744281\n",
      "train loss:0.010112271798270615\n",
      "train loss:0.03550263978141968\n",
      "train loss:0.0036432559670130575\n",
      "train loss:0.016156719050640776\n",
      "train loss:0.050676009855323986\n",
      "train loss:0.01109719489130998\n",
      "train loss:0.030679154214937707\n",
      "train loss:0.0058680147707618905\n",
      "train loss:0.009546631043889307\n",
      "train loss:0.048205815984751196\n",
      "train loss:0.0367812906540691\n",
      "train loss:0.017960426515294458\n",
      "train loss:0.0028505924324459904\n",
      "train loss:0.00403128199224373\n",
      "train loss:0.03511268124520416\n",
      "train loss:0.007068934542856643\n",
      "train loss:0.00843676121668781\n",
      "train loss:0.017271670898072022\n",
      "train loss:0.10237559019910092\n",
      "train loss:0.006727768683730511\n",
      "train loss:0.017538345171231658\n",
      "train loss:0.012251404727479874\n",
      "train loss:0.015013519522119344\n",
      "train loss:0.012259534900448834\n",
      "train loss:0.02109764705885301\n",
      "train loss:0.00917113611110239\n",
      "train loss:0.009284044478358552\n",
      "train loss:0.004292976116054221\n",
      "train loss:0.026133811908997592\n",
      "train loss:0.005951045526763517\n",
      "train loss:0.03214522767202097\n",
      "train loss:0.018174629061850946\n",
      "train loss:0.01499705437369042\n",
      "train loss:0.003821874489448167\n",
      "train loss:0.020757463506344855\n",
      "train loss:0.014712818751979814\n",
      "train loss:0.008349944669538477\n",
      "train loss:0.013232307671479641\n",
      "train loss:0.039189830320011186\n",
      "train loss:0.0054126533450943705\n",
      "train loss:0.012136011507560624\n",
      "train loss:0.024571606136913244\n",
      "train loss:0.014147076408058916\n",
      "train loss:0.04732278168233879\n",
      "train loss:0.034735633986405244\n",
      "train loss:0.030648187356354054\n",
      "train loss:0.034872230252025224\n",
      "train loss:0.020666801752406854\n",
      "train loss:0.011203480622555296\n",
      "train loss:0.005120358363630688\n",
      "train loss:0.017196635161247198\n",
      "train loss:0.008952366346220346\n",
      "train loss:0.002249048754412318\n",
      "train loss:0.0027948050937856316\n",
      "train loss:0.006153198798444849\n",
      "train loss:0.18267456482379685\n",
      "train loss:0.017512738639247172\n",
      "train loss:0.020952654959324513\n",
      "train loss:0.004587652763951328\n",
      "train loss:0.08057376635252365\n",
      "train loss:0.012900676494511647\n",
      "train loss:0.019646151603062424\n",
      "train loss:0.007695759688859533\n",
      "train loss:0.014175346993810295\n",
      "train loss:0.00797368568766327\n",
      "train loss:0.009857832955607384\n",
      "train loss:0.018880259726498667\n",
      "train loss:0.009333715518274577\n",
      "train loss:0.017579243687903864\n",
      "train loss:0.014519407862577658\n",
      "train loss:0.0751421946507395\n",
      "train loss:0.043003100359691265\n",
      "train loss:0.02352721092818217\n",
      "train loss:0.017085627415037417\n",
      "train loss:0.06822784565674812\n",
      "train loss:0.007029035636424791\n",
      "train loss:0.023301538697911538\n",
      "train loss:0.016863573956053764\n",
      "train loss:0.01758556047101275\n",
      "train loss:0.023396921372654957\n",
      "train loss:0.0014624145040455833\n",
      "train loss:0.006756413660485539\n",
      "train loss:0.0022795714041238674\n",
      "train loss:0.006282981110022059\n",
      "train loss:0.03536740149471038\n",
      "train loss:0.010344888969217939\n",
      "train loss:0.03586096630868965\n",
      "train loss:0.017697993636307315\n",
      "train loss:0.007649159137178548\n",
      "train loss:0.014970129008626869\n",
      "train loss:0.00914042902301042\n",
      "train loss:0.00735387633284221\n",
      "train loss:0.027266677076438692\n",
      "train loss:0.035598767600600496\n",
      "train loss:0.006311053021306539\n",
      "train loss:0.031685644978271414\n",
      "train loss:0.02056015169740223\n",
      "train loss:0.036772059994575536\n",
      "train loss:0.028222011701731067\n",
      "train loss:0.003984687253063542\n",
      "train loss:0.023682446749408427\n",
      "train loss:0.015726687186050553\n",
      "train loss:0.014076037722351947\n",
      "train loss:0.018010204214991835\n",
      "train loss:0.012960650439401367\n",
      "train loss:0.0059626836503844215\n",
      "train loss:0.016583561014483134\n",
      "train loss:0.0027594322392331156\n",
      "train loss:0.04824200893642639\n",
      "train loss:0.009588499019165088\n",
      "train loss:0.012538998180404872\n",
      "train loss:0.03951232724617606\n",
      "train loss:0.0034330082752364404\n",
      "train loss:0.08242598091749227\n",
      "train loss:0.005795428564814476\n",
      "train loss:0.013343407082980435\n",
      "train loss:0.01808146811120455\n",
      "train loss:0.004833896462138293\n",
      "train loss:0.05339310554846875\n",
      "train loss:0.002408311597849923\n",
      "train loss:0.021800786882546042\n",
      "train loss:0.013089962650507578\n",
      "train loss:0.0059778603068262\n",
      "train loss:0.01000430745385136\n",
      "train loss:0.01656475541949509\n",
      "train loss:0.03355213902862525\n",
      "train loss:0.006474555502308337\n",
      "train loss:0.0010019218236381835\n",
      "train loss:0.011238570037160868\n",
      "train loss:0.004047431113850525\n",
      "train loss:0.01781458154902841\n",
      "train loss:0.006288061203438314\n",
      "train loss:0.023076316044430098\n",
      "train loss:0.007958941011747015\n",
      "train loss:0.006443435259622009\n",
      "train loss:0.002708349153057442\n",
      "train loss:0.0021054876681024085\n",
      "train loss:0.018602668877620927\n",
      "train loss:0.047857905304120305\n",
      "train loss:0.005331584276784377\n",
      "train loss:0.014065261728286624\n",
      "train loss:0.009934036238868193\n",
      "train loss:0.007781133214265561\n",
      "train loss:0.04490533420907595\n",
      "train loss:0.004140504985906004\n",
      "train loss:0.01086546082812556\n",
      "train loss:0.0070507707955332725\n",
      "train loss:0.003122778308594519\n",
      "train loss:0.0030807187834065177\n",
      "train loss:0.003972726168056579\n",
      "train loss:0.05465513200705611\n",
      "train loss:0.013381256481902283\n",
      "train loss:0.006879873270403847\n",
      "train loss:0.007774893782909222\n",
      "train loss:0.0691979836437006\n",
      "train loss:0.07619939393706904\n",
      "train loss:0.008961296932928981\n",
      "train loss:0.02732702211549324\n",
      "train loss:0.014402297636418326\n",
      "train loss:0.005601036914152872\n",
      "train loss:0.016648983092467495\n",
      "train loss:0.005909118939999689\n",
      "train loss:0.006342326947705711\n",
      "train loss:0.04501899625553827\n",
      "train loss:0.06652439411526834\n",
      "train loss:0.009688212943934503\n",
      "train loss:0.009613017679554879\n",
      "train loss:0.009298944904401023\n",
      "train loss:0.06396373834889273\n",
      "train loss:0.007751560104916014\n",
      "train loss:0.011574187238481087\n",
      "train loss:0.009577806543592911\n",
      "train loss:0.011797353113279972\n",
      "train loss:0.00848773166615117\n",
      "train loss:0.09864728858851869\n",
      "train loss:0.014855356185303488\n",
      "train loss:0.007038889227332267\n",
      "train loss:0.012803387067688504\n",
      "train loss:0.012169063569093346\n",
      "train loss:0.012065161468488688\n",
      "train loss:0.0220838247309554\n",
      "train loss:0.0064368742988074315\n",
      "train loss:0.011860178018227203\n",
      "train loss:0.01592984673583241\n",
      "train loss:0.006296969146943604\n",
      "train loss:0.010792313790428231\n",
      "train loss:0.027338077204653196\n",
      "train loss:0.015349574297627793\n",
      "train loss:0.03912186240424534\n",
      "train loss:0.003928015232456855\n",
      "train loss:0.007692002537788654\n",
      "train loss:0.012818901250835013\n",
      "train loss:0.010596439103879574\n",
      "train loss:0.007500715264620833\n",
      "train loss:0.013476466872408631\n",
      "train loss:0.004186607367488914\n",
      "train loss:0.017872651248979934\n",
      "train loss:0.06381960860448982\n",
      "train loss:0.012591786367021256\n",
      "train loss:0.011061378897669918\n",
      "train loss:0.010298939410203229\n",
      "train loss:0.012262957970712918\n",
      "train loss:0.02862926076225549\n",
      "train loss:0.01381947097454662\n",
      "train loss:0.015817784682590697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04589654920925456\n",
      "train loss:0.005351626445861442\n",
      "train loss:0.04314883505576876\n",
      "train loss:0.007417173893922154\n",
      "train loss:0.011001450434607318\n",
      "train loss:0.01700065099525235\n",
      "train loss:0.002330865992767666\n",
      "=== epoch:8, train acc:0.99, test acc:0.984 ===\n",
      "train loss:0.006410538642198539\n",
      "train loss:0.014189527130565573\n",
      "train loss:0.014484141709756579\n",
      "train loss:0.06427521861862596\n",
      "train loss:0.009392066428814273\n",
      "train loss:0.02304703757517582\n",
      "train loss:0.059671203198613965\n",
      "train loss:0.013783980319946387\n",
      "train loss:0.010087242041051046\n",
      "train loss:0.009926996610126413\n",
      "train loss:0.0028790280211254617\n",
      "train loss:0.021288616566782718\n",
      "train loss:0.022812228780474335\n",
      "train loss:0.007823028807647108\n",
      "train loss:0.008012447582670315\n",
      "train loss:0.005530769491925012\n",
      "train loss:0.015339124612647614\n",
      "train loss:0.008314987988301556\n",
      "train loss:0.006933357785409362\n",
      "train loss:0.018305211675025464\n",
      "train loss:0.019227580835431985\n",
      "train loss:0.02376574026986279\n",
      "train loss:0.021204154358474593\n",
      "train loss:0.009671906576971867\n",
      "train loss:0.023041457621252367\n",
      "train loss:0.004144595399799114\n",
      "train loss:0.00629833172196424\n",
      "train loss:0.0011686074828735243\n",
      "train loss:0.001730393450955738\n",
      "train loss:0.017726737930018513\n",
      "train loss:0.01433482977962703\n",
      "train loss:0.004622385443218089\n",
      "train loss:0.015364000461652367\n",
      "train loss:0.003952232246353524\n",
      "train loss:0.017286754546656943\n",
      "train loss:0.006371888938314134\n",
      "train loss:0.02152701337385403\n",
      "train loss:0.005799363211153105\n",
      "train loss:0.0016826916275754582\n",
      "train loss:0.021259505763808716\n",
      "train loss:0.010767070568339466\n",
      "train loss:0.02824277736510515\n",
      "train loss:0.0035389340074791797\n",
      "train loss:0.0025085157913000066\n",
      "train loss:0.005669203017257282\n",
      "train loss:0.00956668589022966\n",
      "train loss:0.008713079481416551\n",
      "train loss:0.0026560486848359365\n",
      "train loss:0.010319571791591778\n",
      "train loss:0.03527487504912316\n",
      "train loss:0.017839255989416027\n",
      "train loss:0.003943244555289843\n",
      "train loss:0.047732387745303806\n",
      "train loss:0.013344618012801495\n",
      "train loss:0.0032503355993283105\n",
      "train loss:0.0025683997472855575\n",
      "train loss:0.011681108627772295\n",
      "train loss:0.04564334872427984\n",
      "train loss:0.014471701327437223\n",
      "train loss:0.0038312525337504087\n",
      "train loss:0.0005540185420949879\n",
      "train loss:0.00421324611122195\n",
      "train loss:0.003978761597173352\n",
      "train loss:0.024429459859132654\n",
      "train loss:0.0021585318181037254\n",
      "train loss:0.05991415633356634\n",
      "train loss:0.014487883890341405\n",
      "train loss:0.00533394375596581\n",
      "train loss:0.004141854441875904\n",
      "train loss:0.0016831665462295048\n",
      "train loss:0.01117428816186748\n",
      "train loss:0.003818000554068105\n",
      "train loss:0.03957168588312396\n",
      "train loss:0.0062877684095355565\n",
      "train loss:0.005835785145608151\n",
      "train loss:0.0035422724048643777\n",
      "train loss:0.01666623029168892\n",
      "train loss:0.00556631757021962\n",
      "train loss:0.011573416477523262\n",
      "train loss:0.005021692152566696\n",
      "train loss:0.003913272526774067\n",
      "train loss:0.06489155008497755\n",
      "train loss:0.01256905884633999\n",
      "train loss:0.04813117748494398\n",
      "train loss:0.008880502251723234\n",
      "train loss:0.007683066267985819\n",
      "train loss:0.013651010967531543\n",
      "train loss:0.0032029990859732903\n",
      "train loss:0.06225383036266237\n",
      "train loss:0.0058768461681791475\n",
      "train loss:0.002508430730523284\n",
      "train loss:0.0052849434519394375\n",
      "train loss:0.028757110510459198\n",
      "train loss:0.018515096449125697\n",
      "train loss:0.0011660384254769413\n",
      "train loss:0.05609323061177062\n",
      "train loss:0.007167694650836601\n",
      "train loss:0.010539853125327356\n",
      "train loss:0.006778681388548908\n",
      "train loss:0.019857105251892347\n",
      "train loss:0.012063882458576071\n",
      "train loss:0.008872137741277429\n",
      "train loss:0.0363371744153162\n",
      "train loss:0.019238824808020055\n",
      "train loss:0.020576901301785952\n",
      "train loss:0.00784380759913829\n",
      "train loss:0.007636493448998861\n",
      "train loss:0.015624035909452565\n",
      "train loss:0.027140177848902717\n",
      "train loss:0.022921817973482278\n",
      "train loss:0.02492071487164753\n",
      "train loss:0.006574319185874597\n",
      "train loss:0.018061678690204686\n",
      "train loss:0.05823204458089684\n",
      "train loss:0.022628511877850217\n",
      "train loss:0.08654763175116537\n",
      "train loss:0.01631890449734362\n",
      "train loss:0.005863767105409519\n",
      "train loss:0.007400093467309897\n",
      "train loss:0.016069658937791383\n",
      "train loss:0.02506538932826952\n",
      "train loss:0.0867419777307623\n",
      "train loss:0.00658337719420482\n",
      "train loss:0.027539246491097978\n",
      "train loss:0.02290256851521555\n",
      "train loss:0.007167914579304317\n",
      "train loss:0.04809465710989017\n",
      "train loss:0.00305495644523565\n",
      "train loss:0.0022027218509900605\n",
      "train loss:0.03986727475099246\n",
      "train loss:0.021787038606359065\n",
      "train loss:0.019833135042823115\n",
      "train loss:0.006179858289623312\n",
      "train loss:0.016334124402000536\n",
      "train loss:0.015630860379553434\n",
      "train loss:0.02716869210057809\n",
      "train loss:0.005820716667405865\n",
      "train loss:0.0028023496149454518\n",
      "train loss:0.029813390770507233\n",
      "train loss:0.005303575133371431\n",
      "train loss:0.013528273324929946\n",
      "train loss:0.00799542906939334\n",
      "train loss:0.007848760464274402\n",
      "train loss:0.00552669454527804\n",
      "train loss:0.011767849218948084\n",
      "train loss:0.0011064167574222168\n",
      "train loss:0.004286052774725831\n",
      "train loss:0.01403958993409616\n",
      "train loss:0.01088743524182785\n",
      "train loss:0.028242964168830386\n",
      "train loss:0.031437832984589895\n",
      "train loss:0.10578041157494329\n",
      "train loss:0.04886495578289411\n",
      "train loss:0.021881981891984207\n",
      "train loss:0.01442940872896148\n",
      "train loss:0.006679227771675516\n",
      "train loss:0.0026490063810822285\n",
      "train loss:0.013420633041498297\n",
      "train loss:0.011137997942965964\n",
      "train loss:0.005005705608944934\n",
      "train loss:0.02377248034673859\n",
      "train loss:0.03446244430162507\n",
      "train loss:0.040798827743581026\n",
      "train loss:0.010008699622553439\n",
      "train loss:0.021915240091327042\n",
      "train loss:0.0171343548702031\n",
      "train loss:0.0030348069847365797\n",
      "train loss:0.016953487210653605\n",
      "train loss:0.010254414718122204\n",
      "train loss:0.026678430868481363\n",
      "train loss:0.010790608598327266\n",
      "train loss:0.02977152231374882\n",
      "train loss:0.011304801515321629\n",
      "train loss:0.013389563079130154\n",
      "train loss:0.010180739838881757\n",
      "train loss:0.004707960416598264\n",
      "train loss:0.03592960968004066\n",
      "train loss:0.010853336705421157\n",
      "train loss:0.030717675548618945\n",
      "train loss:0.012177870388124288\n",
      "train loss:0.031954716293826596\n",
      "train loss:0.01078996309989816\n",
      "train loss:0.004650735313518362\n",
      "train loss:0.0008347009883994383\n",
      "train loss:0.03426099039710946\n",
      "train loss:0.011119995229261492\n",
      "train loss:0.01481400853275815\n",
      "train loss:0.006026506087625197\n",
      "train loss:0.005111987658784514\n",
      "train loss:0.008392878018297098\n",
      "train loss:0.020041904389983874\n",
      "train loss:0.006033427763393582\n",
      "train loss:0.013530224808963114\n",
      "train loss:0.004573001372566287\n",
      "train loss:0.00443810378168032\n",
      "train loss:0.018365820926919726\n",
      "train loss:0.018616057156686214\n",
      "train loss:0.006094681902218299\n",
      "train loss:0.006598023284609016\n",
      "train loss:0.004189731761111514\n",
      "train loss:0.0053900575746067625\n",
      "train loss:0.015151913865013595\n",
      "train loss:0.0075932367336867266\n",
      "train loss:0.0032181560375608713\n",
      "train loss:0.03142936366539278\n",
      "train loss:0.03810762172559181\n",
      "train loss:0.009610932344830198\n",
      "train loss:0.019090082624285222\n",
      "train loss:0.002985113817317628\n",
      "train loss:0.018149532951781604\n",
      "train loss:0.02849424751980984\n",
      "train loss:0.01779330442995252\n",
      "train loss:0.007102997051745262\n",
      "train loss:0.0019681050995390397\n",
      "train loss:0.007262929428465262\n",
      "train loss:0.007193485839650428\n",
      "train loss:0.02522202261281556\n",
      "train loss:0.01237320154559584\n",
      "train loss:0.016860158058709263\n",
      "train loss:0.005879389188624973\n",
      "train loss:0.021836260987466348\n",
      "train loss:0.010129416888530201\n",
      "train loss:0.0022323895427157105\n",
      "train loss:0.01076801560262042\n",
      "train loss:0.01803168223170265\n",
      "train loss:0.032563311271566066\n",
      "train loss:0.004941082766273575\n",
      "train loss:0.014053222431302814\n",
      "train loss:0.013446415684993987\n",
      "train loss:0.007354887157326366\n",
      "train loss:0.009376928382180605\n",
      "train loss:0.02224793014926823\n",
      "train loss:0.013241727450189639\n",
      "train loss:0.006551975776439877\n",
      "train loss:0.069838233506117\n",
      "train loss:0.02494828409424087\n",
      "train loss:0.05129558434987089\n",
      "train loss:0.00675603117094517\n",
      "train loss:0.019914979109063112\n",
      "train loss:0.0023949112124381795\n",
      "train loss:0.048890922547955365\n",
      "train loss:0.004481338547616866\n",
      "train loss:0.011001545723508083\n",
      "train loss:0.005668663317428653\n",
      "train loss:0.011343978729788899\n",
      "train loss:0.029739870022048996\n",
      "train loss:0.009087766236320518\n",
      "train loss:0.003422170160512262\n",
      "train loss:0.006776382660544124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.034233976468464766\n",
      "train loss:0.006206850381157113\n",
      "train loss:0.003031085323156831\n",
      "train loss:0.015127587128872903\n",
      "train loss:0.013371934070496581\n",
      "train loss:0.0058199932665001395\n",
      "train loss:0.00975736630385085\n",
      "train loss:0.0012317862838705528\n",
      "train loss:0.02097199456565652\n",
      "train loss:0.017715769462776376\n",
      "train loss:0.004540569392932598\n",
      "train loss:0.011689816432783735\n",
      "train loss:0.02283383322429955\n",
      "train loss:0.04085890411314419\n",
      "train loss:0.0029995964013868398\n",
      "train loss:0.007930326007150833\n",
      "train loss:0.01027611453864138\n",
      "train loss:0.007173595849827854\n",
      "train loss:0.014208306996074225\n",
      "train loss:0.008101830843389798\n",
      "train loss:0.004938219712351407\n",
      "train loss:0.014603084656231289\n",
      "train loss:0.012733438320021684\n",
      "train loss:0.01938543944169212\n",
      "train loss:0.003081793855894578\n",
      "train loss:0.017761604133046605\n",
      "train loss:0.005830057997115482\n",
      "train loss:0.010390827935008251\n",
      "train loss:0.008069604053479501\n",
      "train loss:0.012733049634840095\n",
      "train loss:0.008383606938711333\n",
      "train loss:0.014134310269862688\n",
      "train loss:0.007732557069760005\n",
      "train loss:0.004228204296934696\n",
      "train loss:0.03390767396076707\n",
      "train loss:0.0038489268216207874\n",
      "train loss:0.014961249416532827\n",
      "train loss:0.038858051480749965\n",
      "train loss:0.00720647749678034\n",
      "train loss:0.005348295076405614\n",
      "train loss:0.0245387039521226\n",
      "train loss:0.005209698787222149\n",
      "train loss:0.035068312972150045\n",
      "train loss:0.019135474634349173\n",
      "train loss:0.02014944014592033\n",
      "train loss:0.007645723837828363\n",
      "train loss:0.0065702622056334\n",
      "train loss:0.01753903810380203\n",
      "train loss:0.04435810394412938\n",
      "train loss:0.015865913731715996\n",
      "train loss:0.008335319199584556\n",
      "train loss:0.0028607589428658138\n",
      "train loss:0.013413906203425956\n",
      "train loss:0.012611988630195525\n",
      "train loss:0.01266561886038593\n",
      "train loss:0.005666611677869618\n",
      "train loss:0.010834107227958936\n",
      "train loss:0.004914753110872858\n",
      "train loss:0.022107776006997225\n",
      "train loss:0.02201211663593627\n",
      "train loss:0.015121704678251427\n",
      "train loss:0.008546745044339154\n",
      "train loss:0.0018799300972237685\n",
      "train loss:0.006930553755033027\n",
      "train loss:0.015220808420920178\n",
      "train loss:0.0062931974526603965\n",
      "train loss:0.010646130122593665\n",
      "train loss:0.09114822284598864\n",
      "train loss:0.07556621788944609\n",
      "train loss:0.013883736361439535\n",
      "train loss:0.004700009546877309\n",
      "train loss:0.0016587357918178341\n",
      "train loss:0.020757131459559864\n",
      "train loss:0.00291453543435089\n",
      "train loss:0.008230445779270753\n",
      "train loss:0.013154453510948474\n",
      "train loss:0.01100386281102416\n",
      "train loss:0.018299683170400496\n",
      "train loss:0.007618463335015538\n",
      "train loss:0.01844112340384538\n",
      "train loss:0.005252285326887452\n",
      "train loss:0.004395591898228996\n",
      "train loss:0.006562292177941409\n",
      "train loss:0.008620886507115913\n",
      "train loss:0.01623977103879329\n",
      "train loss:0.05554824284934357\n",
      "train loss:0.0029564487809874145\n",
      "train loss:0.02452227367046384\n",
      "train loss:0.011468616595683028\n",
      "train loss:0.0017926044763373434\n",
      "train loss:0.013087445554673434\n",
      "train loss:0.04573744603948024\n",
      "train loss:0.007172845575194597\n",
      "train loss:0.005654644480254879\n",
      "train loss:0.0879505001762004\n",
      "train loss:0.006277057836637888\n",
      "train loss:0.006617167571215499\n",
      "train loss:0.00391038870975317\n",
      "train loss:0.0900206743156073\n",
      "train loss:0.0016307478314961771\n",
      "train loss:0.0115492060634589\n",
      "train loss:0.005535381852763941\n",
      "train loss:0.007049664123045244\n",
      "train loss:0.025837007841164685\n",
      "train loss:0.0023695037979392496\n",
      "train loss:0.009168054624159537\n",
      "train loss:0.05385163038420068\n",
      "train loss:0.005597008248785046\n",
      "train loss:0.040188199249238776\n",
      "train loss:0.015369943271119737\n",
      "train loss:0.020703109121092175\n",
      "train loss:0.015470915588474148\n",
      "train loss:0.028381731375188023\n",
      "train loss:0.040253110437423335\n",
      "train loss:0.022006032534581377\n",
      "train loss:0.007935443114395015\n",
      "train loss:0.0052997018264243955\n",
      "train loss:0.024852578945567038\n",
      "train loss:0.01617125925837969\n",
      "train loss:0.026382590791370578\n",
      "train loss:0.006855194272232348\n",
      "train loss:0.018953702752616583\n",
      "train loss:0.006511349182072975\n",
      "train loss:0.016230492260271078\n",
      "train loss:0.01241563112028735\n",
      "train loss:0.0035433875727603746\n",
      "train loss:0.03873393930886011\n",
      "train loss:0.0027597705282851465\n",
      "train loss:0.01966340948856021\n",
      "train loss:0.01146517565792857\n",
      "train loss:0.0032332817340081253\n",
      "train loss:0.0029918952016473287\n",
      "train loss:0.010065254195682178\n",
      "train loss:0.005327499013341409\n",
      "train loss:0.004856430890465207\n",
      "train loss:0.005409208104346089\n",
      "train loss:0.003665194389837264\n",
      "train loss:0.0032577168027416925\n",
      "train loss:0.0078084471028562665\n",
      "train loss:0.006235981451624324\n",
      "train loss:0.010722983642336545\n",
      "train loss:0.012415514650454507\n",
      "train loss:0.019693315539221216\n",
      "train loss:0.0033076012187991306\n",
      "train loss:0.006999221578712981\n",
      "train loss:0.004302409360841785\n",
      "train loss:0.003946656382080654\n",
      "train loss:0.017942577419273176\n",
      "train loss:0.006531861248381967\n",
      "train loss:0.006760068305251908\n",
      "train loss:0.0415998964291814\n",
      "train loss:0.04708158531302567\n",
      "train loss:0.01394152651488772\n",
      "train loss:0.019850597330517566\n",
      "train loss:0.0031960077132421193\n",
      "train loss:0.007155382255695037\n",
      "train loss:0.013820537050919575\n",
      "train loss:0.013085137966232938\n",
      "train loss:0.012924103359562242\n",
      "train loss:0.006395933423254\n",
      "train loss:0.017891719375165257\n",
      "train loss:0.008456731667840799\n",
      "train loss:0.006609494291093965\n",
      "train loss:0.003074743245033053\n",
      "train loss:0.00649657560705968\n",
      "train loss:0.003526573086127195\n",
      "train loss:0.04542240923665019\n",
      "train loss:0.024734192294996946\n",
      "train loss:0.012382332732051875\n",
      "train loss:0.0023145761823995005\n",
      "train loss:0.024117841076910777\n",
      "train loss:0.017621701576138377\n",
      "train loss:0.000448697256899789\n",
      "train loss:0.00800244078377775\n",
      "train loss:0.011011260869036834\n",
      "train loss:0.017785169946325622\n",
      "train loss:0.0027170345166524583\n",
      "train loss:0.01017530737774978\n",
      "train loss:0.0033282796763754474\n",
      "train loss:0.002354100589524945\n",
      "train loss:0.034620525532542315\n",
      "train loss:0.003390118773435585\n",
      "train loss:0.0017660952928661112\n",
      "train loss:0.01666160589767103\n",
      "train loss:0.009070837787796153\n",
      "train loss:0.005345247378740242\n",
      "train loss:0.007860618422290955\n",
      "train loss:0.01910685086904551\n",
      "train loss:0.0017625871649408146\n",
      "train loss:0.0031277669119031144\n",
      "train loss:0.011700461475510486\n",
      "train loss:0.014616971013591846\n",
      "train loss:0.010041701281124065\n",
      "train loss:0.005327190773400901\n",
      "train loss:0.0016649260760055482\n",
      "train loss:0.022070864706481796\n",
      "train loss:0.009024008794505179\n",
      "train loss:0.0023189110005500226\n",
      "train loss:0.005149884280225916\n",
      "train loss:0.008786055734905963\n",
      "train loss:0.008375850886620158\n",
      "train loss:0.002099818813677275\n",
      "train loss:0.008693075310925532\n",
      "train loss:0.01931280578900009\n",
      "train loss:0.027563914987177394\n",
      "train loss:0.0010260342768082375\n",
      "train loss:0.0026743958148412583\n",
      "train loss:0.002611355069099328\n",
      "train loss:0.0854238791553554\n",
      "train loss:0.07591328772454953\n",
      "train loss:0.01091708164179991\n",
      "train loss:0.0010889823037288505\n",
      "train loss:0.011188357440725341\n",
      "train loss:0.0456859496539171\n",
      "train loss:0.0016264102487657931\n",
      "train loss:0.003359915399968875\n",
      "train loss:0.032749183740441674\n",
      "train loss:0.004504659448916917\n",
      "train loss:0.012051688802423348\n",
      "train loss:0.047554638060604795\n",
      "train loss:0.0034815434787482434\n",
      "train loss:0.002891191011062533\n",
      "train loss:0.00890156855501678\n",
      "train loss:0.011682942321237693\n",
      "train loss:0.006928705541166091\n",
      "train loss:0.002212886307469483\n",
      "train loss:0.007256241731378842\n",
      "train loss:0.005836670730123301\n",
      "train loss:0.011194505974116704\n",
      "train loss:0.007359974009625786\n",
      "train loss:0.006396593917724392\n",
      "train loss:0.006497233983376634\n",
      "train loss:0.002676085675040752\n",
      "train loss:0.006232727263273574\n",
      "train loss:0.003868489833749078\n",
      "train loss:0.006637777572667124\n",
      "train loss:0.010043157345952411\n",
      "train loss:0.011124124661065993\n",
      "train loss:0.03161472866597641\n",
      "train loss:0.002041485983124062\n",
      "train loss:0.0014758570452088793\n",
      "train loss:0.011693539943800282\n",
      "train loss:0.02515782285697868\n",
      "train loss:0.003999920602811182\n",
      "train loss:0.01077756081800919\n",
      "train loss:0.005734942459660772\n",
      "train loss:0.04514008616968732\n",
      "train loss:0.013265075687072036\n",
      "train loss:0.018639035136585307\n",
      "train loss:0.010716301253198246\n",
      "train loss:0.01758987842838315\n",
      "train loss:0.0013481188638540989\n",
      "train loss:0.0067242636660862325\n",
      "train loss:0.028042681409769968\n",
      "train loss:0.007553403052930344\n",
      "train loss:0.011411346822475233\n",
      "train loss:0.011631553981494548\n",
      "train loss:0.02684225317378811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.020897506041139602\n",
      "train loss:0.006145217981067561\n",
      "train loss:0.0033994384383226194\n",
      "train loss:0.015180734772053282\n",
      "train loss:0.02859731561241492\n",
      "train loss:0.005832660987022903\n",
      "train loss:0.010406687177318574\n",
      "train loss:0.002694115283231185\n",
      "train loss:0.002803470676083719\n",
      "train loss:0.00400413591142833\n",
      "train loss:0.02185711458666752\n",
      "train loss:0.026042450217044544\n",
      "train loss:0.005611732676054444\n",
      "train loss:0.002821973597799637\n",
      "train loss:0.0015820908845250183\n",
      "train loss:0.007331433997279505\n",
      "train loss:0.03360328310605718\n",
      "train loss:0.00810529089199509\n",
      "train loss:0.043314981520153534\n",
      "train loss:0.023126425546707175\n",
      "train loss:0.024016522915577806\n",
      "train loss:0.0036440910508760698\n",
      "train loss:0.017424843289031326\n",
      "train loss:0.06638686936741997\n",
      "train loss:0.005323341187165455\n",
      "train loss:0.0022566092104520153\n",
      "train loss:0.006019310304239144\n",
      "train loss:0.05394425618605665\n",
      "train loss:0.02887463369149796\n",
      "train loss:0.002393763295777884\n",
      "train loss:0.015594868035528281\n",
      "train loss:0.005942501053717023\n",
      "train loss:0.004447355940347022\n",
      "train loss:0.031048889182589048\n",
      "train loss:0.0015795663778477933\n",
      "train loss:0.013197536074283206\n",
      "train loss:0.0026140285238219164\n",
      "train loss:0.008866036357661805\n",
      "train loss:0.008673064931080578\n",
      "train loss:0.008047278212869947\n",
      "train loss:0.022534448218889377\n",
      "train loss:0.007157389121484354\n",
      "train loss:0.025096710016507283\n",
      "train loss:0.020332634982852045\n",
      "train loss:0.01762345903571895\n",
      "train loss:0.0030617251407037355\n",
      "train loss:0.001462547667415428\n",
      "train loss:0.03440995009757601\n",
      "train loss:0.0030430251219220883\n",
      "train loss:0.036139449206992094\n",
      "train loss:0.018855550464419158\n",
      "train loss:0.006290214311280778\n",
      "train loss:0.006198316196497593\n",
      "train loss:0.007199831371497265\n",
      "train loss:0.009841982441501198\n",
      "train loss:0.008723679299841604\n",
      "train loss:0.011030589823659644\n",
      "train loss:0.014225085427156316\n",
      "train loss:0.053995491371056165\n",
      "train loss:0.010976612495011395\n",
      "train loss:0.021054827348606593\n",
      "train loss:0.016171537537546826\n",
      "train loss:0.0026568101974278587\n",
      "train loss:0.007575011949894823\n",
      "train loss:0.03417904398567165\n",
      "train loss:0.02399806239589108\n",
      "train loss:0.006341773497062419\n",
      "train loss:0.016140242207655916\n",
      "train loss:0.007001926975621035\n",
      "train loss:0.0018636260097940238\n",
      "train loss:0.015815551225008863\n",
      "train loss:0.013844706351849582\n",
      "train loss:0.019164873543144795\n",
      "train loss:0.01809531777438488\n",
      "train loss:0.024674385196755102\n",
      "train loss:0.015070255518636453\n",
      "train loss:0.00596111926326396\n",
      "train loss:0.019677887530765804\n",
      "train loss:0.007180658228402324\n",
      "train loss:0.004720932609970193\n",
      "train loss:0.06698264422871597\n",
      "train loss:0.003953219873733158\n",
      "train loss:0.006100498893108529\n",
      "train loss:0.02692133394714215\n",
      "train loss:0.003934609667774833\n",
      "train loss:0.04554178908213832\n",
      "train loss:0.012144508088802257\n",
      "train loss:0.006805102754526414\n",
      "train loss:0.01770425323140292\n",
      "train loss:0.004583981946522706\n",
      "train loss:0.0022324339548310517\n",
      "train loss:0.004105129910177957\n",
      "train loss:0.011032776792032455\n",
      "=== epoch:9, train acc:0.993, test acc:0.982 ===\n",
      "train loss:0.039177605311889116\n",
      "train loss:0.004779968100174795\n",
      "train loss:0.008022943312323406\n",
      "train loss:0.0096779349646923\n",
      "train loss:0.02072744188263022\n",
      "train loss:0.00964782900979012\n",
      "train loss:0.007672187642791619\n",
      "train loss:0.00464067855207641\n",
      "train loss:0.0035813822420956974\n",
      "train loss:0.012764295291053418\n",
      "train loss:0.007836444408980948\n",
      "train loss:0.009285316277372953\n",
      "train loss:0.07584863177874589\n",
      "train loss:0.005358139589787163\n",
      "train loss:0.004582312520723364\n",
      "train loss:0.00436813005134775\n",
      "train loss:0.011790153649426203\n",
      "train loss:0.005928846607750892\n",
      "train loss:0.008018506604722342\n",
      "train loss:0.0027399861828031735\n",
      "train loss:0.003426660425547871\n",
      "train loss:0.009379269576369113\n",
      "train loss:0.0037017418952971348\n",
      "train loss:0.0028113954399983433\n",
      "train loss:0.008674681540667169\n",
      "train loss:0.00334439117585916\n",
      "train loss:0.013806435734001744\n",
      "train loss:0.0036175278801317207\n",
      "train loss:0.0006248561544582545\n",
      "train loss:0.01030502393460086\n",
      "train loss:0.002946672652880576\n",
      "train loss:0.0030140492717300326\n",
      "train loss:0.016966568783248454\n",
      "train loss:0.0034290558361789025\n",
      "train loss:0.02516854093831625\n",
      "train loss:0.0034752269826452014\n",
      "train loss:0.018961486622220077\n",
      "train loss:0.0015615923217953387\n",
      "train loss:0.007374425596321318\n",
      "train loss:0.01602995590000987\n",
      "train loss:0.013998511370709215\n",
      "train loss:0.0027937552714763737\n",
      "train loss:0.011062551162649432\n",
      "train loss:0.0026919526466191624\n",
      "train loss:0.030015144379364766\n",
      "train loss:0.007513425062813605\n",
      "train loss:0.018848367673546272\n",
      "train loss:0.0037396054342550174\n",
      "train loss:0.005124393305281594\n",
      "train loss:0.005685232192760085\n",
      "train loss:0.006956527655387028\n",
      "train loss:0.0025603300629866684\n",
      "train loss:0.008385293621922979\n",
      "train loss:0.026961202109992755\n",
      "train loss:0.0006099836208244303\n",
      "train loss:0.02909128459017959\n",
      "train loss:0.002529348635262458\n",
      "train loss:0.020512824705496455\n",
      "train loss:0.0027544887286170255\n",
      "train loss:0.0032377655665985026\n",
      "train loss:0.026939780345002617\n",
      "train loss:0.006488314156288119\n",
      "train loss:0.037552426101307254\n",
      "train loss:0.012608352624942238\n",
      "train loss:0.013362331732167124\n",
      "train loss:0.0012821690666217547\n",
      "train loss:0.0017318113505072246\n",
      "train loss:0.0033915332468205696\n",
      "train loss:0.014770802181302627\n",
      "train loss:0.017794403267623604\n",
      "train loss:0.0053543893435822575\n",
      "train loss:0.011641464410489974\n",
      "train loss:0.007578710865014117\n",
      "train loss:0.0012784653877381197\n",
      "train loss:0.0006585382315394226\n",
      "train loss:0.007789202961188022\n",
      "train loss:0.0038999423257301347\n",
      "train loss:0.007222053408673427\n",
      "train loss:0.01190659412679649\n",
      "train loss:0.022023321968253424\n",
      "train loss:0.06676904366729397\n",
      "train loss:0.00809610185436404\n",
      "train loss:0.00811281744663912\n",
      "train loss:0.003273730808163553\n",
      "train loss:0.011485596832373119\n",
      "train loss:0.0026116416852724946\n",
      "train loss:0.007553968237414939\n",
      "train loss:0.006247551809616413\n",
      "train loss:0.0030086180943041285\n",
      "train loss:0.010411714612934594\n",
      "train loss:0.009232727265704899\n",
      "train loss:0.007864762769178624\n",
      "train loss:0.012480647709643184\n",
      "train loss:0.004669359866486716\n",
      "train loss:0.01063826732322472\n",
      "train loss:0.0031791760905596793\n",
      "train loss:0.01416884693534188\n",
      "train loss:0.01311937712843936\n",
      "train loss:0.009275165365708929\n",
      "train loss:0.005953617842492107\n",
      "train loss:0.005816954668477111\n",
      "train loss:0.0071219264034496285\n",
      "train loss:0.017488744685901416\n",
      "train loss:0.011238837771718796\n",
      "train loss:0.003779615645254251\n",
      "train loss:0.02019613122353259\n",
      "train loss:0.011803298426977846\n",
      "train loss:0.009891168532862437\n",
      "train loss:0.009352114639836078\n",
      "train loss:0.0065755328134787065\n",
      "train loss:0.008431122412564399\n",
      "train loss:0.025655027374609795\n",
      "train loss:0.0050547314650425555\n",
      "train loss:0.002426539326720004\n",
      "train loss:0.004105702437257172\n",
      "train loss:0.004808488137330455\n",
      "train loss:0.005505409423346668\n",
      "train loss:0.010596226383782558\n",
      "train loss:0.0038513606649508893\n",
      "train loss:0.012442076299292422\n",
      "train loss:0.0015429101474564034\n",
      "train loss:0.018991704677657328\n",
      "train loss:0.00748062032237067\n",
      "train loss:0.0048330933298926125\n",
      "train loss:0.01607562073937273\n",
      "train loss:0.023522705842462726\n",
      "train loss:0.012540371495860507\n",
      "train loss:0.00019733656840565945\n",
      "train loss:0.012204890007230724\n",
      "train loss:0.002801653394409341\n",
      "train loss:0.004370562881234091\n",
      "train loss:0.013126458169565985\n",
      "train loss:0.01381692132134104\n",
      "train loss:0.003479561447706707\n",
      "train loss:0.0035285139342820676\n",
      "train loss:0.00704902226575947\n",
      "train loss:0.005691234331857673\n",
      "train loss:0.0014778086619082432\n",
      "train loss:0.005144165593396535\n",
      "train loss:0.011429102451483503\n",
      "train loss:0.004014442588518456\n",
      "train loss:0.014405988393843012\n",
      "train loss:0.0008583404925764113\n",
      "train loss:0.008725298518208005\n",
      "train loss:0.00488692220282014\n",
      "train loss:0.002132210442868013\n",
      "train loss:0.007258411972499776\n",
      "train loss:0.02208512295009717\n",
      "train loss:0.007499167153322289\n",
      "train loss:0.01253809002240152\n",
      "train loss:0.00499418399201167\n",
      "train loss:0.008752370646689949\n",
      "train loss:0.0019497464197853766\n",
      "train loss:0.004955264294940353\n",
      "train loss:0.0024917131982208785\n",
      "train loss:0.0038737030953288746\n",
      "train loss:0.0054023197696804704\n",
      "train loss:0.009786020292635826\n",
      "train loss:0.00553140239065075\n",
      "train loss:0.0069001634802631166\n",
      "train loss:0.07174427375867057\n",
      "train loss:0.011779768472724083\n",
      "train loss:0.007171822889865968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.005082967943078051\n",
      "train loss:0.0023403385175092507\n",
      "train loss:0.016572993389046827\n",
      "train loss:0.008399896863932325\n",
      "train loss:0.015130798332406905\n",
      "train loss:0.0240068395984558\n",
      "train loss:0.001450250055042033\n",
      "train loss:0.011021900528935272\n",
      "train loss:0.008613656694325908\n",
      "train loss:0.023223869843802425\n",
      "train loss:0.010605272284118483\n",
      "train loss:0.0042151801206557205\n",
      "train loss:0.00246723599185476\n",
      "train loss:0.10687616567134726\n",
      "train loss:0.0066984153114118435\n",
      "train loss:0.013391640749053928\n",
      "train loss:0.019141719984197725\n",
      "train loss:0.0035210690753499124\n",
      "train loss:0.0078081482514962995\n",
      "train loss:0.002374930887086637\n",
      "train loss:0.0060119351600763996\n",
      "train loss:0.005550499010285883\n",
      "train loss:0.02126844566884965\n",
      "train loss:0.003283347117594032\n",
      "train loss:0.02263286982031126\n",
      "train loss:0.008121134821641958\n",
      "train loss:0.0026051371944390946\n",
      "train loss:0.004956389489246635\n",
      "train loss:0.006136124592659152\n",
      "train loss:0.0062475750519568815\n",
      "train loss:0.007933118213875899\n",
      "train loss:0.02741627817543242\n",
      "train loss:0.05143225191479495\n",
      "train loss:0.0037138868534619964\n",
      "train loss:0.00763664743245914\n",
      "train loss:0.019194961612458733\n",
      "train loss:0.010128646021205697\n",
      "train loss:0.008967907638878763\n",
      "train loss:0.005090755161739107\n",
      "train loss:0.00798285788668481\n",
      "train loss:0.008352237192112017\n",
      "train loss:0.012530178393291267\n",
      "train loss:0.023627214144231426\n",
      "train loss:0.0036373139766467406\n",
      "train loss:0.0035456498721036372\n",
      "train loss:0.010841887308427372\n",
      "train loss:0.03334918548717114\n",
      "train loss:0.0007285343293602447\n",
      "train loss:0.012972066921811375\n",
      "train loss:0.020845979439270707\n",
      "train loss:0.018780027976396604\n",
      "train loss:0.01937967084670138\n",
      "train loss:0.0019884292737942424\n",
      "train loss:0.005010605336365808\n",
      "train loss:0.008896459845505855\n",
      "train loss:0.009723801124779803\n",
      "train loss:0.048528523081892184\n",
      "train loss:0.003081674872065428\n",
      "train loss:0.004021835117593915\n",
      "train loss:0.011928257491766426\n",
      "train loss:0.025920520664625298\n",
      "train loss:0.0038479691461521665\n",
      "train loss:0.00139304590530959\n",
      "train loss:0.018700848547775942\n",
      "train loss:0.0048613984303961634\n",
      "train loss:0.011585933378841675\n",
      "train loss:0.011329797429245227\n",
      "train loss:0.01001125879361215\n",
      "train loss:0.017008645445025054\n",
      "train loss:0.005494670834021724\n",
      "train loss:0.00316268171975248\n",
      "train loss:0.009367889721778213\n",
      "train loss:0.004359750221038602\n",
      "train loss:0.011298667649664093\n",
      "train loss:0.0025459171862301346\n",
      "train loss:0.009791805005067086\n",
      "train loss:0.001502895118421609\n",
      "train loss:0.05162683191003605\n",
      "train loss:0.003575813030150464\n",
      "train loss:0.02676322835719934\n",
      "train loss:0.0073724395464413025\n",
      "train loss:0.009227834782655715\n",
      "train loss:0.013843848269389121\n",
      "train loss:0.0034858583850506946\n",
      "train loss:0.009260431150562735\n",
      "train loss:0.0006876996965198169\n",
      "train loss:0.005154924155540624\n",
      "train loss:0.013772521038879468\n",
      "train loss:0.015924185558635392\n",
      "train loss:0.011998305031005104\n",
      "train loss:0.0010489910163581444\n",
      "train loss:0.003542197142159911\n",
      "train loss:0.001491500656707474\n",
      "train loss:0.022930155260537613\n",
      "train loss:0.004569566110076268\n",
      "train loss:0.0023466497842412563\n",
      "train loss:0.01509337516636296\n",
      "train loss:0.004623112246469405\n",
      "train loss:0.03075118215467674\n",
      "train loss:0.008782920563386391\n",
      "train loss:0.0006018680981116469\n",
      "train loss:0.01573073128549279\n",
      "train loss:0.007761661956126341\n",
      "train loss:0.004787937635610827\n",
      "train loss:0.005749643489914652\n",
      "train loss:0.023255687756352186\n",
      "train loss:0.004020799913655848\n",
      "train loss:0.01141881297057359\n",
      "train loss:0.019284234024086255\n",
      "train loss:0.010379914509422754\n",
      "train loss:0.01063365548631632\n",
      "train loss:0.0143820714882682\n",
      "train loss:0.014429071766798482\n",
      "train loss:0.006130308394261846\n",
      "train loss:0.010411584202649005\n",
      "train loss:0.00444749547091868\n",
      "train loss:0.015746974294137406\n",
      "train loss:0.010358788434147559\n",
      "train loss:0.00814902689635682\n",
      "train loss:0.005347762191414747\n",
      "train loss:0.03176498892888496\n",
      "train loss:0.012793722769227663\n",
      "train loss:0.0031836266833166348\n",
      "train loss:0.005685722266105725\n",
      "train loss:0.0072338662576598165\n",
      "train loss:0.028281664239397722\n",
      "train loss:0.00863473926376214\n",
      "train loss:0.003769744817998492\n",
      "train loss:0.010065781095835289\n",
      "train loss:0.0014831302781770418\n",
      "train loss:0.010207530279833793\n",
      "train loss:0.009866998565026647\n",
      "train loss:0.0043076814978618\n",
      "train loss:0.0015849543282182759\n",
      "train loss:0.011677242474684067\n",
      "train loss:0.005952176034966953\n",
      "train loss:0.01139518812965448\n",
      "train loss:0.008303531508739154\n",
      "train loss:0.006228115375579241\n",
      "train loss:0.007123767644208693\n",
      "train loss:0.018455022663291423\n",
      "train loss:0.0017009995327990088\n",
      "train loss:0.004491131650009209\n",
      "train loss:0.003401353904526876\n",
      "train loss:0.026426128968624858\n",
      "train loss:0.0036627041739193656\n",
      "train loss:0.004429214773580324\n",
      "train loss:0.004420996172382705\n",
      "train loss:0.002606097471454639\n",
      "train loss:0.0028747297358794484\n",
      "train loss:0.00973280539373458\n",
      "train loss:0.006339204028224576\n",
      "train loss:0.0033320190708602732\n",
      "train loss:0.004167216330532669\n",
      "train loss:0.007247153962168318\n",
      "train loss:0.0030996669869402453\n",
      "train loss:0.012443521356475131\n",
      "train loss:0.0031469068506594818\n",
      "train loss:0.012913060574363306\n",
      "train loss:0.007074413993747491\n",
      "train loss:0.004515061825911738\n",
      "train loss:0.002093235722244722\n",
      "train loss:0.041732991894356274\n",
      "train loss:0.009608193307902605\n",
      "train loss:0.0012385070754384851\n",
      "train loss:0.005600572473876604\n",
      "train loss:0.0012341471651240838\n",
      "train loss:0.016418131066704266\n",
      "train loss:0.005631951579207205\n",
      "train loss:0.0020670784716070255\n",
      "train loss:0.01858528186110293\n",
      "train loss:0.00512782591460614\n",
      "train loss:0.005018077234542447\n",
      "train loss:0.00713931957396141\n",
      "train loss:0.027260687112192164\n",
      "train loss:0.0041496218250670295\n",
      "train loss:0.011510324643032028\n",
      "train loss:0.002833871731683787\n",
      "train loss:0.0008100217569619278\n",
      "train loss:0.0066790065573861974\n",
      "train loss:0.0028840410103582676\n",
      "train loss:0.00703995830025994\n",
      "train loss:0.005334450614711938\n",
      "train loss:0.012135075098541403\n",
      "train loss:0.01754687867469119\n",
      "train loss:0.0037520693120167486\n",
      "train loss:0.00843056415867351\n",
      "train loss:0.003810625777930428\n",
      "train loss:0.005413531890032999\n",
      "train loss:0.002539638019367161\n",
      "train loss:0.00980451885493069\n",
      "train loss:0.004851138175328872\n",
      "train loss:0.007085096869146116\n",
      "train loss:0.0007657774204440465\n",
      "train loss:0.006419316497236688\n",
      "train loss:0.009462409958164945\n",
      "train loss:0.03427498545109288\n",
      "train loss:0.010665601351827047\n",
      "train loss:0.0034913623171462676\n",
      "train loss:0.007614176616866792\n",
      "train loss:0.007664601560068856\n",
      "train loss:0.035515272803262235\n",
      "train loss:0.0038076311642360615\n",
      "train loss:0.0034750993744159485\n",
      "train loss:0.00342061486285822\n",
      "train loss:0.0018502304974681641\n",
      "train loss:0.006109974351537003\n",
      "train loss:0.004215553274973275\n",
      "train loss:0.051700742023177225\n",
      "train loss:0.007159244232371782\n",
      "train loss:0.003349257067262627\n",
      "train loss:0.0019776264804498437\n",
      "train loss:0.015424467515894354\n",
      "train loss:0.004435128389870049\n",
      "train loss:0.002816671156834144\n",
      "train loss:0.01677737916443863\n",
      "train loss:0.0031456745146830776\n",
      "train loss:0.004810453411571454\n",
      "train loss:0.003558259467659358\n",
      "train loss:0.03293525248481547\n",
      "train loss:0.0014198190013397349\n",
      "train loss:0.021325165517509497\n",
      "train loss:0.0020540363165758844\n",
      "train loss:0.002692402773722565\n",
      "train loss:0.04225396410014346\n",
      "train loss:0.0071577921111821916\n",
      "train loss:0.0016081837444285624\n",
      "train loss:0.03206842412833737\n",
      "train loss:0.006919939939407389\n",
      "train loss:0.006244024547675909\n",
      "train loss:0.011409379873002015\n",
      "train loss:0.02597792705333408\n",
      "train loss:0.012393563784712283\n",
      "train loss:0.002183967086380768\n",
      "train loss:0.0347989207837475\n",
      "train loss:0.0008455636474052758\n",
      "train loss:0.0009749649699353774\n",
      "train loss:0.015354943587524619\n",
      "train loss:0.009041104322266381\n",
      "train loss:0.005218415135788761\n",
      "train loss:0.00537973995334226\n",
      "train loss:0.013026337906265282\n",
      "train loss:0.006348799647812102\n",
      "train loss:0.0033981757665931204\n",
      "train loss:0.004670266978938106\n",
      "train loss:0.01175780800393562\n",
      "train loss:0.056167453945069085\n",
      "train loss:0.01870075918044431\n",
      "train loss:0.0032852172073790853\n",
      "train loss:0.01037440007225507\n",
      "train loss:0.011325871291936756\n",
      "train loss:0.004166258141826567\n",
      "train loss:0.0036339500419613753\n",
      "train loss:0.003171679003382011\n",
      "train loss:0.0016299678295658804\n",
      "train loss:0.008231523405968168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.012984373477196332\n",
      "train loss:0.001432441170593491\n",
      "train loss:0.0228788105676467\n",
      "train loss:0.005671215140018514\n",
      "train loss:0.003523615871481175\n",
      "train loss:0.011131623928211098\n",
      "train loss:0.10761927659026142\n",
      "train loss:0.010089244135965713\n",
      "train loss:0.0067090921683239865\n",
      "train loss:0.004001680278684051\n",
      "train loss:0.010564364387922203\n",
      "train loss:0.011255292749452446\n",
      "train loss:0.00547785234411597\n",
      "train loss:0.015599712274427318\n",
      "train loss:0.009430286630109285\n",
      "train loss:0.00635353324347169\n",
      "train loss:0.0031926424710149184\n",
      "train loss:0.010171597176217979\n",
      "train loss:0.0029826414265058587\n",
      "train loss:0.009593999524040104\n",
      "train loss:0.011349717286598766\n",
      "train loss:0.012805853936297762\n",
      "train loss:0.011278815070510383\n",
      "train loss:0.009401690829113216\n",
      "train loss:0.0033871198874047387\n",
      "train loss:0.00438277788610493\n",
      "train loss:0.011900461977027276\n",
      "train loss:0.001047102218545203\n",
      "train loss:0.0038917118936325486\n",
      "train loss:0.00494646817889427\n",
      "train loss:0.01865609542145529\n",
      "train loss:0.003422092345543119\n",
      "train loss:0.027056677580906352\n",
      "train loss:0.17318961494375318\n",
      "train loss:0.018672750399871988\n",
      "train loss:0.008496691766172659\n",
      "train loss:0.0029278434352355298\n",
      "train loss:0.01176017128798272\n",
      "train loss:0.008553963814334405\n",
      "train loss:0.0009627524179420885\n",
      "train loss:0.017387592691173483\n",
      "train loss:0.0033019727518531984\n",
      "train loss:0.014614514556227245\n",
      "train loss:0.01087082626651352\n",
      "train loss:0.005268140322217169\n",
      "train loss:0.007203255291858688\n",
      "train loss:0.01804504411083004\n",
      "train loss:0.007205154496954014\n",
      "train loss:0.0042206924962655355\n",
      "train loss:0.0009675819763725915\n",
      "train loss:0.006091766242074109\n",
      "train loss:0.0028985782520526983\n",
      "train loss:0.006486234675034139\n",
      "train loss:0.021686661605459845\n",
      "train loss:0.00561714225642672\n",
      "train loss:0.004064578437525204\n",
      "train loss:0.01073669484004095\n",
      "train loss:0.005976466151403845\n",
      "train loss:0.023748005312204144\n",
      "train loss:0.01050232787021545\n",
      "train loss:0.0032469383362183947\n",
      "train loss:0.006202630243969732\n",
      "train loss:0.006452324848797871\n",
      "train loss:0.004408047437941238\n",
      "train loss:0.011046150506879477\n",
      "train loss:0.015261614287222827\n",
      "train loss:0.001331178263707317\n",
      "train loss:0.005617759756223115\n",
      "train loss:0.0003954152120227625\n",
      "train loss:0.0019234459970676268\n",
      "train loss:0.0432800059240439\n",
      "train loss:0.004961167684427891\n",
      "train loss:0.0028982887410592103\n",
      "train loss:0.0009524219073942722\n",
      "train loss:0.01448926475474751\n",
      "train loss:0.022429851775147322\n",
      "train loss:0.0019367847347329379\n",
      "train loss:0.0006335087791726686\n",
      "train loss:0.002014459764356812\n",
      "train loss:0.004645056579178449\n",
      "train loss:0.02593143256025537\n",
      "train loss:0.005125227069056143\n",
      "train loss:0.004913540495215638\n",
      "train loss:0.003735407147686157\n",
      "train loss:0.013313135957773965\n",
      "train loss:0.017778685917439828\n",
      "train loss:0.0027712731442771556\n",
      "train loss:0.04965113133360011\n",
      "train loss:0.008717464515853514\n",
      "train loss:0.005427811548940643\n",
      "train loss:0.0038231396659329482\n",
      "train loss:0.0027393677897423435\n",
      "train loss:0.001268527390750135\n",
      "train loss:0.01814771110761827\n",
      "train loss:0.007790222459748236\n",
      "train loss:0.021759366732853738\n",
      "train loss:0.02235949541460455\n",
      "train loss:0.003787531953424412\n",
      "train loss:0.014318058711341085\n",
      "train loss:0.004106954521176922\n",
      "train loss:0.0063908930358058235\n",
      "train loss:0.003123468154554211\n",
      "train loss:0.00829091376309839\n",
      "train loss:0.015453577598242922\n",
      "train loss:0.007828260408800754\n",
      "train loss:0.0036296962424633766\n",
      "train loss:0.007627023139990699\n",
      "train loss:0.003067508124990118\n",
      "train loss:0.0010009538688359502\n",
      "train loss:0.012455199207077237\n",
      "train loss:0.002954061924926318\n",
      "train loss:0.013326838407887665\n",
      "train loss:0.00949142336263552\n",
      "train loss:0.006179880886533945\n",
      "train loss:0.012453421094619686\n",
      "train loss:0.007120156505173574\n",
      "train loss:0.0023877370196530912\n",
      "train loss:0.004989575146657055\n",
      "train loss:0.0010666502692870258\n",
      "train loss:0.0020064113069744077\n",
      "train loss:0.006818698495703892\n",
      "train loss:0.00907523909527422\n",
      "train loss:0.006047682028498865\n",
      "train loss:0.005868586583059454\n",
      "train loss:0.0012436419535716322\n",
      "train loss:0.036406476916723836\n",
      "train loss:0.007074884868333222\n",
      "train loss:0.01842967856907535\n",
      "train loss:0.024229680075700668\n",
      "train loss:0.00940564828738029\n",
      "train loss:0.007108574660642333\n",
      "train loss:0.008035756329789864\n",
      "train loss:0.004660284987828864\n",
      "train loss:0.018606684830999778\n",
      "train loss:0.006668169743363206\n",
      "train loss:0.015424821292784148\n",
      "train loss:0.005059953998612758\n",
      "train loss:0.0048052082134593865\n",
      "train loss:0.00636296234777266\n",
      "train loss:0.0023193714373510085\n",
      "train loss:0.055216011848999595\n",
      "train loss:0.0020711433726623494\n",
      "train loss:0.01879226818733803\n",
      "train loss:0.004632279541138914\n",
      "train loss:0.0026303582834260124\n",
      "train loss:0.00493263208651517\n",
      "train loss:0.002642613930944789\n",
      "train loss:0.020920214526573413\n",
      "train loss:0.013056043928392403\n",
      "train loss:0.0013720222206370322\n",
      "train loss:0.009497146667506675\n",
      "train loss:0.005500057392314994\n",
      "train loss:0.002860424688231171\n",
      "train loss:0.01597747893993968\n",
      "train loss:0.003996833001688146\n",
      "train loss:0.004305470986383192\n",
      "train loss:0.0053185867623484415\n",
      "train loss:0.020019612945680533\n",
      "train loss:0.005678496623292534\n",
      "train loss:0.0038073390582168847\n",
      "train loss:0.006075220368365025\n",
      "train loss:0.0009296658673770566\n",
      "train loss:0.00461121901701047\n",
      "train loss:0.007883697698131987\n",
      "train loss:0.004771431811481043\n",
      "train loss:0.006071057282930332\n",
      "train loss:0.0022858884908151522\n",
      "train loss:0.002547759042405205\n",
      "train loss:0.00985268601580991\n",
      "train loss:0.005842842695606485\n",
      "train loss:0.006412911066522097\n",
      "train loss:0.005462763186973527\n",
      "train loss:0.0012853366194362024\n",
      "train loss:0.03582802038800753\n",
      "train loss:0.0030926044009897797\n",
      "train loss:0.0018734249335751684\n",
      "train loss:0.0025254867764442687\n",
      "train loss:0.02657610197046954\n",
      "train loss:0.011581712400053998\n",
      "train loss:0.005900423157184237\n",
      "train loss:0.0024060980097777547\n",
      "=== epoch:10, train acc:0.992, test acc:0.986 ===\n",
      "train loss:0.004280959459570998\n",
      "train loss:0.0025813663247612633\n",
      "train loss:0.02263582770040492\n",
      "train loss:0.004039218562468257\n",
      "train loss:0.004185504837010494\n",
      "train loss:0.10883642346749661\n",
      "train loss:0.0015573329934187518\n",
      "train loss:0.0031999942013754223\n",
      "train loss:0.009475050984440226\n",
      "train loss:0.009346692475405679\n",
      "train loss:0.004513211272447416\n",
      "train loss:0.004823330829307585\n",
      "train loss:0.0014947148266982694\n",
      "train loss:0.006554023268253052\n",
      "train loss:0.021724926178022554\n",
      "train loss:0.010600948163422736\n",
      "train loss:0.010679572989779083\n",
      "train loss:0.0021686500027377174\n",
      "train loss:0.013009466820842594\n",
      "train loss:0.006328057904785308\n",
      "train loss:0.004885781286608791\n",
      "train loss:0.005844142584730415\n",
      "train loss:0.005129691084994586\n",
      "train loss:0.016486829048055054\n",
      "train loss:0.003699163624900989\n",
      "train loss:0.007790789634969864\n",
      "train loss:0.0028397359755105538\n",
      "train loss:0.003634090878846282\n",
      "train loss:0.006857277481294045\n",
      "train loss:0.003005333731373693\n",
      "train loss:0.01850566136104092\n",
      "train loss:0.0011601760763882277\n",
      "train loss:0.006262992764610621\n",
      "train loss:0.008416376849163994\n",
      "train loss:0.0027716084348049772\n",
      "train loss:0.007426304760413997\n",
      "train loss:0.007356985045954617\n",
      "train loss:0.007384965432697509\n",
      "train loss:0.003719077900000012\n",
      "train loss:0.006246029877105072\n",
      "train loss:0.004981143373659667\n",
      "train loss:0.0030495606049689584\n",
      "train loss:0.004237008970240364\n",
      "train loss:0.0065470246537983525\n",
      "train loss:0.0026115460350893548\n",
      "train loss:0.009483547683572582\n",
      "train loss:0.001912659068733275\n",
      "train loss:0.019869040069292482\n",
      "train loss:0.0042347500753951335\n",
      "train loss:0.01921802211427377\n",
      "train loss:0.002142833071528086\n",
      "train loss:0.024376033911349914\n",
      "train loss:0.002703379785227706\n",
      "train loss:0.0018316781884717074\n",
      "train loss:0.006653724018958789\n",
      "train loss:0.004808499183727164\n",
      "train loss:0.004954592917581823\n",
      "train loss:0.0015490484560778239\n",
      "train loss:0.005208849941596092\n",
      "train loss:0.017880743377108085\n",
      "train loss:0.01698405861215309\n",
      "train loss:0.0019570505163646478\n",
      "train loss:0.008403066469218769\n",
      "train loss:0.0009844316728927767\n",
      "train loss:0.008911341214633483\n",
      "train loss:0.00171774681419378\n",
      "train loss:0.01907064370827391\n",
      "train loss:0.0028264993650473635\n",
      "train loss:0.004004456044828005\n",
      "train loss:0.002623380428592856\n",
      "train loss:0.005982525513317393\n",
      "train loss:0.006284171987100538\n",
      "train loss:0.0009869417271531344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.030318262814271545\n",
      "train loss:0.018762296555352647\n",
      "train loss:0.012335705358200964\n",
      "train loss:0.0014683615434172071\n",
      "train loss:0.007758060823798489\n",
      "train loss:0.011629762514526585\n",
      "train loss:0.0061411919149052694\n",
      "train loss:0.013874651178471144\n",
      "train loss:0.0025362339766772284\n",
      "train loss:0.014706276896816124\n",
      "train loss:0.001740543175199348\n",
      "train loss:0.0008657180404147664\n",
      "train loss:0.009433862911431683\n",
      "train loss:0.027211177478963722\n",
      "train loss:0.002671392161010615\n",
      "train loss:0.007577401105618386\n",
      "train loss:0.019520607971980967\n",
      "train loss:0.0012352920742741797\n",
      "train loss:0.0003796555303960555\n",
      "train loss:0.06538206095293037\n",
      "train loss:0.03879284955079866\n",
      "train loss:0.004003683804608728\n",
      "train loss:0.006251152973638383\n",
      "train loss:0.027263518004902378\n",
      "train loss:0.0021534596924793438\n",
      "train loss:0.0035905093662366213\n",
      "train loss:0.007928007574998381\n",
      "train loss:0.006793562980385882\n",
      "train loss:0.0015563772562421669\n",
      "train loss:0.0006207286068940771\n",
      "train loss:0.0010716739640488212\n",
      "train loss:0.002129923149259365\n",
      "train loss:0.0021932107512456487\n",
      "train loss:0.013112613699824803\n",
      "train loss:0.001248213178776075\n",
      "train loss:0.020921843547591083\n",
      "train loss:0.010838004307981857\n",
      "train loss:0.001855486443342922\n",
      "train loss:0.038251563086411644\n",
      "train loss:0.0007862212106209611\n",
      "train loss:0.01997681770884192\n",
      "train loss:0.012465367465859845\n",
      "train loss:0.006419706664320291\n",
      "train loss:0.001174016514873406\n",
      "train loss:0.002383405240810171\n",
      "train loss:0.0044396814835878946\n",
      "train loss:0.001820388059234225\n",
      "train loss:0.003391562206750788\n",
      "train loss:0.002240965515287794\n",
      "train loss:0.022157317826925083\n",
      "train loss:0.008819467889449093\n",
      "train loss:0.0029095030830406095\n",
      "train loss:0.01638715986854149\n",
      "train loss:0.004412121368132996\n",
      "train loss:0.0040241857139284585\n",
      "train loss:0.008925523368066726\n",
      "train loss:0.008450443231101697\n",
      "train loss:0.0010990765545209238\n",
      "train loss:0.0021178807408842497\n",
      "train loss:0.011744484577411644\n",
      "train loss:0.0014376671628095648\n",
      "train loss:0.001245443801089327\n",
      "train loss:0.0004593154704610514\n",
      "train loss:0.004140356856164601\n",
      "train loss:0.004140798032837096\n",
      "train loss:0.007724775243533413\n",
      "train loss:0.002555572029939606\n",
      "train loss:0.013097280148958732\n",
      "train loss:0.056969395388501515\n",
      "train loss:0.0034260521715693357\n",
      "train loss:0.006512010455689571\n",
      "train loss:0.029957772920372796\n",
      "train loss:0.002147344450022533\n",
      "train loss:0.004401855451125188\n",
      "train loss:0.011927550420735093\n",
      "train loss:0.003198634277401877\n",
      "train loss:0.00700060288969495\n",
      "train loss:0.004790500778941012\n",
      "train loss:0.0067267841778540696\n",
      "train loss:0.003185552262054355\n",
      "train loss:0.004572559821415538\n",
      "train loss:0.0031760102596592125\n",
      "train loss:0.0026315407945233533\n",
      "train loss:0.010675916462052753\n",
      "train loss:0.0039439524150401304\n",
      "train loss:0.006843239233784314\n",
      "train loss:0.05018895255140975\n",
      "train loss:0.0021448359610545813\n",
      "train loss:0.008156637991274826\n",
      "train loss:0.014986453729939333\n",
      "train loss:0.011935431032444134\n",
      "train loss:0.015427679939556937\n",
      "train loss:0.007472877297732534\n",
      "train loss:0.00506414834820183\n",
      "train loss:0.0038019410275242247\n",
      "train loss:0.0014894166600767874\n",
      "train loss:0.015167875777648608\n",
      "train loss:0.005394275919797238\n",
      "train loss:0.0016703335864131634\n",
      "train loss:0.0052767282326601096\n",
      "train loss:0.03086600994420539\n",
      "train loss:0.028522611351164393\n",
      "train loss:0.017844390131219492\n",
      "train loss:0.013707503285084995\n",
      "train loss:0.006757366647086924\n",
      "train loss:0.0006684343970898539\n",
      "train loss:0.001435515054057615\n",
      "train loss:0.0009987639355900698\n",
      "train loss:0.003936019919987045\n",
      "train loss:0.008030271534891385\n",
      "train loss:0.0032166743167802887\n",
      "train loss:0.011536046118908983\n",
      "train loss:0.002960831944152703\n",
      "train loss:0.0042175476864623395\n",
      "train loss:0.002331582596580153\n",
      "train loss:0.009893346321922356\n",
      "train loss:0.060490775031055405\n",
      "train loss:0.0036495493423674163\n",
      "train loss:0.0010152534795569044\n",
      "train loss:0.0013109122391961774\n",
      "train loss:0.012463525630793003\n",
      "train loss:0.012966741046381309\n",
      "train loss:0.02795058953344969\n",
      "train loss:0.005609077932151643\n",
      "train loss:0.003002840724852502\n",
      "train loss:0.004301239311390109\n",
      "train loss:0.021183214940663764\n",
      "train loss:0.01696668305272421\n",
      "train loss:0.000582358497504339\n",
      "train loss:0.004283156096663349\n",
      "train loss:0.0034032917522556034\n",
      "train loss:0.0031617711971340967\n",
      "train loss:0.0051893588110484504\n",
      "train loss:0.005216996190084765\n",
      "train loss:0.0048008228855770715\n",
      "train loss:0.01361662906625523\n",
      "train loss:0.019452844869418572\n",
      "train loss:0.003365730314499299\n",
      "train loss:0.001274426285216004\n",
      "train loss:0.024417820442193223\n",
      "train loss:0.010758491997800483\n",
      "train loss:0.01902226806144507\n",
      "train loss:0.0017991227372942643\n",
      "train loss:0.0011884363189300696\n",
      "train loss:0.007149277996630332\n",
      "train loss:0.01834090149851318\n",
      "train loss:0.0014355739513399737\n",
      "train loss:0.009061935830049064\n",
      "train loss:0.013532752775660038\n",
      "train loss:0.0007987282056988365\n",
      "train loss:0.0063013836030934725\n",
      "train loss:0.002584373240935528\n",
      "train loss:0.004567850994477825\n",
      "train loss:0.00639271416660508\n",
      "train loss:0.010148287710071324\n",
      "train loss:0.018812819292216374\n",
      "train loss:0.007570790760487462\n",
      "train loss:0.016508822854989183\n",
      "train loss:0.0013396176572022147\n",
      "train loss:0.0010087719692724445\n",
      "train loss:0.009160593437839681\n",
      "train loss:0.006403075945653382\n",
      "train loss:0.00924094223509208\n",
      "train loss:0.003975363605569621\n",
      "train loss:0.04193454842705437\n",
      "train loss:0.007525398377670619\n",
      "train loss:0.006011517001997419\n",
      "train loss:0.04317876473765021\n",
      "train loss:0.0017779733333811885\n",
      "train loss:0.008212364145919167\n",
      "train loss:0.0009949604785177114\n",
      "train loss:0.001016067227901896\n",
      "train loss:0.05964775060586004\n",
      "train loss:0.0033175999677315694\n",
      "train loss:0.009682553733749131\n",
      "train loss:0.0016765901172854647\n",
      "train loss:0.00992407865333963\n",
      "train loss:0.03745548124969133\n",
      "train loss:0.0025103932110475957\n",
      "train loss:0.02413474027864543\n",
      "train loss:0.0004956283046402963\n",
      "train loss:0.016520821844436765\n",
      "train loss:0.005922179018088075\n",
      "train loss:0.0034103364885386942\n",
      "train loss:0.009097895063293844\n",
      "train loss:0.05103272417698973\n",
      "train loss:0.000549252923317388\n",
      "train loss:0.005178172745751383\n",
      "train loss:0.005725545470705916\n",
      "train loss:0.0012609829732952416\n",
      "train loss:0.010521573670394945\n",
      "train loss:0.006864087301131536\n",
      "train loss:0.009128331923590438\n",
      "train loss:0.007518539506148034\n",
      "train loss:0.005690328223204014\n",
      "train loss:0.08714288747454603\n",
      "train loss:0.008207823645932649\n",
      "train loss:0.013053767532634266\n",
      "train loss:0.005513831196536709\n",
      "train loss:0.007418331838031132\n",
      "train loss:0.00645936122755347\n",
      "train loss:0.00841986996655127\n",
      "train loss:0.004486049977832054\n",
      "train loss:0.0008906372228335577\n",
      "train loss:0.0016074272324182635\n",
      "train loss:0.0017472911142598965\n",
      "train loss:0.02124756836556247\n",
      "train loss:0.016115077983496456\n",
      "train loss:0.0028876258347837786\n",
      "train loss:0.007795483880385816\n",
      "train loss:0.01967482277181215\n",
      "train loss:0.006863333318521592\n",
      "train loss:0.017186783244944572\n",
      "train loss:0.00407541383479925\n",
      "train loss:0.004220871988255545\n",
      "train loss:0.004036192834272376\n",
      "train loss:0.0016474511015725185\n",
      "train loss:0.005750324138269768\n",
      "train loss:0.0023694111928372554\n",
      "train loss:0.0027384155146332385\n",
      "train loss:0.008330140255070386\n",
      "train loss:0.005877581338210228\n",
      "train loss:0.04049128261859876\n",
      "train loss:0.0025977376760231933\n",
      "train loss:0.004038861185704968\n",
      "train loss:0.0014319740623033662\n",
      "train loss:0.0021068075153297186\n",
      "train loss:0.0016769772923747317\n",
      "train loss:0.002937922398117884\n",
      "train loss:0.0073764691881781684\n",
      "train loss:0.0038640743962764785\n",
      "train loss:0.002259676007124821\n",
      "train loss:0.03097217234590453\n",
      "train loss:0.0032405174542058744\n",
      "train loss:0.0027485604638581348\n",
      "train loss:0.002608746749290869\n",
      "train loss:0.011942016207189082\n",
      "train loss:0.005672721390944963\n",
      "train loss:0.003828246092657177\n",
      "train loss:0.0004289404420697796\n",
      "train loss:0.00021993839708378275\n",
      "train loss:0.0021525884363481993\n",
      "train loss:0.001425586795399726\n",
      "train loss:0.0066058561467163875\n",
      "train loss:0.001039011255430946\n",
      "train loss:0.005902750325598174\n",
      "train loss:0.0028537836762153068\n",
      "train loss:0.0022038758304262483\n",
      "train loss:0.0037519221971882132\n",
      "train loss:0.004850674742039057\n",
      "train loss:0.0015308280387553999\n",
      "train loss:0.0005667433647170559\n",
      "train loss:0.008844298333520413\n",
      "train loss:0.023732910280742314\n",
      "train loss:0.006569168726531404\n",
      "train loss:0.028824141667978104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03335699779571116\n",
      "train loss:0.0042512629635299065\n",
      "train loss:0.0035157552581578\n",
      "train loss:0.0012559236199901416\n",
      "train loss:0.0015570966395977179\n",
      "train loss:0.028942563424580004\n",
      "train loss:0.0062833880079698565\n",
      "train loss:0.026676760469542593\n",
      "train loss:0.01640497766870517\n",
      "train loss:0.0067402508649166935\n",
      "train loss:0.0077294538490917004\n",
      "train loss:0.0023877587553218543\n",
      "train loss:0.0008986810148305765\n",
      "train loss:0.0026432929451219262\n",
      "train loss:0.023818657586511246\n",
      "train loss:0.004813758576970987\n",
      "train loss:0.001296529549737738\n",
      "train loss:0.013292737837430063\n",
      "train loss:0.002316159009605723\n",
      "train loss:0.01383682444142494\n",
      "train loss:0.006062252999084849\n",
      "train loss:0.004163831798939607\n",
      "train loss:0.0034364862824923926\n",
      "train loss:0.01516048509189149\n",
      "train loss:0.02808528836809966\n",
      "train loss:0.009434189416535\n",
      "train loss:0.005121671076321794\n",
      "train loss:0.020229375851334223\n",
      "train loss:0.008024053268482781\n",
      "train loss:0.0016557831642621517\n",
      "train loss:0.005722909930205824\n",
      "train loss:0.010859630908262717\n",
      "train loss:0.007523893939259477\n",
      "train loss:0.01740493318085672\n",
      "train loss:0.001309731836562991\n",
      "train loss:0.002266773626975668\n",
      "train loss:0.011782104052588636\n",
      "train loss:0.012875878823401637\n",
      "train loss:0.0018581957693439386\n",
      "train loss:0.009207615645051924\n",
      "train loss:0.01131711689495803\n",
      "train loss:0.0026973413354625547\n",
      "train loss:0.020492652256457973\n",
      "train loss:0.020867346579965605\n",
      "train loss:0.011143483454557403\n",
      "train loss:0.0050350677630360266\n",
      "train loss:0.0012169118543621834\n",
      "train loss:0.007602359099094248\n",
      "train loss:0.007229224542699084\n",
      "train loss:0.0044069279786560925\n",
      "train loss:0.007300116633052579\n",
      "train loss:0.011813517687112507\n",
      "train loss:0.012582942520455808\n",
      "train loss:0.01042135667519016\n",
      "train loss:0.002463613003167128\n",
      "train loss:0.004600463923553018\n",
      "train loss:0.001964189458817202\n",
      "train loss:0.009005625109743745\n",
      "train loss:0.0023005030699953717\n",
      "train loss:0.0015742201817593134\n",
      "train loss:0.046807465174345975\n",
      "train loss:0.0037496835943783284\n",
      "train loss:0.003129218894242511\n",
      "train loss:0.002397141485965276\n",
      "train loss:0.0030028620599723567\n",
      "train loss:0.003484605426270881\n",
      "train loss:0.0016503873683791758\n",
      "train loss:0.001824211643665141\n",
      "train loss:0.025838029647140978\n",
      "train loss:0.002867108352827518\n",
      "train loss:0.0034797079673725075\n",
      "train loss:0.0009558203323184743\n",
      "train loss:0.003302303671082337\n",
      "train loss:0.04627040964119687\n",
      "train loss:0.013840305008972327\n",
      "train loss:0.0012813572934432963\n",
      "train loss:0.015916885539683245\n",
      "train loss:0.0029781652106347263\n",
      "train loss:0.0013534004223977594\n",
      "train loss:0.002419289699146137\n",
      "train loss:0.002173592327340644\n",
      "train loss:0.0055344088614120625\n",
      "train loss:0.002012429860356772\n",
      "train loss:0.0046082103550373335\n",
      "train loss:0.0029904569193232532\n",
      "train loss:0.0049725023901161765\n",
      "train loss:0.010079527420824496\n",
      "train loss:0.0032425483969918197\n",
      "train loss:0.005966114781572136\n",
      "train loss:0.005031041267784889\n",
      "train loss:0.003621998972981258\n",
      "train loss:0.0004312485623256337\n",
      "train loss:0.0019888587061123978\n",
      "train loss:0.01390405326481698\n",
      "train loss:0.004372717343849754\n",
      "train loss:0.0031011624181570894\n",
      "train loss:0.004460338982068229\n",
      "train loss:0.0014605499786635135\n",
      "train loss:0.00662997977609529\n",
      "train loss:0.0017261479093815422\n",
      "train loss:0.004725577544260484\n",
      "train loss:0.003941703002761917\n",
      "train loss:0.007882659930552937\n",
      "train loss:0.004388547159314598\n",
      "train loss:0.00027353524511589366\n",
      "train loss:0.004069162182506464\n",
      "train loss:0.0033066210058796024\n",
      "train loss:0.0029164268992549662\n",
      "train loss:0.003808204690043587\n",
      "train loss:0.0022942190088607005\n",
      "train loss:0.0029880946540992957\n",
      "train loss:0.01690405734510608\n",
      "train loss:0.012602503582306251\n",
      "train loss:0.016945934534903447\n",
      "train loss:0.002751201245857024\n",
      "train loss:0.0011691988003806936\n",
      "train loss:0.001263989661315901\n",
      "train loss:0.003649287118674313\n",
      "train loss:0.01633035881381095\n",
      "train loss:0.005612889880949814\n",
      "train loss:0.008927207631545185\n",
      "train loss:0.002421870480703061\n",
      "train loss:0.0008622178844671744\n",
      "train loss:0.0026064335361285352\n",
      "train loss:0.0010551603136957272\n",
      "train loss:0.003476908642061488\n",
      "train loss:0.0016018610590948935\n",
      "train loss:0.0014180910863180152\n",
      "train loss:0.0070531809511626044\n",
      "train loss:0.003253350766165576\n",
      "train loss:0.0007651295847344815\n",
      "train loss:0.002767887004593076\n",
      "train loss:0.005320284499225583\n",
      "train loss:0.0005680030670731343\n",
      "train loss:0.0004637460507571653\n",
      "train loss:0.0012681387750862485\n",
      "train loss:0.008392907096626451\n",
      "train loss:0.0035812019739172294\n",
      "train loss:0.003858753467214087\n",
      "train loss:0.008618895595465971\n",
      "train loss:0.0014161806092671484\n",
      "train loss:0.005740298888875249\n",
      "train loss:0.008310285960439178\n",
      "train loss:0.0024591786192370443\n",
      "train loss:0.006699066615073409\n",
      "train loss:0.0135351662408163\n",
      "train loss:0.011386936532119916\n",
      "train loss:0.008287078533854533\n",
      "train loss:0.0023800543996452426\n",
      "train loss:0.004103487467411401\n",
      "train loss:0.005675793460052977\n",
      "train loss:0.02327594525060621\n",
      "train loss:0.012999969761473513\n",
      "train loss:0.0050893003349441295\n",
      "train loss:0.003089255407387182\n",
      "train loss:0.01375589688424585\n",
      "train loss:0.003987673115800118\n",
      "train loss:0.0009580578751250328\n",
      "train loss:0.003188143175563754\n",
      "train loss:0.002303490689387914\n",
      "train loss:0.0026410384097971386\n",
      "train loss:0.0016994665885406502\n",
      "train loss:0.05557855802402388\n",
      "train loss:0.003043922147830032\n",
      "train loss:0.0024703029966802073\n",
      "train loss:0.0021410366866591486\n",
      "train loss:0.01480936233779643\n",
      "train loss:0.000614090161793131\n",
      "train loss:0.0024236129466704403\n",
      "train loss:0.0011556506189043872\n",
      "train loss:0.057243161082174296\n",
      "train loss:0.0053473823002809975\n",
      "train loss:0.0023749301305721277\n",
      "train loss:0.002957671502504456\n",
      "train loss:0.0019707944050718195\n",
      "train loss:0.003875076200370145\n",
      "train loss:0.004547114074173058\n",
      "train loss:0.007816725405240353\n",
      "train loss:0.0037713343034750037\n",
      "train loss:0.009495619105655382\n",
      "train loss:0.006398381050625781\n",
      "train loss:0.014917323935896055\n",
      "train loss:0.012007569726725396\n",
      "train loss:0.005926044384960744\n",
      "train loss:0.005687247728997897\n",
      "train loss:0.0019161687595490578\n",
      "train loss:0.007655809621205746\n",
      "train loss:0.0028013619848099346\n",
      "train loss:0.01140748857577831\n",
      "train loss:0.0029964119034191277\n",
      "train loss:0.0014715070986135797\n",
      "train loss:0.013745028141414695\n",
      "train loss:0.005302399586898095\n",
      "train loss:0.005960131450616389\n",
      "train loss:0.002110197996417484\n",
      "train loss:0.002200368295480356\n",
      "train loss:0.0020298473095973543\n",
      "train loss:0.0038857236957809582\n",
      "train loss:0.002894735696549776\n",
      "train loss:0.004142391405562786\n",
      "train loss:0.001846437603855349\n",
      "train loss:0.00384882884186754\n",
      "train loss:0.001087243296154088\n",
      "train loss:0.006674041512602976\n",
      "train loss:0.0012097239476736687\n",
      "train loss:0.007217242810508145\n",
      "train loss:0.009004503509489037\n",
      "train loss:0.00741295524365954\n",
      "train loss:0.0009140156146336515\n",
      "train loss:0.015881862597213543\n",
      "train loss:0.049582737489482144\n",
      "train loss:0.002383088670422608\n",
      "train loss:0.00492265094370922\n",
      "train loss:0.0038984271618862083\n",
      "train loss:0.0147388765016922\n",
      "train loss:0.038429805241102205\n",
      "train loss:0.004519415244267526\n",
      "train loss:0.012002826962292735\n",
      "train loss:0.006282578554643381\n",
      "train loss:0.003163059677955778\n",
      "train loss:0.012971353925220787\n",
      "train loss:0.008035770832152308\n",
      "train loss:0.0008586102742268061\n",
      "train loss:0.0020373331103196142\n",
      "train loss:0.0041521583290542825\n",
      "train loss:0.0010170622125469147\n",
      "train loss:0.03382888478234528\n",
      "train loss:0.037426892637448875\n",
      "train loss:0.018943394511447192\n",
      "train loss:0.0028430309471688398\n",
      "train loss:0.0025388183289760135\n",
      "train loss:0.03272962589974754\n",
      "train loss:0.005163541858298397\n",
      "train loss:0.009537676582078715\n",
      "train loss:0.005809068046346303\n",
      "train loss:0.0012869837235835603\n",
      "train loss:0.013916959731667032\n",
      "train loss:0.008816542051530839\n",
      "train loss:0.0023990173105245794\n",
      "train loss:0.006520835039135134\n",
      "train loss:0.009855657721369437\n",
      "train loss:0.00205569492019689\n",
      "train loss:0.000732975978067934\n",
      "train loss:0.0014146253802900211\n",
      "train loss:0.01308092070709203\n",
      "train loss:0.018453542706736683\n",
      "train loss:0.005403768654356881\n",
      "train loss:0.005195343877017637\n",
      "train loss:0.00860714798200599\n",
      "train loss:0.0037677530011986328\n",
      "train loss:0.0014707055196735324\n",
      "train loss:0.001862037482255407\n",
      "train loss:0.004576584158851513\n",
      "train loss:0.0037580924251433105\n",
      "train loss:0.005000229567272921\n",
      "train loss:0.001078672372094068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.001770646704359814\n",
      "train loss:0.005827364072636215\n",
      "train loss:0.004381000003892173\n",
      "train loss:0.002122141139582406\n",
      "train loss:0.003981977908992455\n",
      "train loss:0.03200751917387036\n",
      "train loss:0.005513026231407222\n",
      "train loss:0.037343104986710694\n",
      "train loss:0.004398633684429145\n",
      "train loss:0.014602315395799714\n",
      "train loss:0.005525090965501986\n",
      "train loss:0.046069036236286796\n",
      "train loss:0.016215879566919764\n",
      "train loss:0.0019459437620771387\n",
      "train loss:0.003986848587562041\n",
      "=== epoch:11, train acc:0.994, test acc:0.985 ===\n",
      "train loss:0.004691380806448983\n",
      "train loss:0.00026460150684025596\n",
      "train loss:0.0019822996541287376\n",
      "train loss:0.007196204094092043\n",
      "train loss:0.0033078187417332683\n",
      "train loss:0.007730035136547362\n",
      "train loss:0.002681805919520388\n",
      "train loss:0.0014947985973942951\n",
      "train loss:0.004229764881288406\n",
      "train loss:0.0084335311653383\n",
      "train loss:0.0071739440430417314\n",
      "train loss:0.015526324891191356\n",
      "train loss:0.0007902327031451267\n",
      "train loss:0.003024626647217925\n",
      "train loss:0.003444880274742732\n",
      "train loss:0.006958631165842161\n",
      "train loss:0.007161944839450626\n",
      "train loss:0.10978578254975503\n",
      "train loss:0.08723465787783986\n",
      "train loss:0.0017030365480886884\n",
      "train loss:0.0003643064734318585\n",
      "train loss:0.007237660256208955\n",
      "train loss:0.006167368781186371\n",
      "train loss:0.004228500520208269\n",
      "train loss:0.0026595604708348304\n",
      "train loss:0.006549379349688686\n",
      "train loss:0.0018980230771099603\n",
      "train loss:0.0022255141233754245\n",
      "train loss:0.006660948810070225\n",
      "train loss:0.0027154053760161394\n",
      "train loss:0.0018167920197352122\n",
      "train loss:0.010723590157681422\n",
      "train loss:0.0020046852965039997\n",
      "train loss:0.008663122372404639\n",
      "train loss:0.0008784544912550569\n",
      "train loss:0.008785031165558538\n",
      "train loss:0.0015438051567209345\n",
      "train loss:0.007596565819528198\n",
      "train loss:0.006851061844824915\n",
      "train loss:0.010835282537216653\n",
      "train loss:0.0021566412597073077\n",
      "train loss:0.025164810273178215\n",
      "train loss:0.01510722545129241\n",
      "train loss:0.01811119846614929\n",
      "train loss:0.0033665770521782105\n",
      "train loss:0.0013261892728526198\n",
      "train loss:0.0002500126157960312\n",
      "train loss:0.003435987487232624\n",
      "train loss:0.010409933303463006\n",
      "train loss:0.006550424811711256\n",
      "train loss:0.0015690692611406978\n",
      "train loss:0.008074476008577159\n",
      "train loss:0.016741629422643404\n",
      "train loss:0.01777830022680265\n",
      "train loss:0.002078818710792227\n",
      "train loss:0.0013225577684503641\n",
      "train loss:0.0010250672135106627\n",
      "train loss:0.0035172262257532148\n",
      "train loss:0.0012134597754140577\n",
      "train loss:0.0006737608043633288\n",
      "train loss:0.0052972819779219015\n",
      "train loss:0.0009986459438723052\n",
      "train loss:0.002615592246031101\n",
      "train loss:0.01221225777768029\n",
      "train loss:0.03190566903628935\n",
      "train loss:0.007168842978996207\n",
      "train loss:0.003569529984283413\n",
      "train loss:0.0007813037131198818\n",
      "train loss:0.010392271445653016\n",
      "train loss:0.0035917764661265485\n",
      "train loss:0.0015062948361113271\n",
      "train loss:0.005695779238667414\n",
      "train loss:0.0014776303986332841\n",
      "train loss:0.013123725923223879\n",
      "train loss:0.0026787945351746693\n",
      "train loss:0.0006074182743301457\n",
      "train loss:0.0049917567556770825\n",
      "train loss:0.0018937628646144855\n",
      "train loss:0.012790716738384625\n",
      "train loss:0.0022015311438650965\n",
      "train loss:0.0043769267435075625\n",
      "train loss:0.0020149432496515394\n",
      "train loss:0.0024138617398534352\n",
      "train loss:0.0006857123737033396\n",
      "train loss:0.0012355826020195421\n",
      "train loss:0.0027840143237128114\n",
      "train loss:0.0035420969841939848\n",
      "train loss:0.0058053397692491415\n",
      "train loss:0.004650512908218595\n",
      "train loss:0.0035688882355708845\n",
      "train loss:0.0004570038790589882\n",
      "train loss:0.0036042314715398985\n",
      "train loss:0.0033924415375943266\n",
      "train loss:0.003070534637449078\n",
      "train loss:0.0011681025388582066\n",
      "train loss:0.0014092973371611919\n",
      "train loss:0.0011746489155071108\n",
      "train loss:0.0038235363170876784\n",
      "train loss:0.006797795037180827\n",
      "train loss:0.0020363280741912603\n",
      "train loss:0.001700269789829043\n",
      "train loss:0.0015818103693162183\n",
      "train loss:0.022506274214840315\n",
      "train loss:0.016882626955272286\n",
      "train loss:0.0015187920047945837\n",
      "train loss:0.0009234697246798128\n",
      "train loss:0.0015272413132066261\n",
      "train loss:0.0075077473128903624\n",
      "train loss:0.0006287506030290015\n",
      "train loss:0.002874734798927307\n",
      "train loss:0.01834762936110771\n",
      "train loss:0.002118666582239845\n",
      "train loss:0.012307573296295534\n",
      "train loss:0.0029532465020742993\n",
      "train loss:0.03473985227137784\n",
      "train loss:0.0033694552228715783\n",
      "train loss:0.02671694956507085\n",
      "train loss:0.0006723430242696104\n",
      "train loss:0.00844604045814499\n",
      "train loss:0.0047002382698800235\n",
      "train loss:0.0011217411387119713\n",
      "train loss:0.0034488686705386623\n",
      "train loss:0.0031591955564701695\n",
      "train loss:0.009231284514835939\n",
      "train loss:0.005235475119146701\n",
      "train loss:0.05609167973896764\n",
      "train loss:0.005291812053058861\n",
      "train loss:0.010436278305747062\n",
      "train loss:0.004398871251285512\n",
      "train loss:0.013721025605540402\n",
      "train loss:0.001532216906419228\n",
      "train loss:0.002439107032702169\n",
      "train loss:0.00807074013706678\n",
      "train loss:0.008576650317444777\n",
      "train loss:0.00997830219306539\n",
      "train loss:0.0011021012913616363\n",
      "train loss:0.003144479750459277\n",
      "train loss:0.04578249347197174\n",
      "train loss:0.008150720366334803\n",
      "train loss:0.0035310017032039237\n",
      "train loss:0.0035044306775237647\n",
      "train loss:0.001115193527857017\n",
      "train loss:0.019065053064885006\n",
      "train loss:0.0008449721842978675\n",
      "train loss:0.051564829501439806\n",
      "train loss:0.00696391697381317\n",
      "train loss:0.007100376380313327\n",
      "train loss:0.0019209355672317136\n",
      "train loss:0.008974752030201575\n",
      "train loss:0.013484683573328686\n",
      "train loss:0.00652168978977547\n",
      "train loss:0.00422622993768387\n",
      "train loss:0.022606425820275607\n",
      "train loss:0.004077677921194786\n",
      "train loss:0.005107823161391653\n",
      "train loss:0.006844357513542438\n",
      "train loss:0.005573711591992424\n",
      "train loss:0.0012212483083945183\n",
      "train loss:0.001658055411376764\n",
      "train loss:0.009197155698668592\n",
      "train loss:0.0029537280264377145\n",
      "train loss:0.006742562882589885\n",
      "train loss:0.002074335574420347\n",
      "train loss:0.004054049917111806\n",
      "train loss:0.006418853159206856\n",
      "train loss:0.0009256501345315016\n",
      "train loss:0.007247176755639636\n",
      "train loss:0.006178239585377775\n",
      "train loss:0.00954825809831074\n",
      "train loss:0.00234380186650107\n",
      "train loss:0.0054250394876437244\n",
      "train loss:0.016013164494233966\n",
      "train loss:0.0031845750703474434\n",
      "train loss:0.006806946568457429\n",
      "train loss:0.001888819455338456\n",
      "train loss:0.0032348647906117215\n",
      "train loss:0.006442847380954318\n",
      "train loss:0.0028291685079094354\n",
      "train loss:0.002040628067084369\n",
      "train loss:0.0018756139582247018\n",
      "train loss:0.0003472546599232678\n",
      "train loss:0.003100172144424487\n",
      "train loss:0.00678886841284482\n",
      "train loss:0.0019611846594061854\n",
      "train loss:0.00336778453117359\n",
      "train loss:0.0012664783774606692\n",
      "train loss:0.000905488548019988\n",
      "train loss:0.004230883086446497\n",
      "train loss:0.02496270153320239\n",
      "train loss:0.004269853511524611\n",
      "train loss:0.00882747778805596\n",
      "train loss:0.008727218436313603\n",
      "train loss:0.0008094180862775309\n",
      "train loss:0.017586608972306194\n",
      "train loss:0.00023736565649530805\n",
      "train loss:0.014197846631494655\n",
      "train loss:0.04174680720764916\n",
      "train loss:0.005727188344286979\n",
      "train loss:0.004357793678007685\n",
      "train loss:0.00235299864308701\n",
      "train loss:0.006913429188078853\n",
      "train loss:0.0021722401373964403\n",
      "train loss:0.04063394446747029\n",
      "train loss:0.0036472447623842373\n",
      "train loss:0.004904607826538058\n",
      "train loss:0.002219788759447367\n",
      "train loss:0.0018554746430940758\n",
      "train loss:0.004217869991913938\n",
      "train loss:0.011505953545329035\n",
      "train loss:0.002251396769779844\n",
      "train loss:0.008030196511401159\n",
      "train loss:0.0054847600236637185\n",
      "train loss:0.007867611595299839\n",
      "train loss:0.0012156786527415461\n",
      "train loss:0.0006532217876152021\n",
      "train loss:0.0016732577053407533\n",
      "train loss:0.0005862665509785457\n",
      "train loss:0.008086009444291867\n",
      "train loss:0.0027525315042719704\n",
      "train loss:0.03141410199285813\n",
      "train loss:0.003415021169106961\n",
      "train loss:0.001771284893180244\n",
      "train loss:0.0006677674101521622\n",
      "train loss:0.0021527350626588566\n",
      "train loss:0.0022971659918473736\n",
      "train loss:0.005080283869954172\n",
      "train loss:0.018982534701073816\n",
      "train loss:0.003901568090839296\n",
      "train loss:0.012333149498757854\n",
      "train loss:0.0017612302585963203\n",
      "train loss:0.0003480156793554413\n",
      "train loss:0.0006064938274168826\n",
      "train loss:0.0024372867160354513\n",
      "train loss:0.0029449129853099444\n",
      "train loss:0.009883770895309158\n",
      "train loss:0.0030867419726349006\n",
      "train loss:0.005860179406086359\n",
      "train loss:0.0066184199873117544\n",
      "train loss:0.004905367101933849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.17884046070052753\n",
      "train loss:0.0004927899028995574\n",
      "train loss:0.007773730661620831\n",
      "train loss:0.0013390301693171713\n",
      "train loss:0.00586664333841714\n",
      "train loss:0.00724022716214753\n",
      "train loss:0.045048343095996826\n",
      "train loss:0.00906412442092566\n",
      "train loss:0.002581073297495329\n",
      "train loss:0.002838193340432047\n",
      "train loss:0.007418631929651543\n",
      "train loss:0.00767852980007636\n",
      "train loss:0.004426070221502747\n",
      "train loss:0.012361478768151204\n",
      "train loss:0.029525989041634126\n",
      "train loss:0.0042097781271855745\n",
      "train loss:0.0017141812006681676\n",
      "train loss:0.012876231743957847\n",
      "train loss:0.006513419547558848\n",
      "train loss:0.014793770410324592\n",
      "train loss:0.0030079477393482813\n",
      "train loss:0.004817622669025118\n",
      "train loss:0.00037446219352573287\n",
      "train loss:0.0016194509328239237\n",
      "train loss:0.014557843182971113\n",
      "train loss:0.004206473020975074\n",
      "train loss:0.0012891843306268404\n",
      "train loss:0.006922547358763971\n",
      "train loss:0.026126820070775714\n",
      "train loss:0.0030513457601101437\n",
      "train loss:0.00649702577050586\n",
      "train loss:0.08819138437508414\n",
      "train loss:0.015367448942205089\n",
      "train loss:0.015857136185106285\n",
      "train loss:0.004161511274275564\n",
      "train loss:0.003040499074990477\n",
      "train loss:0.008091196386519287\n",
      "train loss:0.01250844011443057\n",
      "train loss:0.004342298248854569\n",
      "train loss:0.022837558245920264\n",
      "train loss:0.009709486829471887\n",
      "train loss:0.05181645338900818\n",
      "train loss:0.002745108660531284\n",
      "train loss:0.0032095540004976565\n",
      "train loss:0.0029646687463977628\n",
      "train loss:0.007933444431149888\n",
      "train loss:0.004102066919102754\n",
      "train loss:0.016630172262526267\n",
      "train loss:0.012983566193101264\n",
      "train loss:0.004917862774973047\n",
      "train loss:0.0029958442436137838\n",
      "train loss:0.02292344577047412\n",
      "train loss:0.004876227939997404\n",
      "train loss:0.004753362959023354\n",
      "train loss:0.003164208792290766\n",
      "train loss:0.004282257703299711\n",
      "train loss:0.01080173904746122\n",
      "train loss:0.038795475333764005\n",
      "train loss:0.0037620957725563783\n",
      "train loss:0.001984474640446753\n",
      "train loss:0.020900698422790204\n",
      "train loss:0.014413305798497067\n",
      "train loss:0.01307055584004521\n",
      "train loss:0.021399971530366223\n",
      "train loss:0.002291891594416049\n",
      "train loss:0.0027029626110844757\n",
      "train loss:0.0010891755749040419\n",
      "train loss:0.0009629095884042371\n",
      "train loss:0.013915080169943271\n",
      "train loss:0.005966540739428339\n",
      "train loss:0.002398704148638624\n",
      "train loss:0.0033112433449947777\n",
      "train loss:0.004564141768918472\n",
      "train loss:0.0019001724245501378\n",
      "train loss:0.008976452258302231\n",
      "train loss:0.004934511330673389\n",
      "train loss:0.0323968306971634\n",
      "train loss:0.0024715850455530246\n",
      "train loss:0.020806349302163282\n",
      "train loss:0.0005246066159611053\n",
      "train loss:0.007212533526321001\n",
      "train loss:0.008939984767523931\n",
      "train loss:0.011915981968518648\n",
      "train loss:0.010068458318265304\n",
      "train loss:0.0053932765147252405\n",
      "train loss:0.002257906261014997\n",
      "train loss:0.003448225018171265\n",
      "train loss:0.001720590682468553\n",
      "train loss:0.021845704823941013\n",
      "train loss:0.024517729977648218\n",
      "train loss:0.0023635283277082934\n",
      "train loss:0.0028701487776293076\n",
      "train loss:0.004754571322790843\n",
      "train loss:0.0016904489376690132\n",
      "train loss:0.006457719972616561\n",
      "train loss:0.028795446611186378\n",
      "train loss:0.009756832531101994\n",
      "train loss:0.004414755502268427\n",
      "train loss:0.002702902502701931\n",
      "train loss:0.005153640522639236\n",
      "train loss:0.004886341807097186\n",
      "train loss:0.003277144652729739\n",
      "train loss:0.004477000016016582\n",
      "train loss:0.001605013001549986\n",
      "train loss:0.004135834055569416\n",
      "train loss:0.007271639460472027\n",
      "train loss:0.007382641091523375\n",
      "train loss:0.002719215217835088\n",
      "train loss:0.0028078100506021718\n",
      "train loss:0.008516430052062117\n",
      "train loss:0.0026916983223825696\n",
      "train loss:0.0006781136984056118\n",
      "train loss:0.008261943810733923\n",
      "train loss:0.06579108324027735\n",
      "train loss:0.007269003935737609\n",
      "train loss:0.00345914152637195\n",
      "train loss:0.002623699323705956\n",
      "train loss:0.0013276713154381841\n",
      "train loss:0.04223835260891717\n",
      "train loss:0.003071718081892228\n",
      "train loss:0.0017827772532941744\n",
      "train loss:0.008469914559055316\n",
      "train loss:0.0017794340291449348\n",
      "train loss:0.006432627192193476\n",
      "train loss:0.0004059317915378495\n",
      "train loss:0.0009819190701285847\n",
      "train loss:0.005836384605773345\n",
      "train loss:0.0007602072274002553\n",
      "train loss:0.007814938479229053\n",
      "train loss:0.0026962340784355936\n",
      "train loss:0.0002824977973160796\n",
      "train loss:0.0018112920945765375\n",
      "train loss:0.06865909408827721\n",
      "train loss:0.0027245129443016192\n",
      "train loss:0.0004615157927257842\n",
      "train loss:0.011395817955861398\n",
      "train loss:0.014779325085359564\n",
      "train loss:0.011368639537134873\n",
      "train loss:0.002802181875585616\n",
      "train loss:0.003055387885261886\n",
      "train loss:0.04579914173328405\n",
      "train loss:0.005054715459156133\n",
      "train loss:0.009675454417244706\n",
      "train loss:0.000876053695400658\n",
      "train loss:0.0036566738967865947\n",
      "train loss:0.00521025056951761\n",
      "train loss:0.010076871696142187\n",
      "train loss:0.0008745447791831113\n",
      "train loss:0.0033240285830107084\n",
      "train loss:0.04734301475354667\n",
      "train loss:0.0009059327721702734\n",
      "train loss:0.010624609220579726\n",
      "train loss:0.0006318694108489893\n",
      "train loss:0.0012506221130462083\n",
      "train loss:0.0075870160664426025\n",
      "train loss:0.005740833011314918\n",
      "train loss:0.00045180710395622463\n",
      "train loss:0.0029628823650304816\n",
      "train loss:0.0015473304842636528\n",
      "train loss:0.005483216363027301\n",
      "train loss:0.0018951019023425283\n",
      "train loss:0.004548601266226319\n",
      "train loss:0.002104888428848567\n",
      "train loss:0.009109873573062257\n",
      "train loss:0.0057574472103677134\n",
      "train loss:0.00311350068408434\n",
      "train loss:0.030894779251917578\n",
      "train loss:0.005640328291612302\n",
      "train loss:0.0034185802348475646\n",
      "train loss:0.003593690231908586\n",
      "train loss:0.0037083291866155687\n",
      "train loss:0.0055268806032735315\n",
      "train loss:0.0038687655566029063\n",
      "train loss:0.001601876128058178\n",
      "train loss:0.0015049548768177798\n",
      "train loss:0.0018866733067846053\n",
      "train loss:0.028424397211693268\n",
      "train loss:0.006359042478250294\n",
      "train loss:0.006028122438249191\n",
      "train loss:0.004353878052405143\n",
      "train loss:0.0032470961869758737\n",
      "train loss:0.004057081322714553\n",
      "train loss:0.003284182700320415\n",
      "train loss:0.005236184156863255\n",
      "train loss:0.002727698215392268\n",
      "train loss:0.0025202767913080886\n",
      "train loss:0.01967451668915786\n",
      "train loss:0.0009333256255402712\n",
      "train loss:0.00032540030116401245\n",
      "train loss:0.002125018570778035\n",
      "train loss:0.002568998875399302\n",
      "train loss:0.0008113940003082783\n",
      "train loss:0.006963059168542416\n",
      "train loss:0.003983678741897225\n",
      "train loss:0.0023843213719985257\n",
      "train loss:0.0026567333772322503\n",
      "train loss:0.011275081697823888\n",
      "train loss:0.001033277685648021\n",
      "train loss:0.008846046734229434\n",
      "train loss:0.008278858855902077\n",
      "train loss:0.000792682406190521\n",
      "train loss:0.010166477297550228\n",
      "train loss:0.02452725465379463\n",
      "train loss:0.012071553611023338\n",
      "train loss:0.00879428360000933\n",
      "train loss:0.006351178710822611\n",
      "train loss:0.0004964672161803758\n",
      "train loss:0.0025872278350414285\n",
      "train loss:0.0037402201164055808\n",
      "train loss:0.003731883955663752\n",
      "train loss:0.008387309443625531\n",
      "train loss:0.010635755039884419\n",
      "train loss:0.0017962448774888984\n",
      "train loss:0.0022880224675207563\n",
      "train loss:0.0024427770193079686\n",
      "train loss:0.0030521804967437033\n",
      "train loss:0.005590103497462855\n",
      "train loss:0.008533129052861646\n",
      "train loss:0.00641759750268193\n",
      "train loss:0.002157501100613485\n",
      "train loss:0.004898288235979874\n",
      "train loss:0.03763196263016691\n",
      "train loss:0.010147136318932602\n",
      "train loss:0.004661960917621361\n",
      "train loss:0.004539071898114835\n",
      "train loss:0.0042984286849619205\n",
      "train loss:0.00407689433940028\n",
      "train loss:0.0010183128734761113\n",
      "train loss:0.0013138600449106657\n",
      "train loss:0.011616843670819832\n",
      "train loss:0.018825388926117174\n",
      "train loss:0.007429587281141156\n",
      "train loss:0.0025614397598490487\n",
      "train loss:0.018509401686271423\n",
      "train loss:0.008145538687520458\n",
      "train loss:0.009770723218178321\n",
      "train loss:0.0010645571911389012\n",
      "train loss:0.0147228080469375\n",
      "train loss:0.007478278484680253\n",
      "train loss:0.001520172799901319\n",
      "train loss:0.012395067215528306\n",
      "train loss:0.0029396817251930323\n",
      "train loss:0.005443396717729512\n",
      "train loss:0.0009548611902710952\n",
      "train loss:0.00019408912205325774\n",
      "train loss:0.002359590576063231\n",
      "train loss:0.0019258849410362822\n",
      "train loss:0.006771013281210059\n",
      "train loss:0.003175429765335322\n",
      "train loss:0.006371826930402196\n",
      "train loss:0.0037874306606117446\n",
      "train loss:0.004911487775163936\n",
      "train loss:0.008958811161646079\n",
      "train loss:0.006578717594350877\n",
      "train loss:0.0056636711437545555\n",
      "train loss:0.00370895044172415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.007564852341269768\n",
      "train loss:0.007807214143334645\n",
      "train loss:0.0034099145070292753\n",
      "train loss:0.004635554264137358\n",
      "train loss:0.004159949090712885\n",
      "train loss:0.0030896622627305482\n",
      "train loss:0.02626115616266184\n",
      "train loss:0.005689987295869995\n",
      "train loss:0.02307473830263322\n",
      "train loss:0.003123473512546126\n",
      "train loss:0.010279008011685003\n",
      "train loss:0.006653997162319035\n",
      "train loss:0.0109471080560713\n",
      "train loss:0.0006935840910294412\n",
      "train loss:0.0023538335183538213\n",
      "train loss:0.0057689031145796744\n",
      "train loss:0.002802870568715658\n",
      "train loss:0.0016242493227587775\n",
      "train loss:0.014254149482776519\n",
      "train loss:0.0067993866786815035\n",
      "train loss:0.0033651946515269727\n",
      "train loss:0.00566830593191176\n",
      "train loss:0.03796083314511259\n",
      "train loss:0.010237504337806519\n",
      "train loss:0.003996161678676916\n",
      "train loss:0.0026461544599912846\n",
      "train loss:0.0027905392924886425\n",
      "train loss:0.02272908730517793\n",
      "train loss:0.014114181232434773\n",
      "train loss:0.0017351989447658934\n",
      "train loss:0.0019121164406491556\n",
      "train loss:0.0006800412116890417\n",
      "train loss:0.001827415897050214\n",
      "train loss:0.001599109918548547\n",
      "train loss:0.001789259170442678\n",
      "train loss:0.010773227916921278\n",
      "train loss:0.0005656070737917909\n",
      "train loss:0.005547302689070039\n",
      "train loss:0.012847800284875002\n",
      "train loss:0.011156667972893197\n",
      "train loss:0.001878258165931631\n",
      "train loss:0.007469960219770492\n",
      "train loss:0.002279003294558529\n",
      "train loss:0.021419263343415613\n",
      "train loss:0.005603526237367749\n",
      "train loss:0.005694149191971583\n",
      "train loss:0.0016876415877331494\n",
      "train loss:0.01560065511029018\n",
      "train loss:0.000611408019516589\n",
      "train loss:0.0015242215222345036\n",
      "train loss:0.0030062393821711974\n",
      "train loss:0.0025944286176568803\n",
      "train loss:0.0016443564297470988\n",
      "train loss:0.0008022823392252929\n",
      "train loss:0.0375690418231498\n",
      "train loss:0.0005308715368845507\n",
      "train loss:0.0006856926340372529\n",
      "train loss:0.0027347756899318908\n",
      "train loss:0.019974576990883872\n",
      "train loss:0.009178520514150077\n",
      "train loss:0.00581032549415408\n",
      "train loss:0.0024618614959105857\n",
      "train loss:0.0009039689840647973\n",
      "train loss:0.0019024277348952013\n",
      "train loss:0.0006625521534873359\n",
      "train loss:0.045578570402448504\n",
      "train loss:0.0030759471979816323\n",
      "train loss:0.026076705848334433\n",
      "train loss:0.0037251464055052634\n",
      "train loss:0.0013678774095348847\n",
      "train loss:0.0035200529881375315\n",
      "train loss:0.007432964236776711\n",
      "train loss:0.0009944233244640237\n",
      "train loss:0.0016758386376099499\n",
      "train loss:0.004678781712062565\n",
      "train loss:0.006276816681679716\n",
      "train loss:0.00861801783968132\n",
      "train loss:0.004268015136494424\n",
      "train loss:0.0037972070059982604\n",
      "train loss:0.005742415599434321\n",
      "train loss:0.0025033396349029404\n",
      "train loss:0.0035067610950055223\n",
      "train loss:0.01049679739777475\n",
      "train loss:0.0005743575508627148\n",
      "train loss:0.00330383010681933\n",
      "train loss:0.01350939865496079\n",
      "train loss:0.008345108166066463\n",
      "train loss:0.01121777872067127\n",
      "train loss:0.0020514918941205037\n",
      "train loss:0.004261569348577361\n",
      "train loss:0.0019002268139802622\n",
      "train loss:0.006952404305815784\n",
      "train loss:0.006524372059294423\n",
      "train loss:0.001007543952290058\n",
      "train loss:0.002264892868404805\n",
      "train loss:0.006470325496970708\n",
      "train loss:0.0077577911130439015\n",
      "train loss:0.004720009124305482\n",
      "train loss:0.0034945880585025607\n",
      "train loss:0.014421891099395592\n",
      "train loss:0.0059532074025216855\n",
      "train loss:0.003792854565474661\n",
      "train loss:0.002122414157317665\n",
      "train loss:0.001789788032377414\n",
      "train loss:0.0011945000705307581\n",
      "=== epoch:12, train acc:0.993, test acc:0.983 ===\n",
      "train loss:0.0011606194636917331\n",
      "train loss:0.0049643323635738855\n",
      "train loss:0.004573902723612951\n",
      "train loss:0.005276338605829544\n",
      "train loss:0.000724018834146396\n",
      "train loss:0.0012976066702239278\n",
      "train loss:0.000157116037457194\n",
      "train loss:0.003947411761397297\n",
      "train loss:0.0021388992185356615\n",
      "train loss:0.020808805297600163\n",
      "train loss:0.004593334389402865\n",
      "train loss:0.0072087867133506026\n",
      "train loss:0.0015451734595001633\n",
      "train loss:0.004326697579819917\n",
      "train loss:0.0041463536091904014\n",
      "train loss:0.0036585155438291334\n",
      "train loss:0.003318669569128784\n",
      "train loss:0.0010803581930647291\n",
      "train loss:0.0007136178271311988\n",
      "train loss:0.008822407298515494\n",
      "train loss:0.001412689022611109\n",
      "train loss:0.001828165802886177\n",
      "train loss:0.0070666690358756525\n",
      "train loss:0.0035524502384149215\n",
      "train loss:0.0009688784896324224\n",
      "train loss:0.00015178509235700647\n",
      "train loss:0.005559194309174653\n",
      "train loss:0.0022685934714342508\n",
      "train loss:0.002287414247597019\n",
      "train loss:0.002003320998519985\n",
      "train loss:0.0008049149859068928\n",
      "train loss:0.032743250452260994\n",
      "train loss:0.0008102562246877026\n",
      "train loss:0.00906568520398591\n",
      "train loss:0.011284259295442065\n",
      "train loss:0.005118350589893436\n",
      "train loss:0.0045992666406158185\n",
      "train loss:0.0028231283563921335\n",
      "train loss:0.0029955065667329945\n",
      "train loss:0.0017602270126830183\n",
      "train loss:0.004848706189353593\n",
      "train loss:0.00176841860283378\n",
      "train loss:0.0043601954794590045\n",
      "train loss:0.0028272068292520772\n",
      "train loss:0.0016953560909831209\n",
      "train loss:0.006093114775423106\n",
      "train loss:0.002167983124463725\n",
      "train loss:0.0032388668002318894\n",
      "train loss:0.004120986034729176\n",
      "train loss:0.0029404162517646564\n",
      "train loss:0.00983722232656231\n",
      "train loss:0.008596088359900387\n",
      "train loss:0.005449293889052877\n",
      "train loss:0.07720142280707985\n",
      "train loss:0.012714588403955332\n",
      "train loss:0.003088600367562107\n",
      "train loss:0.0013240135777959875\n",
      "train loss:0.006056279349700936\n",
      "train loss:0.005418595117020534\n",
      "train loss:0.004761555325009344\n",
      "train loss:0.0016988845638574617\n",
      "train loss:0.0165103788441888\n",
      "train loss:0.0033989105235662782\n",
      "train loss:0.027208912933543265\n",
      "train loss:0.0006069641180105161\n",
      "train loss:0.003986857170103549\n",
      "train loss:0.0013636683252285558\n",
      "train loss:0.0029440012914194393\n",
      "train loss:0.01928414507037472\n",
      "train loss:0.0019413231854738086\n",
      "train loss:0.001701525309492763\n",
      "train loss:0.0009158908091863444\n",
      "train loss:0.008421290497670651\n",
      "train loss:0.001585964034842429\n",
      "train loss:0.0021030261154086623\n",
      "train loss:0.002521209882317527\n",
      "train loss:0.004442722578776478\n",
      "train loss:0.002878373066746072\n",
      "train loss:0.01881196629114665\n",
      "train loss:0.002767183373255988\n",
      "train loss:0.001035290374552061\n",
      "train loss:0.008326986685466333\n",
      "train loss:0.009541749623406473\n",
      "train loss:0.0034025633517195232\n",
      "train loss:0.0009627975493669845\n",
      "train loss:0.009472829189983524\n",
      "train loss:0.0005741296962509636\n",
      "train loss:0.001464400045154332\n",
      "train loss:0.01194744787238285\n",
      "train loss:0.0014596868898007279\n",
      "train loss:0.014263641120888714\n",
      "train loss:0.0030990485905690156\n",
      "train loss:0.0007328841328413094\n",
      "train loss:0.004264155037915995\n",
      "train loss:0.004544184338959326\n",
      "train loss:0.0007647669925702258\n",
      "train loss:0.00512877246815928\n",
      "train loss:0.001999604721515354\n",
      "train loss:0.0027728930231820017\n",
      "train loss:0.005744897563157233\n",
      "train loss:0.03405722178700798\n",
      "train loss:0.012494371848301511\n",
      "train loss:0.0005917142993808741\n",
      "train loss:0.0010270020806136242\n",
      "train loss:0.008125138961753068\n",
      "train loss:0.0007266721226450389\n",
      "train loss:0.003697599538851664\n",
      "train loss:0.0035299453215375614\n",
      "train loss:0.001403823971056979\n",
      "train loss:0.018978490613713307\n",
      "train loss:0.0012549500436668097\n",
      "train loss:0.004853874081151406\n",
      "train loss:0.002850047850494707\n",
      "train loss:0.0008353960956234073\n",
      "train loss:0.004309848212481166\n",
      "train loss:0.0009591195017302394\n",
      "train loss:0.007134685698296433\n",
      "train loss:0.0026524823951387688\n",
      "train loss:0.0014295651659382163\n",
      "train loss:0.0033984285263880002\n",
      "train loss:0.0011712451765359684\n",
      "train loss:0.005805779053565281\n",
      "train loss:0.0009498849846269071\n",
      "train loss:0.0032238122933910883\n",
      "train loss:0.0004608577291765218\n",
      "train loss:0.004534238860778321\n",
      "train loss:0.000568839585928021\n",
      "train loss:0.005434721510050469\n",
      "train loss:0.002272504137489538\n",
      "train loss:0.00540563263585583\n",
      "train loss:0.032436395696925656\n",
      "train loss:0.01131460897281868\n",
      "train loss:0.004570843304711184\n",
      "train loss:0.0013486428268752512\n",
      "train loss:0.0038391914772608173\n",
      "train loss:0.013244650619945975\n",
      "train loss:0.0011008588286717043\n",
      "train loss:0.014346484537419919\n",
      "train loss:0.010673300758662931\n",
      "train loss:0.000886209235884644\n",
      "train loss:0.0035419963348590194\n",
      "train loss:0.00046860004039426894\n",
      "train loss:0.000650593959355813\n",
      "train loss:0.0015604155944442702\n",
      "train loss:0.004414088860558666\n",
      "train loss:0.0006281989410278624\n",
      "train loss:0.010216734641685992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0050911270999894785\n",
      "train loss:0.0014951966506336883\n",
      "train loss:0.0005106830022921185\n",
      "train loss:0.00348056315239907\n",
      "train loss:0.002146466606191452\n",
      "train loss:0.001272065636303327\n",
      "train loss:0.007537160615230001\n",
      "train loss:0.005308085134190782\n",
      "train loss:0.0006212561022870771\n",
      "train loss:0.003643188301284099\n",
      "train loss:0.005069312706604633\n",
      "train loss:0.00024850397969504873\n",
      "train loss:0.0006810485851064367\n",
      "train loss:0.003578476677013784\n",
      "train loss:0.004266410143412711\n",
      "train loss:0.014492406508880564\n",
      "train loss:0.003702020689699566\n",
      "train loss:0.0012690887343798035\n",
      "train loss:0.011731108723546303\n",
      "train loss:0.004679320534178427\n",
      "train loss:0.0008285006254121872\n",
      "train loss:0.0011206053881640593\n",
      "train loss:0.0009936942531632567\n",
      "train loss:9.100519734457925e-05\n",
      "train loss:0.0011728154006587978\n",
      "train loss:0.0030931787692371603\n",
      "train loss:0.004612679864728918\n",
      "train loss:0.00594881235781939\n",
      "train loss:0.005796124824892513\n",
      "train loss:0.000301694547061062\n",
      "train loss:0.032173596258875385\n",
      "train loss:0.003444971015697966\n",
      "train loss:0.000303175326163266\n",
      "train loss:0.0010644997896210688\n",
      "train loss:0.0007890392182303428\n",
      "train loss:0.0012865284558952483\n",
      "train loss:0.0002300630497068197\n",
      "train loss:0.01999881040058695\n",
      "train loss:0.003920002134345024\n",
      "train loss:0.02993070393113804\n",
      "train loss:0.002552500879388238\n",
      "train loss:0.007507468771635638\n",
      "train loss:0.00232568775763591\n",
      "train loss:0.0065614489217277385\n",
      "train loss:0.0027676270289252003\n",
      "train loss:8.265015435313553e-05\n",
      "train loss:0.0006737207569171369\n",
      "train loss:0.0005397986707093102\n",
      "train loss:0.0012711309780313405\n",
      "train loss:0.002597227696205814\n",
      "train loss:0.002557760008236634\n",
      "train loss:0.0024028738332387213\n",
      "train loss:0.00427527002881343\n",
      "train loss:0.005003523660617114\n",
      "train loss:0.0020650577962853587\n",
      "train loss:0.001337527951342854\n",
      "train loss:0.005423066240944138\n",
      "train loss:0.005376332749379403\n",
      "train loss:0.013949791173382487\n",
      "train loss:0.001045864657017922\n",
      "train loss:0.0061319470283440055\n",
      "train loss:0.0036390152405123043\n",
      "train loss:0.0022991039419718324\n",
      "train loss:0.0037065683689254682\n",
      "train loss:0.0014081887896013375\n",
      "train loss:0.010335242386684325\n",
      "train loss:0.0010893722598335049\n",
      "train loss:0.009152743284906926\n",
      "train loss:0.007721347044159484\n",
      "train loss:0.0038297966828553798\n",
      "train loss:0.0014674640555927695\n",
      "train loss:0.003639331492211916\n",
      "train loss:0.0030925663985675706\n",
      "train loss:0.004739084767765575\n",
      "train loss:0.0030373792875876752\n",
      "train loss:0.0018716194176304577\n",
      "train loss:0.0033018182428139575\n",
      "train loss:0.00018624579744278396\n",
      "train loss:0.0011247850355676136\n",
      "train loss:0.0017916543440381718\n",
      "train loss:0.00022264968424586976\n",
      "train loss:0.012636507780655077\n",
      "train loss:0.0006195961508221682\n",
      "train loss:0.0002818322923751893\n",
      "train loss:0.008358339892225473\n",
      "train loss:0.004586905772778317\n",
      "train loss:0.020494521377495946\n",
      "train loss:0.026025600343718733\n",
      "train loss:0.002537707014189666\n",
      "train loss:0.0025776458548012913\n",
      "train loss:0.0038217732864072596\n",
      "train loss:0.00040863355738232925\n",
      "train loss:0.008015231040058858\n",
      "train loss:0.011265816785118113\n",
      "train loss:0.004340297630175963\n",
      "train loss:0.0012348623766068236\n",
      "train loss:0.011427176621644726\n",
      "train loss:0.021133519080974113\n",
      "train loss:0.0029191369767178825\n",
      "train loss:0.0037285772866871335\n",
      "train loss:0.001172367876602632\n",
      "train loss:0.0012953364573572129\n",
      "train loss:0.007920019497514648\n",
      "train loss:0.0023774224819138544\n",
      "train loss:0.003323106458156234\n",
      "train loss:0.001958167745433167\n",
      "train loss:0.03968569217635608\n",
      "train loss:0.004289595566060751\n",
      "train loss:0.0016316438138020157\n",
      "train loss:0.003565446953642734\n",
      "train loss:0.0005437983725916523\n",
      "train loss:0.004170472301435776\n",
      "train loss:0.004043953205124318\n",
      "train loss:0.0025226295026917226\n",
      "train loss:0.03184104518410457\n",
      "train loss:0.001682565383888992\n",
      "train loss:0.0010412572549186453\n",
      "train loss:0.0034130983949947798\n",
      "train loss:0.006697084267786111\n",
      "train loss:0.016971577789603055\n",
      "train loss:0.003238226998542629\n",
      "train loss:0.001214054530338362\n",
      "train loss:0.0034735793595869736\n",
      "train loss:0.0007221968724196861\n",
      "train loss:0.012072388421889098\n",
      "train loss:0.0037138809313056066\n",
      "train loss:0.001559906920478238\n",
      "train loss:0.0012960714784658206\n",
      "train loss:0.0010482246333872347\n",
      "train loss:0.0012360235960129564\n",
      "train loss:0.0024017077922331743\n",
      "train loss:0.008746723561372157\n",
      "train loss:0.012248515369627122\n",
      "train loss:0.005780277565657462\n",
      "train loss:0.002631521924023948\n",
      "train loss:0.0020042705033581167\n",
      "train loss:0.029623633481426593\n",
      "train loss:0.0012207925195469498\n",
      "train loss:0.004651198916862192\n",
      "train loss:0.0015150858579216426\n",
      "train loss:0.004770202863835239\n",
      "train loss:0.0013514913454655496\n",
      "train loss:0.007763115342418944\n",
      "train loss:0.002185276327671298\n",
      "train loss:0.020456333909741106\n",
      "train loss:0.004940647010918012\n",
      "train loss:0.006033653728465006\n",
      "train loss:0.0017442770011933196\n",
      "train loss:0.001333261614797623\n",
      "train loss:0.000889434791407457\n",
      "train loss:0.0011031485084813592\n",
      "train loss:0.0012030169694024042\n",
      "train loss:0.019461041746511698\n",
      "train loss:0.000993247844535163\n",
      "train loss:0.007408190025085018\n",
      "train loss:0.0020656455186121273\n",
      "train loss:0.003538744613880362\n",
      "train loss:0.03415265349602171\n",
      "train loss:0.0014061353895180478\n",
      "train loss:0.0016529042751641492\n",
      "train loss:0.002802331362228578\n",
      "train loss:0.004767588992516345\n",
      "train loss:0.013152163504761845\n",
      "train loss:0.0008368221128227321\n",
      "train loss:0.001738866113748054\n",
      "train loss:0.0005282557418642999\n",
      "train loss:0.004540631090226047\n",
      "train loss:0.004781202005548375\n",
      "train loss:0.004477198731745489\n",
      "train loss:0.0033779804177396424\n",
      "train loss:0.006955569619875184\n",
      "train loss:0.002189825344560848\n",
      "train loss:0.002157075263014484\n",
      "train loss:0.0020471211822250548\n",
      "train loss:0.0008082750192925449\n",
      "train loss:0.0012070664595249033\n",
      "train loss:0.0007028046808672458\n",
      "train loss:0.00041824801912085445\n",
      "train loss:0.0039449415204653795\n",
      "train loss:0.002205572882566812\n",
      "train loss:0.0009687549497259603\n",
      "train loss:0.004760861503431788\n",
      "train loss:0.00678328459021222\n",
      "train loss:0.0006138299467836613\n",
      "train loss:0.004547790681859618\n",
      "train loss:0.004085567242510398\n",
      "train loss:0.0003378573369779359\n",
      "train loss:0.003728872914530317\n",
      "train loss:0.0028180212104252657\n",
      "train loss:0.0029636219222889297\n",
      "train loss:0.0033979088489176235\n",
      "train loss:0.012220962757724078\n",
      "train loss:0.004667294623370511\n",
      "train loss:0.0021182886719769947\n",
      "train loss:0.00040323720813664326\n",
      "train loss:0.0011020532255188167\n",
      "train loss:0.0012324767137838553\n",
      "train loss:0.0126662482372382\n",
      "train loss:0.0018147003378375214\n",
      "train loss:0.004282000938084724\n",
      "train loss:0.000387431695861212\n",
      "train loss:0.013775846363121513\n",
      "train loss:0.0011517650114871896\n",
      "train loss:0.011890337606685595\n",
      "train loss:0.00372459187765594\n",
      "train loss:0.016045884854044127\n",
      "train loss:0.0036201226424319114\n",
      "train loss:0.0010369960394290157\n",
      "train loss:0.0019607249344740724\n",
      "train loss:0.001436090130682811\n",
      "train loss:0.004936477915611398\n",
      "train loss:0.005581317591746462\n",
      "train loss:0.0031831445399793617\n",
      "train loss:0.004706923951463887\n",
      "train loss:0.00040601030538256076\n",
      "train loss:0.00047400981429552235\n",
      "train loss:0.015400445727971883\n",
      "train loss:0.007122093247054071\n",
      "train loss:0.009149331957868547\n",
      "train loss:0.0013205851224784861\n",
      "train loss:0.015098925513968875\n",
      "train loss:0.0007619489381397762\n",
      "train loss:0.007221638323860085\n",
      "train loss:0.018333579946316684\n",
      "train loss:0.009609456594968296\n",
      "train loss:0.0008502787535938686\n",
      "train loss:0.004979755737738388\n",
      "train loss:0.00012544748297891705\n",
      "train loss:0.0030120927784861025\n",
      "train loss:0.006828702843863062\n",
      "train loss:0.00201305952072248\n",
      "train loss:0.0036866845774138905\n",
      "train loss:0.0009801321565056232\n",
      "train loss:0.004147791708141882\n",
      "train loss:0.00555103463497853\n",
      "train loss:0.0027210213007832656\n",
      "train loss:0.002298939610763459\n",
      "train loss:0.00104620056971822\n",
      "train loss:0.028697373085961334\n",
      "train loss:0.007801163712375564\n",
      "train loss:0.00900633636937918\n",
      "train loss:0.0007809632793390564\n",
      "train loss:0.00524206540231186\n",
      "train loss:0.016616553533661895\n",
      "train loss:0.0006673606333966872\n",
      "train loss:0.0046967750505042035\n",
      "train loss:0.013136711962307108\n",
      "train loss:0.0018685814212426202\n",
      "train loss:0.0012938208386603543\n",
      "train loss:0.003051471167419101\n",
      "train loss:0.0030351026934210584\n",
      "train loss:0.0421054787626558\n",
      "train loss:0.005154282858872195\n",
      "train loss:0.006396251904226698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0025783261010969725\n",
      "train loss:0.0018010946631918778\n",
      "train loss:0.006028123930978156\n",
      "train loss:0.01603431172447167\n",
      "train loss:0.010652236492849902\n",
      "train loss:0.00043445513445635386\n",
      "train loss:0.009534677696470933\n",
      "train loss:0.0024247569862976418\n",
      "train loss:0.00814246407942833\n",
      "train loss:0.001343411604847646\n",
      "train loss:0.002672378848865996\n",
      "train loss:0.00026143783567218406\n",
      "train loss:0.006345305060284241\n",
      "train loss:0.0049547196223198465\n",
      "train loss:0.0048060227977511924\n",
      "train loss:0.003512983658502308\n",
      "train loss:0.002505650680189236\n",
      "train loss:0.09913731526539048\n",
      "train loss:0.005154798532943835\n",
      "train loss:0.002909229746880832\n",
      "train loss:0.0008952016655554988\n",
      "train loss:0.00570637175207343\n",
      "train loss:0.0340631136356562\n",
      "train loss:0.0001716955542753277\n",
      "train loss:0.018628825515812665\n",
      "train loss:0.003822456163610302\n",
      "train loss:0.0012032151531062446\n",
      "train loss:0.0010744277577284277\n",
      "train loss:0.018591296477058514\n",
      "train loss:0.002507935423490476\n",
      "train loss:0.00041759124003877735\n",
      "train loss:0.0029355180789103845\n",
      "train loss:0.0034055527262001994\n",
      "train loss:0.0019666034092539007\n",
      "train loss:0.0010983883805999115\n",
      "train loss:0.0010232920492620234\n",
      "train loss:0.002325197400941195\n",
      "train loss:0.0012782320840986452\n",
      "train loss:0.007479683260205135\n",
      "train loss:0.008017221347730355\n",
      "train loss:0.00640312942857083\n",
      "train loss:0.0031113751971491776\n",
      "train loss:0.0012593232653252693\n",
      "train loss:0.004344021072101256\n",
      "train loss:0.007344131336542437\n",
      "train loss:0.0010532873677587822\n",
      "train loss:0.004590217858862801\n",
      "train loss:0.0020077656176420086\n",
      "train loss:0.007636731358259945\n",
      "train loss:0.0035198778321574383\n",
      "train loss:0.021548072894771845\n",
      "train loss:0.009395252489997886\n",
      "train loss:0.018130989566208552\n",
      "train loss:0.0001092920249777341\n",
      "train loss:0.00169864413235874\n",
      "train loss:0.0012206219436730755\n",
      "train loss:0.0031207334861490465\n",
      "train loss:0.002134603059068125\n",
      "train loss:0.0078166236857428\n",
      "train loss:0.001697973920817996\n",
      "train loss:0.011756191478882549\n",
      "train loss:0.04054811036552862\n",
      "train loss:0.007864919515891283\n",
      "train loss:0.01235795969049183\n",
      "train loss:0.0037573467354470856\n",
      "train loss:0.00011645273946003788\n",
      "train loss:0.008745717297856592\n",
      "train loss:0.0046140288764457095\n",
      "train loss:0.038523338937957764\n",
      "train loss:0.006439750281764859\n",
      "train loss:0.0035346688957057933\n",
      "train loss:0.002406722706895878\n",
      "train loss:0.0038548052495045015\n",
      "train loss:0.004755197037557144\n",
      "train loss:0.0002976655514209826\n",
      "train loss:0.00686316736269434\n",
      "train loss:0.004628180179347888\n",
      "train loss:0.0003058923657814813\n",
      "train loss:0.004897548841635433\n",
      "train loss:0.00023341676469608232\n",
      "train loss:0.0027222284987983604\n",
      "train loss:0.0033606429995493114\n",
      "train loss:0.0026151284365227845\n",
      "train loss:0.0037040075053068654\n",
      "train loss:0.006364855308836809\n",
      "train loss:0.0010460499925422587\n",
      "train loss:0.0011328156195538656\n",
      "train loss:0.0009106833829932222\n",
      "train loss:0.010016699881058817\n",
      "train loss:0.00021444479669375315\n",
      "train loss:0.0033033706733462194\n",
      "train loss:0.0020790293621074166\n",
      "train loss:0.003376098200676717\n",
      "train loss:0.048231564786245455\n",
      "train loss:0.0010452499817756363\n",
      "train loss:0.0031326629793838167\n",
      "train loss:0.0003672910899753322\n",
      "train loss:0.0012675699914569145\n",
      "train loss:0.0006857817159463786\n",
      "train loss:0.0021211955590876965\n",
      "train loss:0.002838108925268295\n",
      "train loss:0.0013744094852502942\n",
      "train loss:0.0007281793414002501\n",
      "train loss:0.005407014978237196\n",
      "train loss:0.005125156554480989\n",
      "train loss:0.0006665133820169175\n",
      "train loss:0.006416116157013064\n",
      "train loss:0.007434531788056927\n",
      "train loss:0.0004680801333745955\n",
      "train loss:0.0007514083867926015\n",
      "train loss:0.0006963966510428372\n",
      "train loss:0.0028733824977319818\n",
      "train loss:0.01677080385966756\n",
      "train loss:0.001507554158211724\n",
      "train loss:0.03892558785641646\n",
      "train loss:0.004922710675765827\n",
      "train loss:0.0006670014488049201\n",
      "train loss:0.0026596785445514716\n",
      "train loss:0.006017666498173722\n",
      "train loss:0.0033962893166398635\n",
      "train loss:0.00234638952807342\n",
      "train loss:0.0015793625943417292\n",
      "train loss:0.05806853469123614\n",
      "train loss:0.002588837893800972\n",
      "train loss:0.006250605273700583\n",
      "train loss:0.004877821670564712\n",
      "train loss:0.0015256341545251898\n",
      "train loss:0.0029384666657799987\n",
      "train loss:0.00667052969183765\n",
      "train loss:0.004092653184704673\n",
      "train loss:0.001492355533109553\n",
      "train loss:0.0007509869386327459\n",
      "train loss:0.00032570185805914974\n",
      "train loss:0.0011512119017788513\n",
      "train loss:0.0013894826782587832\n",
      "train loss:0.005656528284006042\n",
      "train loss:0.003744554028445005\n",
      "train loss:0.0034794266897168486\n",
      "train loss:0.0003719940326681258\n",
      "train loss:0.008127099426019921\n",
      "train loss:0.03190689088533673\n",
      "train loss:0.007795861594022243\n",
      "train loss:0.006830620306539415\n",
      "train loss:0.002089504569253464\n",
      "train loss:0.005979801500195099\n",
      "train loss:0.004118378488428297\n",
      "train loss:0.0005932935574073963\n",
      "train loss:0.0033584436146513164\n",
      "train loss:0.0029113349876721843\n",
      "train loss:0.0007266097242990369\n",
      "train loss:0.00287400069616532\n",
      "train loss:0.0029351563191591466\n",
      "train loss:0.004896861384978426\n",
      "train loss:0.003115515145636173\n",
      "train loss:0.0022878395475202807\n",
      "train loss:0.0022292453940393605\n",
      "train loss:0.01574399513323108\n",
      "train loss:0.0017364721629563285\n",
      "train loss:0.005329803966353047\n",
      "train loss:0.002055936958217632\n",
      "train loss:0.0016875672285170246\n",
      "train loss:0.0020125342363394435\n",
      "train loss:0.00023667059679416869\n",
      "train loss:0.0018078827019896845\n",
      "train loss:0.0016086157567065207\n",
      "train loss:0.011806247136276966\n",
      "train loss:0.004329175313266224\n",
      "train loss:0.0005960585280679536\n",
      "train loss:0.009286171937332898\n",
      "train loss:0.004983459176021249\n",
      "train loss:0.00204174199678502\n",
      "train loss:0.0008905361715956581\n",
      "train loss:0.0009725626510860701\n",
      "train loss:0.0034227244005206863\n",
      "train loss:0.005021054407098064\n",
      "train loss:0.005871513450405248\n",
      "train loss:0.00406462335251472\n",
      "train loss:0.002741443432994038\n",
      "train loss:0.0028738573330505494\n",
      "train loss:0.003258985393080964\n",
      "train loss:0.005435442061834454\n",
      "train loss:0.000980342555695986\n",
      "train loss:0.0037067174566783384\n",
      "train loss:0.002529193978194402\n",
      "train loss:0.0009151467420535092\n",
      "train loss:0.007320559260638312\n",
      "train loss:0.0042008140552708545\n",
      "train loss:0.00794363578523147\n",
      "train loss:0.008778132575333913\n",
      "train loss:0.0026346393306256365\n",
      "train loss:0.005483232478725073\n",
      "train loss:0.0014461338662916609\n",
      "train loss:0.004075216584088202\n",
      "train loss:0.0009753082740322087\n",
      "train loss:0.001340027476157079\n",
      "train loss:0.0032500776272989\n",
      "train loss:0.0072732312112794885\n",
      "train loss:0.004521008054434347\n",
      "train loss:0.003631455254884762\n",
      "=== epoch:13, train acc:0.995, test acc:0.979 ===\n",
      "train loss:0.003013993188133462\n",
      "train loss:0.01934990948557923\n",
      "train loss:0.0076254289504260955\n",
      "train loss:0.0034213844878224204\n",
      "train loss:0.004546163083375706\n",
      "train loss:0.001242777486795704\n",
      "train loss:0.012753845513885398\n",
      "train loss:0.028879922435261007\n",
      "train loss:0.0042147751960316645\n",
      "train loss:0.003706263117565952\n",
      "train loss:0.005153316696878621\n",
      "train loss:0.0024744368614594385\n",
      "train loss:0.003986440388506223\n",
      "train loss:0.004992484851822233\n",
      "train loss:0.008948747489310321\n",
      "train loss:0.006075747072121647\n",
      "train loss:0.035586298451580595\n",
      "train loss:0.015073162520595604\n",
      "train loss:0.032509082060120875\n",
      "train loss:0.005346534575254954\n",
      "train loss:0.0016077396815412964\n",
      "train loss:0.0021294236020049646\n",
      "train loss:0.020654185316995387\n",
      "train loss:0.00546798322876847\n",
      "train loss:0.0008621221188060159\n",
      "train loss:0.0009186011995793252\n",
      "train loss:0.0018026416566591244\n",
      "train loss:0.0019564191854209677\n",
      "train loss:0.003089111772094792\n",
      "train loss:0.011705052590029805\n",
      "train loss:0.0010689812943003108\n",
      "train loss:0.0031088761338478855\n",
      "train loss:0.0061557658124876714\n",
      "train loss:0.007667665273195521\n",
      "train loss:0.009567243263445793\n",
      "train loss:0.008120627469679866\n",
      "train loss:0.006312424201963809\n",
      "train loss:0.03572816219221929\n",
      "train loss:0.0029006661129654006\n",
      "train loss:0.00021066897882565712\n",
      "train loss:0.0012477207333155304\n",
      "train loss:0.004110625375496528\n",
      "train loss:0.000721257455525848\n",
      "train loss:0.015916185175762362\n",
      "train loss:0.003509885538339816\n",
      "train loss:0.004261081497257368\n",
      "train loss:0.001144555666425966\n",
      "train loss:0.00043736304974059084\n",
      "train loss:0.0022748333316529416\n",
      "train loss:0.005372397631133944\n",
      "train loss:0.0006168515381505719\n",
      "train loss:0.004522939018962577\n",
      "train loss:0.0014122507937701823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0037398898829251914\n",
      "train loss:0.006678766272216622\n",
      "train loss:0.0024310834134117065\n",
      "train loss:0.001834264106662073\n",
      "train loss:0.003570298365756052\n",
      "train loss:0.0035962061889766883\n",
      "train loss:0.0048313709260650005\n",
      "train loss:0.002089185240717678\n",
      "train loss:0.010584810078811227\n",
      "train loss:0.0134004963129789\n",
      "train loss:0.0015296676422916426\n",
      "train loss:0.0024050157639769025\n",
      "train loss:0.0011247071298816837\n",
      "train loss:0.0014177073783361504\n",
      "train loss:0.011168805231793815\n",
      "train loss:0.003234585298768416\n",
      "train loss:0.009998617221647266\n",
      "train loss:0.0013678947724144552\n",
      "train loss:0.0010353111951767475\n",
      "train loss:0.0004807639273050918\n",
      "train loss:0.0018979372309109105\n",
      "train loss:0.009425113037019123\n",
      "train loss:0.0016072049386326736\n",
      "train loss:0.009097116276954078\n",
      "train loss:0.005931932835321353\n",
      "train loss:0.0063622847481756735\n",
      "train loss:0.015019390487587745\n",
      "train loss:0.001784092128472994\n",
      "train loss:0.0074670038324011565\n",
      "train loss:0.0012514341884646352\n",
      "train loss:0.0054162373675807705\n",
      "train loss:0.0015579785274208952\n",
      "train loss:0.002891356797675177\n",
      "train loss:0.0038579885991195232\n",
      "train loss:0.0010144911786183445\n",
      "train loss:0.0058064140482835536\n",
      "train loss:0.0007117460217239374\n",
      "train loss:0.0019263330318541145\n",
      "train loss:0.06710510320400201\n",
      "train loss:0.005588088354477214\n",
      "train loss:0.001980951020397648\n",
      "train loss:0.0012658793736441895\n",
      "train loss:0.00038727038942067306\n",
      "train loss:0.0014652664819198064\n",
      "train loss:0.00019711238588642591\n",
      "train loss:0.002616902182701551\n",
      "train loss:0.010258876960941774\n",
      "train loss:0.0027335432409952955\n",
      "train loss:0.0073345854814987855\n",
      "train loss:0.0011194187405748205\n",
      "train loss:0.0010423033002998666\n",
      "train loss:0.031261530006060884\n",
      "train loss:0.00012188837076779823\n",
      "train loss:0.009179505549647277\n",
      "train loss:0.0009775169653293298\n",
      "train loss:0.0007731009554251212\n",
      "train loss:0.008009256683652217\n",
      "train loss:0.0011366561926285726\n",
      "train loss:0.003950705721444985\n",
      "train loss:0.004452584248921364\n",
      "train loss:0.007586668803507831\n",
      "train loss:0.001182424352862137\n",
      "train loss:0.004389332484071675\n",
      "train loss:0.0007949416618442395\n",
      "train loss:0.0024297339546614145\n",
      "train loss:0.0032542281195979184\n",
      "train loss:0.01629468503365847\n",
      "train loss:0.0014610093894821302\n",
      "train loss:0.0008397697835490356\n",
      "train loss:0.0032889987807395997\n",
      "train loss:0.006595533936405554\n",
      "train loss:0.0011914460073645702\n",
      "train loss:0.0008568260020694235\n",
      "train loss:0.008286892703862627\n",
      "train loss:0.006435549402911176\n",
      "train loss:0.006122447475358102\n",
      "train loss:0.000665451212721103\n",
      "train loss:0.00010463818483819618\n",
      "train loss:0.004611696572270613\n",
      "train loss:0.0015246364997436334\n",
      "train loss:0.006465809499538635\n",
      "train loss:0.002308130434245864\n",
      "train loss:0.0008730855982696018\n",
      "train loss:0.00259252106020534\n",
      "train loss:0.001206631837848489\n",
      "train loss:0.0027270268948824837\n",
      "train loss:0.0017009247211374982\n",
      "train loss:0.0095525363173477\n",
      "train loss:0.004968606917497091\n",
      "train loss:0.005768661294835657\n",
      "train loss:0.001493353100339866\n",
      "train loss:0.0010692236515131494\n",
      "train loss:0.011185617658687691\n",
      "train loss:0.0021152712867062383\n",
      "train loss:0.001055733758186807\n",
      "train loss:0.0136475912744435\n",
      "train loss:0.006506972646932241\n",
      "train loss:0.000710834330977773\n",
      "train loss:0.0008072090056776426\n",
      "train loss:0.0025918456260171007\n",
      "train loss:0.0007699364623008679\n",
      "train loss:0.009057028924423083\n",
      "train loss:0.0018265471230345782\n",
      "train loss:0.008666287672141684\n",
      "train loss:0.0029673075190990244\n",
      "train loss:0.003811679000794418\n",
      "train loss:0.0006391667534379822\n",
      "train loss:0.0027803900558317166\n",
      "train loss:0.0027940481001044693\n",
      "train loss:0.00033467432270799924\n",
      "train loss:0.014177209511589114\n",
      "train loss:0.0008776523170278118\n",
      "train loss:0.0343910324331101\n",
      "train loss:0.0006257870710133532\n",
      "train loss:0.036228266586656364\n",
      "train loss:0.0019829140805161668\n",
      "train loss:0.00191631874383703\n",
      "train loss:0.0034442297516703376\n",
      "train loss:0.0016280037594069898\n",
      "train loss:0.010142726393356411\n",
      "train loss:0.029959432035100052\n",
      "train loss:0.004441469207096662\n",
      "train loss:0.006026715484772024\n",
      "train loss:0.009828080578316096\n",
      "train loss:0.008231984926989245\n",
      "train loss:0.003007553357099687\n",
      "train loss:0.0019552939412126407\n",
      "train loss:0.0012249419941858406\n",
      "train loss:0.004461626521511616\n",
      "train loss:0.011769922532753933\n",
      "train loss:0.0013001590101948829\n",
      "train loss:0.006643026361224963\n",
      "train loss:0.005371800744759534\n",
      "train loss:0.009488039746827296\n",
      "train loss:0.0022267445724567414\n",
      "train loss:0.0012309189922516648\n",
      "train loss:0.008313994439410917\n",
      "train loss:0.05288577630764656\n",
      "train loss:0.014027895711223773\n",
      "train loss:0.016332606297339455\n",
      "train loss:0.006085714854875345\n",
      "train loss:0.003047140618775695\n",
      "train loss:0.002838444409673982\n",
      "train loss:0.01343296164104168\n",
      "train loss:0.004049167423400626\n",
      "train loss:0.004162227307115678\n",
      "train loss:0.007984656280251669\n",
      "train loss:0.0019106330774601066\n",
      "train loss:0.0033121766477309756\n",
      "train loss:0.0012818939306131533\n",
      "train loss:0.0003375720912742774\n",
      "train loss:0.0032219528506720256\n",
      "train loss:0.0010768942994135366\n",
      "train loss:0.0024707860089748165\n",
      "train loss:0.00831477337517473\n",
      "train loss:5.8945214512869645e-05\n",
      "train loss:0.0024768383258983763\n",
      "train loss:0.0016602739093850337\n",
      "train loss:0.023917866203474216\n",
      "train loss:0.026710795320087136\n",
      "train loss:0.0043395521297390025\n",
      "train loss:0.0069386658182623686\n",
      "train loss:0.0013745362528596048\n",
      "train loss:0.0027185785525125077\n",
      "train loss:0.0017948812534038213\n",
      "train loss:0.00976722313939385\n",
      "train loss:0.0012092364777591077\n",
      "train loss:0.0002441571043074662\n",
      "train loss:0.004319935038376282\n",
      "train loss:0.0007312085783632264\n",
      "train loss:0.009940132278452907\n",
      "train loss:0.0008122231114424523\n",
      "train loss:0.006516420107522443\n",
      "train loss:0.00036259598546567814\n",
      "train loss:0.014451054892144417\n",
      "train loss:0.002176647547964483\n",
      "train loss:0.01482990978366588\n",
      "train loss:0.01123995751588114\n",
      "train loss:0.0010722730035358497\n",
      "train loss:0.0008153140939680721\n",
      "train loss:0.0016777971474943678\n",
      "train loss:0.0005902177858830149\n",
      "train loss:0.0009823539526906937\n",
      "train loss:0.0077776933091698195\n",
      "train loss:0.0032230655386904466\n",
      "train loss:0.0018033326186548096\n",
      "train loss:0.003523385916149918\n",
      "train loss:0.0007989218430146476\n",
      "train loss:0.004508714505927464\n",
      "train loss:0.002120195702574678\n",
      "train loss:0.008656536822772598\n",
      "train loss:0.0008940604597505025\n",
      "train loss:0.0009343047700001586\n",
      "train loss:0.0029258222752955336\n",
      "train loss:0.005050307942358545\n",
      "train loss:0.013043064953505719\n",
      "train loss:0.00699065905094563\n",
      "train loss:0.012118294594274015\n",
      "train loss:0.0016527803581560358\n",
      "train loss:0.001919301603464348\n",
      "train loss:0.002947019540807597\n",
      "train loss:0.0031409979288812362\n",
      "train loss:0.0033183639661572522\n",
      "train loss:0.000660867122661627\n",
      "train loss:0.0015804907178717026\n",
      "train loss:0.0007644481252612378\n",
      "train loss:0.005468995047929289\n",
      "train loss:0.0011679561122791052\n",
      "train loss:0.0020896196366261962\n",
      "train loss:0.006809021466577577\n",
      "train loss:0.01096808362486222\n",
      "train loss:0.006601491433015061\n",
      "train loss:0.0008088667990828062\n",
      "train loss:0.001853519965886244\n",
      "train loss:0.00041946513359686474\n",
      "train loss:0.003695272312621374\n",
      "train loss:0.004300581825845242\n",
      "train loss:0.0003782522975799351\n",
      "train loss:0.00011743398865209658\n",
      "train loss:0.004276460284206086\n",
      "train loss:0.0020038865421869523\n",
      "train loss:0.018563543532326685\n",
      "train loss:0.0011851585584561981\n",
      "train loss:0.0005126007484167346\n",
      "train loss:0.006554282933566064\n",
      "train loss:0.0006572418065547698\n",
      "train loss:0.0014139784936607605\n",
      "train loss:0.0005545124128612365\n",
      "train loss:0.007723316028492382\n",
      "train loss:0.008471303031806893\n",
      "train loss:0.0011091181028333524\n",
      "train loss:0.005303924294498453\n",
      "train loss:0.00021965326777306158\n",
      "train loss:0.0027012317247488266\n",
      "train loss:0.0029614802572906895\n",
      "train loss:0.001285582669449456\n",
      "train loss:0.0067966788903661216\n",
      "train loss:0.16109522233772772\n",
      "train loss:0.0020524681965209224\n",
      "train loss:0.002839098022454882\n",
      "train loss:0.0013940754931923902\n",
      "train loss:0.003968247328635887\n",
      "train loss:0.0032863334986669656\n",
      "train loss:0.010350713838808045\n",
      "train loss:0.0006682605685255454\n",
      "train loss:0.0027685515653023056\n",
      "train loss:0.0013704131605604636\n",
      "train loss:0.005422578397209981\n",
      "train loss:0.002004919704936497\n",
      "train loss:0.0031680045412537535\n",
      "train loss:0.0020312802838272264\n",
      "train loss:0.0018774566128224681\n",
      "train loss:0.000539186122757742\n",
      "train loss:0.0029100329413823595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0005703159958279547\n",
      "train loss:0.0032805108985530635\n",
      "train loss:0.0016015341515529136\n",
      "train loss:0.0036484221430648505\n",
      "train loss:0.028962336507280754\n",
      "train loss:0.0026219925402535763\n",
      "train loss:0.003079279201134766\n",
      "train loss:0.00555899484071221\n",
      "train loss:0.0034397470744173663\n",
      "train loss:0.013186233148859812\n",
      "train loss:0.0019751984430445993\n",
      "train loss:0.005041140528745046\n",
      "train loss:0.0029086507080051177\n",
      "train loss:0.0008845332778309073\n",
      "train loss:0.00038459111369052996\n",
      "train loss:0.012183883430808057\n",
      "train loss:0.0048241229925879035\n",
      "train loss:0.0005996203122937447\n",
      "train loss:0.0022576042312693004\n",
      "train loss:0.008936911659175345\n",
      "train loss:0.0060002395653882944\n",
      "train loss:0.003038737444880773\n",
      "train loss:0.0009913124920395836\n",
      "train loss:0.002959536299904295\n",
      "train loss:0.0015949719767813935\n",
      "train loss:0.0012175460759582999\n",
      "train loss:0.0022168570970778003\n",
      "train loss:0.005100135661700724\n",
      "train loss:0.0017440469754697013\n",
      "train loss:0.0007051718688139118\n",
      "train loss:0.0008036578513337631\n",
      "train loss:0.0007253656364588762\n",
      "train loss:0.00329298983695965\n",
      "train loss:0.0008343068937732522\n",
      "train loss:0.0009770195605045592\n",
      "train loss:0.0014495514438137075\n",
      "train loss:0.004563777871427355\n",
      "train loss:0.0017804221281562931\n",
      "train loss:0.002422830523552563\n",
      "train loss:0.00015557023437680315\n",
      "train loss:0.0007254659887672377\n",
      "train loss:0.0040900695318162065\n",
      "train loss:0.00046654353887097576\n",
      "train loss:0.0019780210984618697\n",
      "train loss:0.007712706590734321\n",
      "train loss:0.0005553762432226813\n",
      "train loss:0.010771602219679382\n",
      "train loss:0.0005824202077703837\n",
      "train loss:0.0022544721399382008\n",
      "train loss:0.0019227429930764237\n",
      "train loss:0.007697645134528624\n",
      "train loss:0.002750522495209154\n",
      "train loss:0.0017582662562085401\n",
      "train loss:0.00042736637187401253\n",
      "train loss:0.00022725508902319923\n",
      "train loss:0.0008589952970694592\n",
      "train loss:0.0013765038324814608\n",
      "train loss:0.004928985144904885\n",
      "train loss:0.00777459159161793\n",
      "train loss:0.005896267272333099\n",
      "train loss:0.0018199582786749002\n",
      "train loss:0.0008241041316298108\n",
      "train loss:0.0009885764308749311\n",
      "train loss:0.012631586063816959\n",
      "train loss:0.005110459709842058\n",
      "train loss:0.00893634535741249\n",
      "train loss:0.00786170912475817\n",
      "train loss:0.0015142731274693678\n",
      "train loss:0.0004879859744564172\n",
      "train loss:0.0022258276650874136\n",
      "train loss:0.0011863753829566342\n",
      "train loss:0.004383687073700172\n",
      "train loss:0.0010471704372745558\n",
      "train loss:0.0004268448856170793\n",
      "train loss:0.0021357942731310875\n",
      "train loss:0.0018931861636512423\n",
      "train loss:0.010926763061978864\n",
      "train loss:0.0007843034058745338\n",
      "train loss:0.0026793250720382827\n",
      "train loss:0.0005513611180524667\n",
      "train loss:0.001239242102799939\n",
      "train loss:0.002646295491397933\n",
      "train loss:0.0027861656398678034\n",
      "train loss:0.0004425968379498813\n",
      "train loss:0.002209868806949091\n",
      "train loss:0.0050231956269310325\n",
      "train loss:0.0033601587902872716\n",
      "train loss:0.0022300710978997067\n",
      "train loss:0.00372993658116387\n",
      "train loss:0.005704727549620141\n",
      "train loss:0.0019013803837346541\n",
      "train loss:0.0019672144094548256\n",
      "train loss:0.0002741774746398656\n",
      "train loss:0.0023976720157645576\n",
      "train loss:0.0028857244275474476\n",
      "train loss:0.004363737389358694\n",
      "train loss:0.005777681672608951\n",
      "train loss:0.00039885446179810727\n",
      "train loss:0.003093514970830872\n",
      "train loss:0.0006382011882906973\n",
      "train loss:0.00047720062642115236\n",
      "train loss:0.002743742258764914\n",
      "train loss:0.0021072723143651835\n",
      "train loss:0.00040176416805159357\n",
      "train loss:0.000124153708912732\n",
      "train loss:0.004665686145746532\n",
      "train loss:0.004285630206522854\n",
      "train loss:0.00012117080765488517\n",
      "train loss:0.0025074066892484988\n",
      "train loss:0.0004216914179362843\n",
      "train loss:0.001318648568341248\n",
      "train loss:0.008123839899576091\n",
      "train loss:0.0015100959210721302\n",
      "train loss:0.0024999026294367445\n",
      "train loss:0.0014155655407279242\n",
      "train loss:0.004296348173576387\n",
      "train loss:0.000526850254019999\n",
      "train loss:0.005015766258076839\n",
      "train loss:0.0030431551790621337\n",
      "train loss:0.045701008773529964\n",
      "train loss:0.0012472047834760963\n",
      "train loss:0.0026219258384942595\n",
      "train loss:0.003979957969642515\n",
      "train loss:0.002652187931802764\n",
      "train loss:0.002595202009131562\n",
      "train loss:0.001786356889048232\n",
      "train loss:0.006649713923745632\n",
      "train loss:0.0006927933692314986\n",
      "train loss:0.0017030319514073414\n",
      "train loss:0.0006467551544367286\n",
      "train loss:0.004506267840423504\n",
      "train loss:0.0014020932927189026\n",
      "train loss:0.004259566380002594\n",
      "train loss:0.0027560459321799287\n",
      "train loss:0.00029676443190922505\n",
      "train loss:0.0022065111494432898\n",
      "train loss:0.00158506066692499\n",
      "train loss:0.00017919727320586185\n",
      "train loss:0.0004533099077061122\n",
      "train loss:0.003752347934212823\n",
      "train loss:0.001546512591944675\n",
      "train loss:0.0016111608219002477\n",
      "train loss:0.002008303233912643\n",
      "train loss:0.01101003529865575\n",
      "train loss:0.00658365983469133\n",
      "train loss:0.007581139351629167\n",
      "train loss:0.0008200822585360348\n",
      "train loss:0.015921721797364674\n",
      "train loss:0.004628208674737426\n",
      "train loss:0.00012943408945633367\n",
      "train loss:0.001713967863457249\n",
      "train loss:0.012462719534044956\n",
      "train loss:0.0023593375725568713\n",
      "train loss:0.001369878566732593\n",
      "train loss:0.0016742867350800414\n",
      "train loss:0.00023173032520895986\n",
      "train loss:0.0004156050650532105\n",
      "train loss:0.0004660956350510278\n",
      "train loss:0.007416690465699261\n",
      "train loss:0.0008484284982326033\n",
      "train loss:0.03390055156559272\n",
      "train loss:0.005659920293319317\n",
      "train loss:0.0011454866002090943\n",
      "train loss:0.01035197721143604\n",
      "train loss:0.0004410934127621973\n",
      "train loss:0.027166548752520253\n",
      "train loss:0.005740639594342798\n",
      "train loss:0.0008552808852334263\n",
      "train loss:0.002733504745075185\n",
      "train loss:0.004726167937744555\n",
      "train loss:0.00033332238269702255\n",
      "train loss:0.010797681642418124\n",
      "train loss:0.008751843750263041\n",
      "train loss:0.0011323727600107331\n",
      "train loss:0.00039434328275350606\n",
      "train loss:0.008806912950990344\n",
      "train loss:0.0038245077860879677\n",
      "train loss:0.006243696999485415\n",
      "train loss:0.0017231550742716395\n",
      "train loss:0.0023787481856957543\n",
      "train loss:0.00029853843497083766\n",
      "train loss:0.04801899553431447\n",
      "train loss:0.0008225428889429286\n",
      "train loss:0.003273810109335115\n",
      "train loss:0.001117068882014076\n",
      "train loss:0.006727462699379611\n",
      "train loss:0.0008143970535884864\n",
      "train loss:0.005225860355049064\n",
      "train loss:4.3824532554629125e-05\n",
      "train loss:0.014946845568480758\n",
      "train loss:0.006885899215934685\n",
      "train loss:0.0005474507408581503\n",
      "train loss:0.0009395364951063827\n",
      "train loss:0.00021072474329834637\n",
      "train loss:0.027007751035819066\n",
      "train loss:0.012500115225778404\n",
      "train loss:0.0002409712347774894\n",
      "train loss:0.001773732196930956\n",
      "train loss:0.0018539010816331495\n",
      "train loss:0.005859588789717276\n",
      "train loss:0.02617761952183757\n",
      "train loss:0.0006226501132186606\n",
      "train loss:0.03820005876043556\n",
      "train loss:0.002785573725079443\n",
      "train loss:0.002291000511986934\n",
      "train loss:0.006749774843341665\n",
      "train loss:0.0012843047551899442\n",
      "train loss:0.0021956621343015986\n",
      "train loss:0.0021211464144404034\n",
      "train loss:0.002408458611781567\n",
      "train loss:0.004121455820386217\n",
      "train loss:0.00023705849881413394\n",
      "train loss:0.0054054992678091275\n",
      "train loss:0.0017029161601739371\n",
      "train loss:0.005882505321029101\n",
      "train loss:0.001019185695589233\n",
      "train loss:0.0016198763375940698\n",
      "train loss:0.002628136498438678\n",
      "train loss:0.000625169481481143\n",
      "train loss:0.006983796248351414\n",
      "train loss:0.0020643053025080054\n",
      "train loss:0.004817805334279194\n",
      "train loss:0.005342093235537654\n",
      "train loss:0.0013563872316750684\n",
      "train loss:0.005409624588952475\n",
      "train loss:0.019867346420158426\n",
      "train loss:0.0032794052477506347\n",
      "train loss:0.0028730647748681074\n",
      "train loss:0.000989175859224503\n",
      "train loss:0.0008959249030312979\n",
      "train loss:0.0006462247905216088\n",
      "train loss:0.002601952643102715\n",
      "train loss:0.004390615003546777\n",
      "train loss:0.04419269273674181\n",
      "train loss:0.012420425909332653\n",
      "train loss:0.004211656088256427\n",
      "train loss:0.0034603109621239646\n",
      "train loss:0.0027762683731415018\n",
      "train loss:0.003924273414202981\n",
      "train loss:0.0001385735020801214\n",
      "train loss:0.0070913345044976706\n",
      "train loss:0.013247791457826414\n",
      "train loss:0.00433089433724012\n",
      "train loss:0.0016508194519897113\n",
      "train loss:0.0011966740928107634\n",
      "train loss:0.003305243643723873\n",
      "train loss:0.0012725111290890847\n",
      "train loss:0.0019102447264950214\n",
      "train loss:0.005908130322984126\n",
      "train loss:0.0024822981086522486\n",
      "train loss:0.0015936582453881234\n",
      "train loss:0.019136734096173353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.008471436599324278\n",
      "train loss:0.010367000323069767\n",
      "train loss:0.009035141012086604\n",
      "train loss:0.006044650512248565\n",
      "train loss:0.005600926075508122\n",
      "train loss:0.006442927726429035\n",
      "train loss:0.002238956609431488\n",
      "train loss:0.0012914860189753895\n",
      "train loss:0.0033257860396214856\n",
      "train loss:0.0018400635790132886\n",
      "train loss:0.000711842079481312\n",
      "train loss:0.005799959631970386\n",
      "train loss:0.0035557856920539967\n",
      "train loss:0.0007225574135756298\n",
      "train loss:0.0037024087312731552\n",
      "train loss:0.0035869623006071893\n",
      "train loss:0.003105517005682225\n",
      "train loss:0.002566169194420859\n",
      "train loss:0.0036518162202511147\n",
      "train loss:0.01846094498439143\n",
      "train loss:0.0056337247592722715\n",
      "train loss:0.0013282499732165873\n",
      "train loss:0.004325195451682107\n",
      "train loss:0.031259073810651274\n",
      "train loss:0.004486352268911486\n",
      "train loss:0.0025861048312715617\n",
      "train loss:0.004759367940639719\n",
      "train loss:0.005349889435443915\n",
      "train loss:0.012785021672378152\n",
      "train loss:0.0009914559943799035\n",
      "train loss:0.0018215383524269708\n",
      "train loss:0.0042753445139635145\n",
      "train loss:0.00030065558946372717\n",
      "train loss:0.003127458420795795\n",
      "train loss:0.0027518094297291763\n",
      "train loss:0.003983854672522893\n",
      "train loss:0.005899338333247877\n",
      "train loss:0.0141839261636686\n",
      "train loss:0.0016575167610836045\n",
      "train loss:0.000947162741442886\n",
      "train loss:0.0025752115609879163\n",
      "=== epoch:14, train acc:0.998, test acc:0.989 ===\n",
      "train loss:0.0010455304189746996\n",
      "train loss:0.0009766534288239038\n",
      "train loss:0.00045028655589172544\n",
      "train loss:0.001396887402627267\n",
      "train loss:0.00015807192008314444\n",
      "train loss:0.0033737226040784574\n",
      "train loss:0.0027052083436515813\n",
      "train loss:0.00045469695126459665\n",
      "train loss:0.0003561980696023958\n",
      "train loss:0.002363443105552297\n",
      "train loss:0.0033802957474767226\n",
      "train loss:0.0018118050445681234\n",
      "train loss:0.0006367597348399342\n",
      "train loss:0.0006858110847235234\n",
      "train loss:0.0004016266642801645\n",
      "train loss:0.0018237523355433905\n",
      "train loss:0.0003144019684333467\n",
      "train loss:0.00014912039315829393\n",
      "train loss:0.004123430894281463\n",
      "train loss:0.0010796951632542997\n",
      "train loss:0.0032183049551106173\n",
      "train loss:0.00033517360023091057\n",
      "train loss:3.6972921035494215e-05\n",
      "train loss:0.001084793043368709\n",
      "train loss:0.000503348536113423\n",
      "train loss:0.017657357285768928\n",
      "train loss:0.0034405940576475013\n",
      "train loss:0.001643769196871905\n",
      "train loss:0.0004016241224840075\n",
      "train loss:0.0006889650431442831\n",
      "train loss:0.006525325319986846\n",
      "train loss:0.004725646492156362\n",
      "train loss:0.0008121464130170725\n",
      "train loss:0.000672710591334649\n",
      "train loss:0.004108653822406898\n",
      "train loss:0.0038065762784591163\n",
      "train loss:0.01018389306509365\n",
      "train loss:0.005012644937555802\n",
      "train loss:0.0007544984541781904\n",
      "train loss:0.0013747077822663773\n",
      "train loss:0.0072379162331334115\n",
      "train loss:0.0008632592824210579\n",
      "train loss:0.001974932798378465\n",
      "train loss:0.0035908113625569217\n",
      "train loss:0.003915825344473496\n",
      "train loss:0.0011523323416052966\n",
      "train loss:0.006801555294392517\n",
      "train loss:0.00861028200642344\n",
      "train loss:0.003698951490024868\n",
      "train loss:0.0014645921070310843\n",
      "train loss:0.008590981213802435\n",
      "train loss:0.003864523387582126\n",
      "train loss:0.0024744862093900137\n",
      "train loss:0.0033602523713823916\n",
      "train loss:0.0013155593171591746\n",
      "train loss:0.0019267477267333032\n",
      "train loss:0.0016584482249258933\n",
      "train loss:0.006756040335543938\n",
      "train loss:0.004316891519934277\n",
      "train loss:0.012152583309301447\n",
      "train loss:0.002138367621190356\n",
      "train loss:0.0007762692598231937\n",
      "train loss:0.0006038911423755479\n",
      "train loss:0.0005593550005365786\n",
      "train loss:0.0003097919490355142\n",
      "train loss:0.0029106018053360896\n",
      "train loss:0.0017479352547293328\n",
      "train loss:0.00410004488503801\n",
      "train loss:0.01057845679126522\n",
      "train loss:9.942891696183078e-05\n",
      "train loss:0.0058446494043267565\n",
      "train loss:0.0014339307299965115\n",
      "train loss:0.010350366182160438\n",
      "train loss:0.00035603989058023423\n",
      "train loss:0.0016575717938877113\n",
      "train loss:0.003504964307980084\n",
      "train loss:0.004029465221256667\n",
      "train loss:0.002605121717319332\n",
      "train loss:0.005021790340174058\n",
      "train loss:0.016420549221722286\n",
      "train loss:0.010205953247681148\n",
      "train loss:0.029800127034077355\n",
      "train loss:0.000148256382135186\n",
      "train loss:0.0019408473973268625\n",
      "train loss:0.0015818695197571072\n",
      "train loss:0.003455167453871445\n",
      "train loss:0.003051288540960008\n",
      "train loss:0.004197642291412916\n",
      "train loss:0.0010130937075766653\n",
      "train loss:0.007096626812127971\n",
      "train loss:0.008994414214203774\n",
      "train loss:0.0030929723385131253\n",
      "train loss:0.022264174450398834\n",
      "train loss:0.0003834325564404636\n",
      "train loss:0.0008971128340480909\n",
      "train loss:0.00026347401573832257\n",
      "train loss:0.0006712368588973949\n",
      "train loss:0.001768722177681256\n",
      "train loss:0.0017206509366040484\n",
      "train loss:0.004622127542434169\n",
      "train loss:0.003775633764679521\n",
      "train loss:0.020945748488441534\n",
      "train loss:0.0007023918419186804\n",
      "train loss:0.0008956020751168599\n",
      "train loss:0.0003388108309365212\n",
      "train loss:0.0027347317466210335\n",
      "train loss:0.003393476212067757\n",
      "train loss:0.001084768458859387\n",
      "train loss:0.003149093065924904\n",
      "train loss:0.0032530512617556023\n",
      "train loss:0.0016791169928137215\n",
      "train loss:0.005405485457882296\n",
      "train loss:0.0011069090762484256\n",
      "train loss:0.007062180255995413\n",
      "train loss:0.0007678376311087822\n",
      "train loss:0.0007838551593998017\n",
      "train loss:0.0007489330861106799\n",
      "train loss:0.007547604550021545\n",
      "train loss:0.00011796660400874782\n",
      "train loss:0.0025609562087131566\n",
      "train loss:0.005396439666722641\n",
      "train loss:0.005122711359819917\n",
      "train loss:3.9658459835944115e-05\n",
      "train loss:0.0012175567849650754\n",
      "train loss:0.0006825347768689208\n",
      "train loss:0.002240573844122679\n",
      "train loss:0.001251449644416244\n",
      "train loss:0.0010064231657509561\n",
      "train loss:0.0009124757115319746\n",
      "train loss:0.0007554512790770993\n",
      "train loss:0.0022981402785702343\n",
      "train loss:0.0015507532315501243\n",
      "train loss:0.005238435745289169\n",
      "train loss:0.00025189631206164556\n",
      "train loss:0.0006136491735891413\n",
      "train loss:0.0029968349428539263\n",
      "train loss:0.006697008315937749\n",
      "train loss:0.004636672351011352\n",
      "train loss:0.0007459847079685497\n",
      "train loss:0.0019817846738823673\n",
      "train loss:0.0015430016518969878\n",
      "train loss:0.000775776355866999\n",
      "train loss:0.0008956760121354262\n",
      "train loss:0.009744439474412064\n",
      "train loss:0.019541920436595656\n",
      "train loss:0.0026591141269503133\n",
      "train loss:0.0013295219836975513\n",
      "train loss:0.006503847432777221\n",
      "train loss:0.000703325220355926\n",
      "train loss:0.0007888113974756803\n",
      "train loss:0.006182416720892317\n",
      "train loss:0.0019514241341077165\n",
      "train loss:0.004583699628990914\n",
      "train loss:0.006361456585206736\n",
      "train loss:0.00037758822700183903\n",
      "train loss:0.0006573136920821878\n",
      "train loss:0.001648880709862755\n",
      "train loss:0.0026540762819959075\n",
      "train loss:0.004357989800669489\n",
      "train loss:6.39234974157721e-05\n",
      "train loss:0.003614669705757202\n",
      "train loss:0.0024241697564716565\n",
      "train loss:0.00376775956650316\n",
      "train loss:0.001323875506681469\n",
      "train loss:0.002262037042428534\n",
      "train loss:0.0026925649107628125\n",
      "train loss:0.002596678544959916\n",
      "train loss:0.003002294172356983\n",
      "train loss:0.053933810019370435\n",
      "train loss:0.0011731384795860675\n",
      "train loss:0.0011728622718769123\n",
      "train loss:0.011176884258929622\n",
      "train loss:0.0006518095514764858\n",
      "train loss:0.000505990105485679\n",
      "train loss:0.01835386799531135\n",
      "train loss:0.001325199810951804\n",
      "train loss:0.08231426072578542\n",
      "train loss:0.007436002982676295\n",
      "train loss:0.00477169995729071\n",
      "train loss:0.00010285054576804489\n",
      "train loss:0.0017965671960455606\n",
      "train loss:0.00979327486844513\n",
      "train loss:0.002109071452576817\n",
      "train loss:0.005348561703355792\n",
      "train loss:0.001141853827708194\n",
      "train loss:0.006091174771367134\n",
      "train loss:0.014697764100258044\n",
      "train loss:0.0014187892998276418\n",
      "train loss:0.002910685331033052\n",
      "train loss:0.001026566265327041\n",
      "train loss:0.007646950170404482\n",
      "train loss:0.010612743793490708\n",
      "train loss:0.002364754977023346\n",
      "train loss:0.0028337208955246072\n",
      "train loss:0.006930596118686184\n",
      "train loss:0.006846533295768662\n",
      "train loss:0.00022083176412157342\n",
      "train loss:0.0031711245440917742\n",
      "train loss:0.001899729064887413\n",
      "train loss:0.007453238334834355\n",
      "train loss:0.0007429999471866075\n",
      "train loss:0.00157652030564223\n",
      "train loss:0.00873592470766755\n",
      "train loss:0.015004148546653013\n",
      "train loss:0.0004987317427785343\n",
      "train loss:0.000491487828111719\n",
      "train loss:0.0017979874203205458\n",
      "train loss:0.001032412898078721\n",
      "train loss:0.002178569701214174\n",
      "train loss:0.0022030693445939924\n",
      "train loss:0.003270934585741034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.004710048576296693\n",
      "train loss:0.0008970238717325563\n",
      "train loss:0.002496383885987301\n",
      "train loss:0.00047506271404116257\n",
      "train loss:0.0014589461232204475\n",
      "train loss:0.007124792878055614\n",
      "train loss:0.005279989977670885\n",
      "train loss:0.0012823030601210846\n",
      "train loss:0.016683173890168913\n",
      "train loss:0.009938413446094219\n",
      "train loss:0.0026754692554526514\n",
      "train loss:0.0030311842294901375\n",
      "train loss:0.0019107644946359096\n",
      "train loss:0.020745259146394125\n",
      "train loss:0.002402212497505986\n",
      "train loss:0.003741932840728216\n",
      "train loss:0.005407302803568143\n",
      "train loss:0.001216806910221306\n",
      "train loss:0.00016879583397866365\n",
      "train loss:0.0056934450947314155\n",
      "train loss:0.0010427790545360402\n",
      "train loss:0.003947867787680424\n",
      "train loss:0.0012362387117250948\n",
      "train loss:0.008929614877310574\n",
      "train loss:0.0025402702606790086\n",
      "train loss:0.0012354034396444688\n",
      "train loss:0.0007958983554518075\n",
      "train loss:0.0028186603196296628\n",
      "train loss:0.0017663847610531061\n",
      "train loss:0.0047365812209731965\n",
      "train loss:0.0004238389652866099\n",
      "train loss:0.0017597306434696007\n",
      "train loss:0.013210454972878033\n",
      "train loss:0.0031117772176956038\n",
      "train loss:0.0002835873244166514\n",
      "train loss:0.0019774312259762477\n",
      "train loss:0.005572549061639105\n",
      "train loss:0.004106651115264895\n",
      "train loss:0.005043885586242355\n",
      "train loss:0.001755998778998193\n",
      "train loss:0.0061374634039155695\n",
      "train loss:0.002749271446944426\n",
      "train loss:0.0092692731351283\n",
      "train loss:0.0002785926901953102\n",
      "train loss:0.0019189270816169703\n",
      "train loss:0.0003820987540086969\n",
      "train loss:0.0008798640219458152\n",
      "train loss:0.0015344662659615555\n",
      "train loss:0.003964530128146226\n",
      "train loss:0.0004172110066779767\n",
      "train loss:0.0012470185608080778\n",
      "train loss:0.0012379665846155438\n",
      "train loss:0.0006100880047846584\n",
      "train loss:0.009066616897666512\n",
      "train loss:0.013449524494919231\n",
      "train loss:0.0028926046593420896\n",
      "train loss:0.0003574554272880729\n",
      "train loss:0.0022496891363469607\n",
      "train loss:0.0007457982129869729\n",
      "train loss:0.00029875051469682577\n",
      "train loss:0.0033270155172892683\n",
      "train loss:0.0016799337131480528\n",
      "train loss:0.001048331981204796\n",
      "train loss:0.0026465807204699306\n",
      "train loss:0.0016795442694117091\n",
      "train loss:0.013936751806053577\n",
      "train loss:0.00020817657569066793\n",
      "train loss:0.001855682022712745\n",
      "train loss:0.0026056361784719474\n",
      "train loss:0.004315770380158644\n",
      "train loss:0.0009762995027651836\n",
      "train loss:0.0015824210014627558\n",
      "train loss:0.006324128352031504\n",
      "train loss:0.004620531963828932\n",
      "train loss:0.0038177781666057075\n",
      "train loss:0.002176062292695015\n",
      "train loss:0.0004706280680995133\n",
      "train loss:0.0013285882096286318\n",
      "train loss:0.0027088882149024223\n",
      "train loss:7.308138091426453e-05\n",
      "train loss:0.026337739914050716\n",
      "train loss:0.001838519127836042\n",
      "train loss:0.0007691334167648643\n",
      "train loss:0.0083933004595616\n",
      "train loss:0.00170573272889935\n",
      "train loss:0.0025080493214694947\n",
      "train loss:0.007315039734228554\n",
      "train loss:0.0011771989365854348\n",
      "train loss:0.0026677822193386424\n",
      "train loss:0.0034873926203544876\n",
      "train loss:0.0007721884200191673\n",
      "train loss:0.00413170946609001\n",
      "train loss:0.005626820507447419\n",
      "train loss:0.01098752232967264\n",
      "train loss:0.0016348940906165616\n",
      "train loss:0.0020274504807556563\n",
      "train loss:0.0013625266443485142\n",
      "train loss:0.0014571465652780005\n",
      "train loss:0.00045258486795392663\n",
      "train loss:0.000359507895848332\n",
      "train loss:0.0040812659699247245\n",
      "train loss:0.005473607614709049\n",
      "train loss:0.001570813980254376\n",
      "train loss:0.004227724949076629\n",
      "train loss:0.0003947525652474091\n",
      "train loss:0.0013433337112526813\n",
      "train loss:0.012050693265994368\n",
      "train loss:0.0011596007639910401\n",
      "train loss:0.002935869276938382\n",
      "train loss:0.004531403478411709\n",
      "train loss:0.0015255741668974154\n",
      "train loss:0.003059825533505896\n",
      "train loss:0.002825772546735342\n",
      "train loss:0.0037252756432835116\n",
      "train loss:0.0073200510111745476\n",
      "train loss:0.0027707727356450582\n",
      "train loss:0.0001086415374696328\n",
      "train loss:0.0018480351541974094\n",
      "train loss:0.0008187743722775781\n",
      "train loss:0.0025880661364873097\n",
      "train loss:0.001246385911980502\n",
      "train loss:0.0013282023881217397\n",
      "train loss:0.001042530030539675\n",
      "train loss:0.0002404224828831471\n",
      "train loss:0.0003940461800155979\n",
      "train loss:0.002554419449052917\n",
      "train loss:0.0047431436830813615\n",
      "train loss:0.00017609631134189195\n",
      "train loss:0.0006269042531174356\n",
      "train loss:0.004898836789745914\n",
      "train loss:0.0035480385278570292\n",
      "train loss:0.003129734168095867\n",
      "train loss:0.002908659771495861\n",
      "train loss:0.0025047826276803366\n",
      "train loss:0.0011074540442985088\n",
      "train loss:0.003163321350971704\n",
      "train loss:0.0006531875370195812\n",
      "train loss:0.005073840005089877\n",
      "train loss:0.000247255115227867\n",
      "train loss:0.00026801026201660546\n",
      "train loss:0.0004129167800124362\n",
      "train loss:0.0013961565008545561\n",
      "train loss:9.929976927969734e-05\n",
      "train loss:0.006083041444889363\n",
      "train loss:0.002631390171667776\n",
      "train loss:0.002052619925335536\n",
      "train loss:0.0004521479647614837\n",
      "train loss:0.0015784834704757789\n",
      "train loss:0.0006829696262487688\n",
      "train loss:0.0012639162040069727\n",
      "train loss:0.0007790211132036981\n",
      "train loss:0.007086503915277725\n",
      "train loss:0.0012569665077713631\n",
      "train loss:0.0016217950200857567\n",
      "train loss:0.00016613542301266787\n",
      "train loss:0.004614461703044098\n",
      "train loss:0.0031993429471517854\n",
      "train loss:0.0009360866914650545\n",
      "train loss:0.00033006357386581453\n",
      "train loss:0.003048011317106194\n",
      "train loss:0.0005951041586663444\n",
      "train loss:0.0003974168800287769\n",
      "train loss:0.0021365676245697117\n",
      "train loss:0.0025905656888608308\n",
      "train loss:0.002434167061084278\n",
      "train loss:0.007067714618416896\n",
      "train loss:0.006022406559251693\n",
      "train loss:0.00010472199850453356\n",
      "train loss:0.0010523929154047935\n",
      "train loss:0.002399564765073029\n",
      "train loss:0.00013698411991276368\n",
      "train loss:4.13131016675168e-05\n",
      "train loss:0.0005432373176877594\n",
      "train loss:0.0007090125019187516\n",
      "train loss:0.004955160878935659\n",
      "train loss:0.0009798355343753774\n",
      "train loss:0.006617014927642775\n",
      "train loss:0.00024543828958435867\n",
      "train loss:0.0004781449178332702\n",
      "train loss:0.002070674006210726\n",
      "train loss:0.0018522982659584493\n",
      "train loss:0.0034279968546285723\n",
      "train loss:0.006655135249772044\n",
      "train loss:0.004355218519528688\n",
      "train loss:0.0006311789362396252\n",
      "train loss:0.003924725971916795\n",
      "train loss:0.000999100615285568\n",
      "train loss:0.001803686581524571\n",
      "train loss:0.003789774319340157\n",
      "train loss:0.0012633115792190742\n",
      "train loss:0.0004079908692602772\n",
      "train loss:0.004935397525220263\n",
      "train loss:0.0015977408069120871\n",
      "train loss:0.0018575504272871583\n",
      "train loss:0.0005433753776764098\n",
      "train loss:0.002528807363611565\n",
      "train loss:0.004414841451171144\n",
      "train loss:0.010175745229767696\n",
      "train loss:0.0012572043477531221\n",
      "train loss:0.0017991335273968478\n",
      "train loss:0.0005446804785268073\n",
      "train loss:0.0006437641772360049\n",
      "train loss:0.001522635377794886\n",
      "train loss:0.00012674273094172666\n",
      "train loss:0.000956568148386733\n",
      "train loss:0.0005653317161860168\n",
      "train loss:0.004287517194366005\n",
      "train loss:0.0007556021035969547\n",
      "train loss:5.876412969517446e-05\n",
      "train loss:0.0020120373129931773\n",
      "train loss:0.0015052524956970128\n",
      "train loss:0.0005035116187303789\n",
      "train loss:0.0019172370487715246\n",
      "train loss:0.002295678401810636\n",
      "train loss:0.0007161238510750879\n",
      "train loss:0.0038673950297060957\n",
      "train loss:0.006290661264588855\n",
      "train loss:0.0010332441422696522\n",
      "train loss:0.0005051350511494995\n",
      "train loss:0.0033397306960303323\n",
      "train loss:0.0026365837050607915\n",
      "train loss:0.0001500489313651755\n",
      "train loss:0.0039832172558275005\n",
      "train loss:0.004875929852733065\n",
      "train loss:0.0014247726126267507\n",
      "train loss:0.0015407739541666925\n",
      "train loss:0.0035826000788194826\n",
      "train loss:0.0014385175882144885\n",
      "train loss:0.009351323355051363\n",
      "train loss:0.003961115788749636\n",
      "train loss:0.0015658435902342642\n",
      "train loss:0.0005501586316709335\n",
      "train loss:0.0014967859645520925\n",
      "train loss:0.001905665129354634\n",
      "train loss:0.0031228940878176466\n",
      "train loss:0.00189199477887013\n",
      "train loss:4.359081693823891e-05\n",
      "train loss:0.002730405092713385\n",
      "train loss:0.0007873685066135965\n",
      "train loss:0.0012042559909763125\n",
      "train loss:0.00143583002989884\n",
      "train loss:0.002948302494819966\n",
      "train loss:0.005604983178540788\n",
      "train loss:0.0014085626416622766\n",
      "train loss:0.0001907410514210861\n",
      "train loss:0.0030453857725805545\n",
      "train loss:0.008475888271257236\n",
      "train loss:0.0007900203731639065\n",
      "train loss:0.0004597263736274974\n",
      "train loss:0.005705822286423067\n",
      "train loss:0.010271777413548044\n",
      "train loss:0.0015822657799742482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.005582518798087712\n",
      "train loss:5.429329641484851e-05\n",
      "train loss:0.005065942090216692\n",
      "train loss:0.0005096703117133972\n",
      "train loss:0.003315695961923397\n",
      "train loss:0.0003890833757209958\n",
      "train loss:0.006833873302768514\n",
      "train loss:0.0002527958377337516\n",
      "train loss:0.0005656359805024682\n",
      "train loss:0.0009605001815443119\n",
      "train loss:0.001464704977543836\n",
      "train loss:0.000561767834254374\n",
      "train loss:0.0007237284951202542\n",
      "train loss:0.0009852228892659354\n",
      "train loss:0.0016901345952280732\n",
      "train loss:0.0005202727172714623\n",
      "train loss:0.0014049377145807633\n",
      "train loss:0.004959993264951006\n",
      "train loss:0.005608063747596488\n",
      "train loss:0.004240018071945208\n",
      "train loss:0.0038050039868231904\n",
      "train loss:0.00020788985116917586\n",
      "train loss:0.003692273224863974\n",
      "train loss:0.004294677173892916\n",
      "train loss:0.16261395672933407\n",
      "train loss:0.0022463470413196223\n",
      "train loss:0.0005991207224794438\n",
      "train loss:0.012469827692807038\n",
      "train loss:0.0014286138691871613\n",
      "train loss:0.0003310708437574399\n",
      "train loss:0.00019348916129112757\n",
      "train loss:0.0005750323835780201\n",
      "train loss:0.0007852562591327759\n",
      "train loss:0.0001604920055854315\n",
      "train loss:0.0012272358631117142\n",
      "train loss:0.0013882275338093926\n",
      "train loss:4.626913185242958e-05\n",
      "train loss:0.002093231414020995\n",
      "train loss:0.0010897980221628854\n",
      "train loss:0.001291916671892312\n",
      "train loss:0.00032749700077184184\n",
      "train loss:0.0014922098228373382\n",
      "train loss:0.001173840266970493\n",
      "train loss:0.007383603913599613\n",
      "train loss:0.002201924030004285\n",
      "train loss:0.0012599670661199572\n",
      "train loss:0.0025499203261367316\n",
      "train loss:0.0020012500575393023\n",
      "train loss:0.0010170571150405008\n",
      "train loss:0.004305289842057535\n",
      "train loss:0.0004820169100797228\n",
      "train loss:7.8254963591961e-05\n",
      "train loss:0.00033729471771968807\n",
      "train loss:0.0009276612047443661\n",
      "train loss:0.0008478655440903211\n",
      "train loss:0.003971164398472812\n",
      "train loss:0.004982689191329596\n",
      "train loss:0.0012009540090142682\n",
      "train loss:0.0020611263442119948\n",
      "train loss:0.0013856673192066712\n",
      "train loss:9.022132682046753e-05\n",
      "train loss:0.002235230993427302\n",
      "train loss:0.028858641517125\n",
      "train loss:0.0016127737418794436\n",
      "train loss:0.0028624754962885478\n",
      "train loss:0.00015285562720307303\n",
      "train loss:0.00039607693423629583\n",
      "train loss:0.001994186086800008\n",
      "train loss:0.0024098594840190925\n",
      "train loss:0.0056314877059216685\n",
      "train loss:0.0018894184796114542\n",
      "train loss:0.0001003843203382278\n",
      "train loss:0.0003711881143167149\n",
      "train loss:0.0023108019137151234\n",
      "train loss:0.0003201229182024408\n",
      "train loss:0.00011849852803450146\n",
      "train loss:0.0004701147642057123\n",
      "train loss:0.0010272425277548186\n",
      "train loss:0.0036658349659618662\n",
      "train loss:9.943485634067419e-05\n",
      "train loss:0.004852663340306982\n",
      "train loss:0.007729214142535484\n",
      "train loss:0.0011153815035254428\n",
      "train loss:0.0004634001511085931\n",
      "train loss:0.0008484742383049742\n",
      "train loss:0.00172760608081981\n",
      "train loss:0.0009097204794035832\n",
      "train loss:0.0025799403429279015\n",
      "train loss:0.0012263692489704085\n",
      "train loss:0.028319351289930416\n",
      "train loss:0.0002238691877037824\n",
      "train loss:0.002586426417665633\n",
      "train loss:0.0006944466986399949\n",
      "train loss:0.0027865004397476974\n",
      "train loss:0.000839611295533078\n",
      "train loss:0.0007990300888414699\n",
      "train loss:0.0010987408815198735\n",
      "train loss:0.001088870095188495\n",
      "train loss:0.004611666930987787\n",
      "train loss:0.0014044543710053942\n",
      "train loss:0.0005468130978745256\n",
      "train loss:0.0026916914966030408\n",
      "train loss:0.012230518754409727\n",
      "train loss:0.033928337393118624\n",
      "train loss:0.006763843971024331\n",
      "train loss:0.0005196087048599216\n",
      "train loss:0.0010566483819631244\n",
      "train loss:0.001036668511340201\n",
      "train loss:0.006434177241001673\n",
      "train loss:0.014212395295569757\n",
      "train loss:0.0004430610453159506\n",
      "train loss:0.0008938137556393114\n",
      "train loss:0.0025505880513377634\n",
      "train loss:0.002071015313423198\n",
      "train loss:0.00584440222188583\n",
      "train loss:0.004049199159448653\n",
      "train loss:0.000469517458085622\n",
      "train loss:0.0021739704881386453\n",
      "train loss:9.09643334636273e-05\n",
      "train loss:0.0053590384091608415\n",
      "train loss:0.0030973213816132945\n",
      "train loss:0.0005581862081531817\n",
      "train loss:0.0019805343542188066\n",
      "train loss:0.001532558202163873\n",
      "train loss:0.001565139349712761\n",
      "train loss:0.0021634063816842373\n",
      "train loss:0.01696468454349557\n",
      "train loss:0.0012189657895775327\n",
      "train loss:0.0005143946833616219\n",
      "train loss:0.00018019168227661952\n",
      "train loss:0.0006524249892861877\n",
      "train loss:0.00035354527566381564\n",
      "train loss:0.0011123707793179636\n",
      "train loss:0.0017098135271183855\n",
      "train loss:0.0021089529367443916\n",
      "train loss:0.00043806490654687674\n",
      "train loss:0.000965743223642784\n",
      "=== epoch:15, train acc:0.997, test acc:0.987 ===\n",
      "train loss:0.000429789287936765\n",
      "train loss:0.0073542376826737435\n",
      "train loss:0.006130126442481199\n",
      "train loss:0.0005253823561050739\n",
      "train loss:0.0003087334729155038\n",
      "train loss:0.014169317955858702\n",
      "train loss:0.00028617951921923674\n",
      "train loss:0.0012199255591614367\n",
      "train loss:0.0038321284608034733\n",
      "train loss:0.0026357349314352833\n",
      "train loss:0.028292779859010412\n",
      "train loss:0.019800518636112388\n",
      "train loss:0.002483667224105898\n",
      "train loss:0.0006757524912049509\n",
      "train loss:0.0004026344869894376\n",
      "train loss:0.0009149026529489075\n",
      "train loss:0.0008754767770262634\n",
      "train loss:0.004003244782484165\n",
      "train loss:0.0019947305282323117\n",
      "train loss:0.0028674805797125474\n",
      "train loss:0.0015522595884125236\n",
      "train loss:0.002616432707575882\n",
      "train loss:0.024263375838183564\n",
      "train loss:0.008282495456670865\n",
      "train loss:0.003061753161122208\n",
      "train loss:0.0017829102007617787\n",
      "train loss:0.0010281493607491692\n",
      "train loss:0.007110333424809593\n",
      "train loss:0.005600608984391169\n",
      "train loss:0.006932867639040208\n",
      "train loss:0.0013524454944488385\n",
      "train loss:0.001043612497821499\n",
      "train loss:0.0008899601648510606\n",
      "train loss:0.005698137081274833\n",
      "train loss:0.002196006892834441\n",
      "train loss:0.0025490167518041327\n",
      "train loss:0.0003274512101268446\n",
      "train loss:0.0067790780212921295\n",
      "train loss:0.009146907516643238\n",
      "train loss:0.0027490457771161864\n",
      "train loss:0.0007904509836434034\n",
      "train loss:0.029291216399610128\n",
      "train loss:0.0024686809352998298\n",
      "train loss:0.0038183469161783383\n",
      "train loss:0.0012222586763506766\n",
      "train loss:0.0008163709038001787\n",
      "train loss:0.003584067275958896\n",
      "train loss:0.0025353381856326566\n",
      "train loss:0.0015104044425000972\n",
      "train loss:0.0004811147035870462\n",
      "train loss:0.0009866670721113183\n",
      "train loss:0.0005991960994195145\n",
      "train loss:0.0006147275057017037\n",
      "train loss:0.0027145871130068583\n",
      "train loss:0.0004887238096414866\n",
      "train loss:0.0020418870842166745\n",
      "train loss:0.000733918117601965\n",
      "train loss:0.0005667087660792083\n",
      "train loss:0.03236503173913956\n",
      "train loss:0.002405273820161067\n",
      "train loss:0.0024507547732467937\n",
      "train loss:0.0033678036095500853\n",
      "train loss:0.0010395100218772294\n",
      "train loss:0.0020542840413294923\n",
      "train loss:0.0013757661668204108\n",
      "train loss:0.003662214139867482\n",
      "train loss:0.0022260966982451127\n",
      "train loss:0.0041533490769853\n",
      "train loss:0.00029534631197053257\n",
      "train loss:4.214419379318653e-05\n",
      "train loss:0.002548373196632116\n",
      "train loss:0.0015212815360835224\n",
      "train loss:0.002879820695294037\n",
      "train loss:0.0002779379682567969\n",
      "train loss:0.0013710606154049647\n",
      "train loss:0.004269667694142747\n",
      "train loss:0.0008014195488536657\n",
      "train loss:0.00253438186552224\n",
      "train loss:0.0027841926520806772\n",
      "train loss:0.007864253612054627\n",
      "train loss:0.00028015083637233256\n",
      "train loss:0.004190268935910202\n",
      "train loss:0.003792979558231806\n",
      "train loss:0.00021726315947271112\n",
      "train loss:0.00013056742231235417\n",
      "train loss:0.004364573611595038\n",
      "train loss:0.005684362020906446\n",
      "train loss:0.002884761095659248\n",
      "train loss:0.0051933319476297965\n",
      "train loss:0.00168607620897748\n",
      "train loss:0.0004689468082628502\n",
      "train loss:0.0022352452856558513\n",
      "train loss:0.004638435420941651\n",
      "train loss:0.002510362082831494\n",
      "train loss:0.00018989916988864912\n",
      "train loss:0.0008570518621263874\n",
      "train loss:0.0036953798014948917\n",
      "train loss:0.0017515145796649146\n",
      "train loss:0.021425413379080945\n",
      "train loss:0.002228804658379676\n",
      "train loss:0.0003709008801026945\n",
      "train loss:0.0006111614410875725\n",
      "train loss:0.0010091609612472175\n",
      "train loss:0.0019303207475368448\n",
      "train loss:0.001341782395384167\n",
      "train loss:0.00021296679580077592\n",
      "train loss:0.016386867740705944\n",
      "train loss:0.0015345764321096805\n",
      "train loss:0.002974035872652179\n",
      "train loss:0.007867235746655508\n",
      "train loss:0.0025981906655167726\n",
      "train loss:0.0004544825452399778\n",
      "train loss:0.0012131750409958256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.014559786663660944\n",
      "train loss:0.0005281858145543947\n",
      "train loss:0.0004895479368001143\n",
      "train loss:0.0028914032788615422\n",
      "train loss:0.0024233509374558745\n",
      "train loss:0.0018182303354996369\n",
      "train loss:0.008685708186960017\n",
      "train loss:0.002741962886510135\n",
      "train loss:0.004441354652423834\n",
      "train loss:0.0005973387246746037\n",
      "train loss:0.0006420827808988934\n",
      "train loss:0.0038883017693707635\n",
      "train loss:0.014817709417136147\n",
      "train loss:0.001649590755575002\n",
      "train loss:0.001924524601117704\n",
      "train loss:0.0010688041156545719\n",
      "train loss:0.0007964431599496576\n",
      "train loss:0.0003884005036283405\n",
      "train loss:0.00454096036752295\n",
      "train loss:0.000718818346554769\n",
      "train loss:0.002044663786521678\n",
      "train loss:0.0038044504043856613\n",
      "train loss:0.005288148121588604\n",
      "train loss:0.0005022731752970183\n",
      "train loss:0.001516237374255261\n",
      "train loss:0.002029465865891093\n",
      "train loss:0.008097643478908293\n",
      "train loss:0.0002418754894805093\n",
      "train loss:0.0028408022946604665\n",
      "train loss:0.000997506941532873\n",
      "train loss:0.0010212953507441884\n",
      "train loss:0.0065978342656754715\n",
      "train loss:0.004873846299927396\n",
      "train loss:0.000995177212282823\n",
      "train loss:0.002258472470306419\n",
      "train loss:0.0031285699043209275\n",
      "train loss:0.0014448270304515537\n",
      "train loss:0.0009088296769220782\n",
      "train loss:0.00010065055868344534\n",
      "train loss:0.0016686456911703049\n",
      "train loss:0.0019305623971830885\n",
      "train loss:0.0011264337409575957\n",
      "train loss:0.003244288914454647\n",
      "train loss:0.002954922248045132\n",
      "train loss:0.004045609284157369\n",
      "train loss:0.0017802689725303672\n",
      "train loss:0.0013733490375095754\n",
      "train loss:0.004747433001867866\n",
      "train loss:0.0013707035988627424\n",
      "train loss:0.00072840533517541\n",
      "train loss:0.0005542926133373705\n",
      "train loss:0.012910911847601021\n",
      "train loss:0.0017162261283474364\n",
      "train loss:0.000294101961114959\n",
      "train loss:0.0025165794754697275\n",
      "train loss:0.0054195435530667805\n",
      "train loss:0.0006896346607633769\n",
      "train loss:0.0024700432750040273\n",
      "train loss:0.0005822392088959177\n",
      "train loss:0.0005596348608862997\n",
      "train loss:0.0019195383067865065\n",
      "train loss:0.0016311589945344745\n",
      "train loss:0.0023048787975352837\n",
      "train loss:0.0002659180676426774\n",
      "train loss:0.00017779725977648506\n",
      "train loss:0.005444757765354432\n",
      "train loss:0.0008277169732755584\n",
      "train loss:0.0007732779127417767\n",
      "train loss:0.0011944527204015697\n",
      "train loss:0.0032963684240470757\n",
      "train loss:0.007303372068868591\n",
      "train loss:0.0011619405370507342\n",
      "train loss:0.0011147477307755613\n",
      "train loss:0.00045177242455068516\n",
      "train loss:0.0003802657900376768\n",
      "train loss:0.005616152355623386\n",
      "train loss:0.0027807698851492002\n",
      "train loss:0.00843975444717354\n",
      "train loss:0.005134037003589334\n",
      "train loss:0.00015278368603119407\n",
      "train loss:0.002003854134822342\n",
      "train loss:0.002059619844021225\n",
      "train loss:0.0007865076375251871\n",
      "train loss:0.0005559306607330468\n",
      "train loss:0.001366813227706718\n",
      "train loss:0.0029310727544582444\n",
      "train loss:0.0038346851531738746\n",
      "train loss:0.0016836839677883946\n",
      "train loss:0.00047321144984214863\n",
      "train loss:0.005830480746927407\n",
      "train loss:0.0015299281223762192\n",
      "train loss:0.0038078614241285292\n",
      "train loss:0.0011435304309818548\n",
      "train loss:0.0005461408376968849\n",
      "train loss:0.006633355593477715\n",
      "train loss:0.003032212603515303\n",
      "train loss:0.006224356788050866\n",
      "train loss:0.0015057486405856842\n",
      "train loss:0.00046275676544053126\n",
      "train loss:0.0005314296331204004\n",
      "train loss:0.002421582525687902\n",
      "train loss:0.002729407146963205\n",
      "train loss:0.0018266152456423144\n",
      "train loss:0.0015768829184674745\n",
      "train loss:0.0031879449902849927\n",
      "train loss:0.008326143455223138\n",
      "train loss:0.0025140885511747437\n",
      "train loss:0.0002973802561186353\n",
      "train loss:0.0038465249785472385\n",
      "train loss:0.004274088541002942\n",
      "train loss:0.00759723726226637\n",
      "train loss:0.005484967804908478\n",
      "train loss:0.00045033364185960727\n",
      "train loss:0.014130641165964688\n",
      "train loss:0.00037444233397340585\n",
      "train loss:0.00030404036017860216\n",
      "train loss:0.006970571945957248\n",
      "train loss:0.00582149589061314\n",
      "train loss:0.06992848450785258\n",
      "train loss:0.00046057926846317717\n",
      "train loss:0.007168814503538322\n",
      "train loss:0.0011873956958357397\n",
      "train loss:0.0070109180030693795\n",
      "train loss:0.0024314783319516974\n",
      "train loss:0.0012171748202202386\n",
      "train loss:0.012391157332023488\n",
      "train loss:0.004076829436277175\n",
      "train loss:0.0025065898764859164\n",
      "train loss:0.001421728080093738\n",
      "train loss:0.003998015968090061\n",
      "train loss:0.001878079724692995\n",
      "train loss:0.0029925580457322147\n",
      "train loss:0.0038810451824928534\n",
      "train loss:0.0011929816074855613\n",
      "train loss:0.0067603043221675105\n",
      "train loss:0.002331914692787625\n",
      "train loss:0.0015739492618702155\n",
      "train loss:0.007038706279871962\n",
      "train loss:0.0015796690357827111\n",
      "train loss:0.003543817333270969\n",
      "train loss:0.0011490376135761817\n",
      "train loss:0.00030967832328707964\n",
      "train loss:0.003408778021675784\n",
      "train loss:0.0037704145159733167\n",
      "train loss:0.00017203661228953443\n",
      "train loss:0.005443764374347038\n",
      "train loss:0.00298573066125212\n",
      "train loss:0.0072938492934105405\n",
      "train loss:0.0035935820077473953\n",
      "train loss:0.0013348698785850676\n",
      "train loss:0.0037766646003356574\n",
      "train loss:0.004488609289814038\n",
      "train loss:0.00213598349158661\n",
      "train loss:0.0007118868064586915\n",
      "train loss:0.0009601805126217595\n",
      "train loss:0.004830537584348121\n",
      "train loss:0.002204459550999457\n",
      "train loss:0.013199577579521893\n",
      "train loss:0.003489501096182866\n",
      "train loss:0.003527546569342702\n",
      "train loss:0.0014887851207961477\n",
      "train loss:0.0041234824451096596\n",
      "train loss:0.011572116352075484\n",
      "train loss:0.003082507730274646\n",
      "train loss:0.0006683111831236231\n",
      "train loss:0.0013410781707609645\n",
      "train loss:0.00034693684595236453\n",
      "train loss:0.0016798223697501518\n",
      "train loss:0.0017525828564033245\n",
      "train loss:0.02346681316633168\n",
      "train loss:0.007964785294402574\n",
      "train loss:0.0027223898415546905\n",
      "train loss:0.001132697138384342\n",
      "train loss:0.003694883177984795\n",
      "train loss:0.0026494274607615277\n",
      "train loss:0.0002979749821861805\n",
      "train loss:0.0010243631705359833\n",
      "train loss:0.002099229736384448\n",
      "train loss:0.001280265764950975\n",
      "train loss:0.0013722777342979834\n",
      "train loss:0.002248178400592275\n",
      "train loss:0.00012834961310062023\n",
      "train loss:0.00020036602582509586\n",
      "train loss:0.0015117335255283469\n",
      "train loss:0.0011058712488576814\n",
      "train loss:0.004070642055422456\n",
      "train loss:0.0007609386819507514\n",
      "train loss:0.000611397431604625\n",
      "train loss:0.007850770336901663\n",
      "train loss:0.022908052259853675\n",
      "train loss:0.0029464288767259406\n",
      "train loss:0.002375799664197653\n",
      "train loss:0.0062563630688689705\n",
      "train loss:0.0006919195139628131\n",
      "train loss:0.0005852696969983928\n",
      "train loss:0.00010287029306184143\n",
      "train loss:0.006512521578803545\n",
      "train loss:0.0032757762071920896\n",
      "train loss:0.0002639326419253449\n",
      "train loss:0.00037923355542814665\n",
      "train loss:0.002445411951029687\n",
      "train loss:0.0040916065609329835\n",
      "train loss:0.0004102658800245852\n",
      "train loss:0.0004904621629791623\n",
      "train loss:0.0015351125923619136\n",
      "train loss:0.0067232090500380145\n",
      "train loss:0.006162331014344926\n",
      "train loss:0.0017093002094596588\n",
      "train loss:0.003750704233430891\n",
      "train loss:0.0026443910095513513\n",
      "train loss:0.0011511973013302737\n",
      "train loss:0.0005418858761036984\n",
      "train loss:0.002754368401866095\n",
      "train loss:0.004093000910464591\n",
      "train loss:0.009007704923411151\n",
      "train loss:0.002157151241510751\n",
      "train loss:0.007033893585971952\n",
      "train loss:0.01240018041027732\n",
      "train loss:0.005267377344459174\n",
      "train loss:0.0026554767613452216\n",
      "train loss:0.0040684278991314285\n",
      "train loss:0.0030575955444650128\n",
      "train loss:0.004257950152744797\n",
      "train loss:0.060866650518233006\n",
      "train loss:0.0004029022725846309\n",
      "train loss:0.001027993961916528\n",
      "train loss:0.0063132700464608\n",
      "train loss:0.004682587659956994\n",
      "train loss:0.020694100185622333\n",
      "train loss:0.002555874573221474\n",
      "train loss:0.0036890473490264297\n",
      "train loss:0.007126762718697035\n",
      "train loss:0.003802406488060448\n",
      "train loss:0.003274113363192102\n",
      "train loss:0.01593967171861341\n",
      "train loss:0.032737480190089666\n",
      "train loss:0.0003515715694791622\n",
      "train loss:0.0034615611136788633\n",
      "train loss:0.012177742743782624\n",
      "train loss:0.006538895050065002\n",
      "train loss:0.004518817375479885\n",
      "train loss:0.0030169313721694646\n",
      "train loss:6.645420409132104e-05\n",
      "train loss:0.0020245500737821644\n",
      "train loss:0.0010330182200936034\n",
      "train loss:0.005460407695608627\n",
      "train loss:0.006317172035924834\n",
      "train loss:0.002800382275077119\n",
      "train loss:0.0039514314349548725\n",
      "train loss:0.003239134877100113\n",
      "train loss:0.0022441220041322226\n",
      "train loss:0.0059782110255278695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.003552199053310679\n",
      "train loss:0.0027140129069069103\n",
      "train loss:0.0008358670744195397\n",
      "train loss:0.0001639754908962855\n",
      "train loss:0.009285756504588894\n",
      "train loss:0.010240850472743687\n",
      "train loss:0.00993392144270967\n",
      "train loss:0.001120493837638643\n",
      "train loss:0.0010204194253381456\n",
      "train loss:0.0010198106881608004\n",
      "train loss:0.005576465418917416\n",
      "train loss:0.017751746929974117\n",
      "train loss:0.0010031728939686147\n",
      "train loss:0.0005174592618275311\n",
      "train loss:0.000529482469749817\n",
      "train loss:0.0030754629870285155\n",
      "train loss:0.0001105816909195349\n",
      "train loss:0.0008897478299691414\n",
      "train loss:0.004664214039309051\n",
      "train loss:0.005156230282356675\n",
      "train loss:0.0025167262798604663\n",
      "train loss:0.0022756256571899135\n",
      "train loss:0.00030262528303426134\n",
      "train loss:0.0009348115176835796\n",
      "train loss:0.000354499775284525\n",
      "train loss:0.0016155402879280865\n",
      "train loss:0.0032037546224891957\n",
      "train loss:0.0007532232428690673\n",
      "train loss:0.006509519874476629\n",
      "train loss:0.0044541076770268214\n",
      "train loss:0.0003414164472831489\n",
      "train loss:0.0030043917244822944\n",
      "train loss:0.0016324917975661\n",
      "train loss:0.0024249790343965224\n",
      "train loss:0.00038901820976713057\n",
      "train loss:0.01971476589793815\n",
      "train loss:0.0038071649124699473\n",
      "train loss:0.0009405195896079417\n",
      "train loss:0.004452297518530845\n",
      "train loss:0.0003761375560160853\n",
      "train loss:0.0020031143911263542\n",
      "train loss:0.0032510524186316323\n",
      "train loss:0.002984448175245495\n",
      "train loss:0.0005209342258653483\n",
      "train loss:0.0013639612268408435\n",
      "train loss:0.004879443613522457\n",
      "train loss:0.0014695425944654785\n",
      "train loss:0.002388040298879398\n",
      "train loss:0.006642395062499167\n",
      "train loss:0.005366647852316768\n",
      "train loss:0.03167857379357585\n",
      "train loss:0.0033736592701203837\n",
      "train loss:0.00014487832973669076\n",
      "train loss:0.0005118651758050108\n",
      "train loss:0.0004518039900146421\n",
      "train loss:0.0015089172605604858\n",
      "train loss:0.0003399280071150692\n",
      "train loss:0.0009318820735727376\n",
      "train loss:0.0012427421509771587\n",
      "train loss:0.0035057069559368257\n",
      "train loss:0.0005618880020469076\n",
      "train loss:0.000661476003782304\n",
      "train loss:0.0002365330652083698\n",
      "train loss:0.0005209814112149556\n",
      "train loss:0.0007212314684415546\n",
      "train loss:0.0013879593644419035\n",
      "train loss:0.005340268048721224\n",
      "train loss:3.21463585935033e-05\n",
      "train loss:0.0003996669309012483\n",
      "train loss:0.0027064523065370187\n",
      "train loss:0.005817842571360352\n",
      "train loss:0.0008554123282724761\n",
      "train loss:0.0018171136853690722\n",
      "train loss:0.001556432043241393\n",
      "train loss:0.0003439087973191888\n",
      "train loss:0.0011506062567535641\n",
      "train loss:0.0027950004694801806\n",
      "train loss:0.0018355505413875884\n",
      "train loss:0.00407852251312352\n",
      "train loss:0.003809848520801787\n",
      "train loss:0.0002764405088290014\n",
      "train loss:0.001840098183420677\n",
      "train loss:0.00048100154229943723\n",
      "train loss:0.00042465774367803316\n",
      "train loss:5.43765779273798e-05\n",
      "train loss:0.002750286401412788\n",
      "train loss:0.0008866488348301769\n",
      "train loss:0.0011347325472583368\n",
      "train loss:0.0038060885501631225\n",
      "train loss:0.005778480558071254\n",
      "train loss:0.0010208091972426477\n",
      "train loss:0.04499802194040555\n",
      "train loss:0.00012782213915070635\n",
      "train loss:0.018298886435172516\n",
      "train loss:0.00170090691006225\n",
      "train loss:0.003040708150366901\n",
      "train loss:0.0004348669190407434\n",
      "train loss:0.0011735022512656531\n",
      "train loss:0.000262561066649853\n",
      "train loss:0.011176715803170301\n",
      "train loss:0.006435834320394834\n",
      "train loss:0.039188882650795606\n",
      "train loss:0.0026537448765618452\n",
      "train loss:0.0030248276685876847\n",
      "train loss:0.0027834272149687396\n",
      "train loss:0.0027198082324575725\n",
      "train loss:0.000690396726503808\n",
      "train loss:0.013586421145924073\n",
      "train loss:0.01051395671937338\n",
      "train loss:0.0107636357822473\n",
      "train loss:0.03691112138339833\n",
      "train loss:0.004075797067698314\n",
      "train loss:0.0024336914343253842\n",
      "train loss:0.0025170532728121015\n",
      "train loss:0.0009734238245019786\n",
      "train loss:0.00038983373079799674\n",
      "train loss:0.0029527298072300288\n",
      "train loss:0.0009913538491044848\n",
      "train loss:0.0016607291540115256\n",
      "train loss:0.004713935070149574\n",
      "train loss:0.00031758542716431494\n",
      "train loss:0.005686696440066896\n",
      "train loss:0.0013229542610177477\n",
      "train loss:0.00040187892306197667\n",
      "train loss:0.0016441309233395799\n",
      "train loss:0.00207557547428872\n",
      "train loss:0.0023358426589636253\n",
      "train loss:0.002671128169054364\n",
      "train loss:0.0010449778693934816\n",
      "train loss:0.001549671484872117\n",
      "train loss:0.010069210794175509\n",
      "train loss:0.00047387474890730016\n",
      "train loss:0.0022493094926360046\n",
      "train loss:0.0026525246176251124\n",
      "train loss:0.0007002205015836536\n",
      "train loss:0.0019294531082280617\n",
      "train loss:0.009055520718597705\n",
      "train loss:0.0003236995359416201\n",
      "train loss:0.002723158617914217\n",
      "train loss:0.00022214215232200065\n",
      "train loss:0.04301667148807933\n",
      "train loss:0.00346701742376799\n",
      "train loss:0.0016296774490602763\n",
      "train loss:0.00013063878331454678\n",
      "train loss:0.0008836402088701375\n",
      "train loss:0.006238443441438375\n",
      "train loss:0.0037245723537694376\n",
      "train loss:0.0049202413637793015\n",
      "train loss:0.0008955651045763588\n",
      "train loss:0.0013443065994653228\n",
      "train loss:0.0041271458717033254\n",
      "train loss:0.00022478181734988497\n",
      "train loss:0.003427598207351273\n",
      "train loss:0.0012567622083724309\n",
      "train loss:0.0590690706393503\n",
      "train loss:0.0010619061544258155\n",
      "train loss:0.0013839995310078053\n",
      "train loss:0.01191247502459648\n",
      "train loss:0.0005191041305146682\n",
      "train loss:0.002982125714091024\n",
      "train loss:0.005678741978952616\n",
      "train loss:9.197113350764924e-05\n",
      "train loss:0.003638780007843222\n",
      "train loss:0.001906056502567325\n",
      "train loss:0.0008190133319373978\n",
      "train loss:0.015924963774046386\n",
      "train loss:0.0030148373429317813\n",
      "train loss:0.0003580928005850637\n",
      "train loss:0.0005653803180617685\n",
      "train loss:0.0009559445843643933\n",
      "train loss:0.0011233944917451296\n",
      "train loss:0.0017869789332715874\n",
      "train loss:0.0024677857145764793\n",
      "train loss:0.00015320967473410956\n",
      "train loss:0.002735238670698361\n",
      "train loss:0.004333367595465272\n",
      "train loss:0.005703726358874169\n",
      "train loss:0.0046716664784897674\n",
      "train loss:0.00409349402317449\n",
      "train loss:0.0010418515399567374\n",
      "train loss:0.005075470201158124\n",
      "train loss:0.0006192126666036556\n",
      "train loss:0.00332594363495689\n",
      "train loss:0.0010810723932143057\n",
      "train loss:0.0027567021194139217\n",
      "train loss:0.0020024344954190304\n",
      "train loss:0.0008840350998742737\n",
      "train loss:0.0056331243491276\n",
      "train loss:0.010687885274819032\n",
      "train loss:0.0029083237451956985\n",
      "train loss:0.00045535253255992765\n",
      "train loss:0.0008684461590074878\n",
      "train loss:0.0008637616882007429\n",
      "train loss:0.0033474327310592826\n",
      "train loss:0.011038865546289488\n",
      "train loss:0.0026993342190055874\n",
      "train loss:0.0009202724807685066\n",
      "train loss:0.012483902741190258\n",
      "train loss:0.0007545562211993034\n",
      "train loss:0.003768474576354166\n",
      "train loss:0.000784219692439805\n",
      "train loss:0.0013107143053603008\n",
      "train loss:0.004754406998602226\n",
      "train loss:0.0005582329021433847\n",
      "train loss:0.00045421444273752057\n",
      "train loss:0.001822330910122483\n",
      "train loss:0.0035235751305995318\n",
      "train loss:0.0026411674316820914\n",
      "train loss:0.0003954644004897169\n",
      "train loss:0.01824921851497604\n",
      "train loss:0.0005011958530038089\n",
      "train loss:0.004668178015666032\n",
      "train loss:0.006187451542059327\n",
      "train loss:0.0023958196112885983\n",
      "train loss:0.006662579730555958\n",
      "train loss:0.0031428781822147524\n",
      "train loss:0.005382333901203292\n",
      "train loss:0.0037775067194279932\n",
      "train loss:0.0002442570888424351\n",
      "train loss:0.005422081409395455\n",
      "train loss:0.0018281031835817166\n",
      "train loss:0.01882570692992726\n",
      "train loss:0.0023670475462366936\n",
      "train loss:0.00015241530891578833\n",
      "train loss:0.00022398464552203634\n",
      "train loss:0.003237264107727468\n",
      "train loss:0.0025011758805749875\n",
      "train loss:0.002154924983390502\n",
      "train loss:0.0031832750194316626\n",
      "train loss:0.0011113433052946768\n",
      "train loss:0.002381682656457584\n",
      "train loss:0.0005391647890636419\n",
      "train loss:0.0002699494659659966\n",
      "train loss:0.020342575888728013\n",
      "train loss:0.005505945339059568\n",
      "=== epoch:16, train acc:0.998, test acc:0.985 ===\n",
      "train loss:0.00081956487268205\n",
      "train loss:0.0001538797458667911\n",
      "train loss:0.00024955443967305277\n",
      "train loss:0.0034868533696711303\n",
      "train loss:0.0029209405493259024\n",
      "train loss:0.0043210487053273105\n",
      "train loss:0.0037839389405587144\n",
      "train loss:0.0025421003653523268\n",
      "train loss:0.001376149308720784\n",
      "train loss:0.001633895383457456\n",
      "train loss:0.0019431247940058906\n",
      "train loss:0.0019974492340701254\n",
      "train loss:0.00557855594355655\n",
      "train loss:0.0026037927532420037\n",
      "train loss:0.01197443404872648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0017258527104640179\n",
      "train loss:0.00458954669658616\n",
      "train loss:0.0013566704062506622\n",
      "train loss:0.0014799365207125122\n",
      "train loss:0.0026870602918496926\n",
      "train loss:0.0008855020702082472\n",
      "train loss:0.02958826754149865\n",
      "train loss:0.0011080414830932962\n",
      "train loss:0.0013923530048149768\n",
      "train loss:0.0002459903395019982\n",
      "train loss:0.00015928913937823737\n",
      "train loss:0.004066746326274891\n",
      "train loss:0.0007076190094433908\n",
      "train loss:0.0011120817745730843\n",
      "train loss:0.0010606514328476459\n",
      "train loss:0.007232229530404004\n",
      "train loss:0.002058418695443218\n",
      "train loss:0.0009080513513782633\n",
      "train loss:0.0007168201655654536\n",
      "train loss:0.005179914195815213\n",
      "train loss:0.002747561525840831\n",
      "train loss:0.004537281133601323\n",
      "train loss:0.005420547980746298\n",
      "train loss:0.09741995553447276\n",
      "train loss:0.0007780896901182209\n",
      "train loss:0.00021797504566835663\n",
      "train loss:0.001194188441921939\n",
      "train loss:0.00025851285642593757\n",
      "train loss:0.0006275812008357083\n",
      "train loss:0.007744832351971511\n",
      "train loss:0.00037192257118648554\n",
      "train loss:0.0006949217796935703\n",
      "train loss:5.3724716910475655e-05\n",
      "train loss:0.0012755450153258143\n",
      "train loss:0.004036431622975584\n",
      "train loss:0.0032180385416902147\n",
      "train loss:0.004652062879358495\n",
      "train loss:0.005311043190726442\n",
      "train loss:0.002136325765089088\n",
      "train loss:0.03448874171482207\n",
      "train loss:0.0017618370322076686\n",
      "train loss:0.007002767341371733\n",
      "train loss:0.0005115322048219672\n",
      "train loss:0.002496438579107764\n",
      "train loss:0.003952817068417266\n",
      "train loss:0.00017136195088436143\n",
      "train loss:0.004719784709729761\n",
      "train loss:0.0021799616256794273\n",
      "train loss:0.0007636946918278394\n",
      "train loss:0.002506732939153021\n",
      "train loss:0.04429905640412548\n",
      "train loss:0.0020606244818920224\n",
      "train loss:0.001913625601373579\n",
      "train loss:0.010240071806153273\n",
      "train loss:0.0013295981995669012\n",
      "train loss:0.013957411012705272\n",
      "train loss:0.0026503388367024837\n",
      "train loss:0.002207505716016023\n",
      "train loss:0.012427159431158146\n",
      "train loss:0.0006470599421497399\n",
      "train loss:0.0016540618703357573\n",
      "train loss:0.0006441529786412985\n",
      "train loss:0.007013242880238329\n",
      "train loss:0.010517962430397803\n",
      "train loss:0.00023922086802758414\n",
      "train loss:0.0006700862639358436\n",
      "train loss:0.0033565411809143675\n",
      "train loss:0.004748874809588396\n",
      "train loss:0.001148425973315016\n",
      "train loss:0.00394038924611404\n",
      "train loss:0.00037322851077307073\n",
      "train loss:0.001186944001369038\n",
      "train loss:0.0008367923342567471\n",
      "train loss:0.0029192935490466566\n",
      "train loss:0.0034014663927637067\n",
      "train loss:0.006508343543331377\n",
      "train loss:0.0009285844090561317\n",
      "train loss:0.0023213943467825463\n",
      "train loss:0.004649300570879484\n",
      "train loss:0.0031692174851325676\n",
      "train loss:0.003378919069464642\n",
      "train loss:0.006676183058203619\n",
      "train loss:0.0021906111593501666\n",
      "train loss:0.0007915767336645952\n",
      "train loss:0.0030864739900452265\n",
      "train loss:0.0011848750302263491\n",
      "train loss:0.0002856448910012426\n",
      "train loss:0.006260936094049119\n",
      "train loss:0.0004310722593941643\n",
      "train loss:0.0010057952261678161\n",
      "train loss:0.0029360046473577025\n",
      "train loss:0.002305106373789593\n",
      "train loss:0.002891868533443616\n",
      "train loss:0.001369082070795769\n",
      "train loss:0.003327352297088817\n",
      "train loss:0.0024248982877373808\n",
      "train loss:0.0037510490396292164\n",
      "train loss:0.0010852414219169413\n",
      "train loss:0.0038456733199651915\n",
      "train loss:0.00022787364408199284\n",
      "train loss:0.004864717804040851\n",
      "train loss:0.0003887481660404036\n",
      "train loss:0.0020713815123396496\n",
      "train loss:0.002722464095187572\n",
      "train loss:0.0017260505278121592\n",
      "train loss:0.010511929408384102\n",
      "train loss:0.002320885409417459\n",
      "train loss:0.0004315108969083581\n",
      "train loss:0.004380071417571078\n",
      "train loss:0.0007514047575332454\n",
      "train loss:0.0009543243278212778\n",
      "train loss:0.009186551449669367\n",
      "train loss:0.00519787709227979\n",
      "train loss:0.0016367303201748023\n",
      "train loss:0.00024991382121909847\n",
      "train loss:0.007802214811717175\n",
      "train loss:0.008755735096537378\n",
      "train loss:0.002360848198170114\n",
      "train loss:0.010514586633936694\n",
      "train loss:0.0021304776501037733\n",
      "train loss:0.0006306769654834633\n",
      "train loss:7.097444824752283e-05\n",
      "train loss:0.0013130231973378103\n",
      "train loss:0.0021152115518594944\n",
      "train loss:0.0010014225647797944\n",
      "train loss:0.003494441149540972\n",
      "train loss:0.0019407113983988054\n",
      "train loss:0.0036400751176885204\n",
      "train loss:0.0011305975387370948\n",
      "train loss:0.0031190169790948747\n",
      "train loss:0.0018938873816752753\n",
      "train loss:0.0014478141575375106\n",
      "train loss:0.000568083123027505\n",
      "train loss:0.00327125985775298\n",
      "train loss:0.0027938338204706852\n",
      "train loss:5.1296172535626436e-05\n",
      "train loss:0.0003156364157161245\n",
      "train loss:0.001767198818990747\n",
      "train loss:0.00574945218773344\n",
      "train loss:0.0014261433755925071\n",
      "train loss:0.038453851436254835\n",
      "train loss:0.00275258647769595\n",
      "train loss:0.0014142423196341028\n",
      "train loss:0.0010099192783449248\n",
      "train loss:0.0016157599384489239\n",
      "train loss:0.0075340192814542985\n",
      "train loss:0.005972851290910717\n",
      "train loss:0.001529903649719075\n",
      "train loss:0.0008042686431577436\n",
      "train loss:0.0009148672381100373\n",
      "train loss:0.000401362842881753\n",
      "train loss:0.0012954839779932734\n",
      "train loss:0.0018277241766132715\n",
      "train loss:0.001914425993839822\n",
      "train loss:0.0005177214835118001\n",
      "train loss:0.003330502799679726\n",
      "train loss:0.0007811042084046628\n",
      "train loss:0.00522462293722829\n",
      "train loss:0.004198682044422133\n",
      "train loss:0.0018362164495711098\n",
      "train loss:0.0028579068443513327\n",
      "train loss:0.001586821795368436\n",
      "train loss:0.007318083419385435\n",
      "train loss:0.003979341729006429\n",
      "train loss:0.002782017555861783\n",
      "train loss:0.0004580168747173647\n",
      "train loss:0.0013728691265164144\n",
      "train loss:0.0009304788675277574\n",
      "train loss:0.0017508922076515014\n",
      "train loss:0.00200469450807146\n",
      "train loss:0.0017800256454822965\n",
      "train loss:0.005825924072441722\n",
      "train loss:0.006650412203031425\n",
      "train loss:0.00043014584398825086\n",
      "train loss:0.00899353105854894\n",
      "train loss:0.0018580460178368088\n",
      "train loss:0.0025165645654066436\n",
      "train loss:0.011154431622830991\n",
      "train loss:0.001959301580568826\n",
      "train loss:0.0005608769567269331\n",
      "train loss:0.00020779140852964778\n",
      "train loss:0.006189865462906524\n",
      "train loss:0.004111753428486011\n",
      "train loss:0.0041955712494990425\n",
      "train loss:0.00046572495813176353\n",
      "train loss:0.006667421160390853\n",
      "train loss:0.005551837632040947\n",
      "train loss:0.002809494116894653\n",
      "train loss:0.0011235113961488654\n",
      "train loss:0.0002866871254736162\n",
      "train loss:0.003899640655627302\n",
      "train loss:6.641031505894137e-05\n",
      "train loss:0.0034939432751503506\n",
      "train loss:0.00014925748466575094\n",
      "train loss:0.0032029821600778365\n",
      "train loss:0.0033932336109933486\n",
      "train loss:0.0009798490165256694\n",
      "train loss:0.002018944901826533\n",
      "train loss:0.0014742202087493564\n",
      "train loss:0.005779700294231054\n",
      "train loss:0.002843771992155608\n",
      "train loss:0.005542092282049712\n",
      "train loss:0.020019132692193912\n",
      "train loss:0.0018549778384702991\n",
      "train loss:0.000707373154890496\n",
      "train loss:0.0015783361710348795\n",
      "train loss:0.001760281876783416\n",
      "train loss:0.0033470567399097274\n",
      "train loss:0.0014697127340624447\n",
      "train loss:0.002231333520558991\n",
      "train loss:0.005094880062564524\n",
      "train loss:6.888611376553382e-05\n",
      "train loss:0.001595024998490721\n",
      "train loss:0.0010869031090587887\n",
      "train loss:0.00020409713419062325\n",
      "train loss:0.0007604009270837563\n",
      "train loss:0.0028623445317465597\n",
      "train loss:0.0002795000721800618\n",
      "train loss:0.0013821008672976834\n",
      "train loss:0.0003647131377901484\n",
      "train loss:0.006258858497470919\n",
      "train loss:0.004040139303731335\n",
      "train loss:0.0014653006462792256\n",
      "train loss:0.00038657506012272457\n",
      "train loss:0.0032772237706507963\n",
      "train loss:0.0019429309983799118\n",
      "train loss:0.002072118691754226\n",
      "train loss:0.0034850507091023475\n",
      "train loss:0.001043148326129586\n",
      "train loss:0.0009317612427614258\n",
      "train loss:0.0008801448302910036\n",
      "train loss:0.0009269223490943901\n",
      "train loss:0.0019294312902913408\n",
      "train loss:0.00036593027081174183\n",
      "train loss:0.0015917325272747857\n",
      "train loss:0.002378892552499653\n",
      "train loss:0.0008909620906069911\n",
      "train loss:0.018949009775434244\n",
      "train loss:0.0039931770530716535\n",
      "train loss:0.0015862392387361848\n",
      "train loss:0.009369865984839397\n",
      "train loss:0.0030375014298125143\n",
      "train loss:0.0011733227734182122\n",
      "train loss:0.0003419859856660671\n",
      "train loss:0.0028637101827112344\n",
      "train loss:0.0018036340625498266\n",
      "train loss:0.0003126213799321046\n",
      "train loss:0.002231438412598922\n",
      "train loss:0.0006898994897838622\n",
      "train loss:0.0025667453339868335\n",
      "train loss:0.0015085259672453924\n",
      "train loss:5.565939016188681e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.001406050513979573\n",
      "train loss:0.0080211808096464\n",
      "train loss:0.000778327864688193\n",
      "train loss:0.00046169743591298036\n",
      "train loss:0.0009504071071258179\n",
      "train loss:0.004573643103167331\n",
      "train loss:0.007901955839868982\n",
      "train loss:0.0032290641811985586\n",
      "train loss:0.03149460097540298\n",
      "train loss:0.00011331986264362609\n",
      "train loss:0.002689894289209314\n",
      "train loss:0.00508404114175747\n",
      "train loss:0.0018936162272579422\n",
      "train loss:0.015078804216517559\n",
      "train loss:0.003799029312979183\n",
      "train loss:0.0010649153889119962\n",
      "train loss:0.0009343355285077678\n",
      "train loss:0.00048474775737208914\n",
      "train loss:0.0015008351903510126\n",
      "train loss:0.0004046167577729079\n",
      "train loss:0.0030185494281234544\n",
      "train loss:0.0002816837110715223\n",
      "train loss:0.0001673491833166358\n",
      "train loss:0.0018930869389108755\n",
      "train loss:0.012490065756741638\n",
      "train loss:0.00122436535034939\n",
      "train loss:0.0011745872697877706\n",
      "train loss:0.004293029265475399\n",
      "train loss:0.0020547623220506643\n",
      "train loss:0.020325957729373942\n",
      "train loss:0.0006453156419638129\n",
      "train loss:0.0028002752382526364\n",
      "train loss:0.00147922359812757\n",
      "train loss:0.002510776483570446\n",
      "train loss:0.02532890144831369\n",
      "train loss:0.0036669133257623754\n",
      "train loss:0.0025019512415320796\n",
      "train loss:0.00029707031696612725\n",
      "train loss:0.0007083373410997719\n",
      "train loss:0.0038450130710111106\n",
      "train loss:0.002918696293621316\n",
      "train loss:0.030068285455658567\n",
      "train loss:0.00047627097754760714\n",
      "train loss:0.0025650280568908527\n",
      "train loss:0.0002649359877123204\n",
      "train loss:0.00018978144275389776\n",
      "train loss:0.00046211001161223673\n",
      "train loss:0.0009111581284575233\n",
      "train loss:0.001206468626423983\n",
      "train loss:0.0004667806322966055\n",
      "train loss:0.002469580769275249\n",
      "train loss:0.0007775033800721513\n",
      "train loss:0.004542420630950804\n",
      "train loss:0.00012931804661953943\n",
      "train loss:0.0016184740162507574\n",
      "train loss:0.007554016651699743\n",
      "train loss:0.002800710470928597\n",
      "train loss:0.0007821527570517984\n",
      "train loss:0.00012601554973597055\n",
      "train loss:0.0014346783176685737\n",
      "train loss:0.0070637195574511135\n",
      "train loss:0.0005047962623064331\n",
      "train loss:0.0028459419306525752\n",
      "train loss:0.0021984811308112793\n",
      "train loss:0.003453703792191759\n",
      "train loss:0.0009381226349988586\n",
      "train loss:0.006306541329936667\n",
      "train loss:0.00019373118810825236\n",
      "train loss:0.0034793161559460595\n",
      "train loss:0.0005589288146991559\n",
      "train loss:0.0006303491427443439\n",
      "train loss:0.0054813444712225\n",
      "train loss:0.0023365084311384675\n",
      "train loss:0.00034360312887142335\n",
      "train loss:0.0008366423500646247\n",
      "train loss:0.00028123697621504286\n",
      "train loss:0.0008027676714925781\n",
      "train loss:0.003527480311959089\n",
      "train loss:0.0017692862177613763\n",
      "train loss:0.001308329529735989\n",
      "train loss:0.0006334467003618794\n",
      "train loss:0.0001756557773084156\n",
      "train loss:5.910170553670707e-05\n",
      "train loss:0.00034508350586968126\n",
      "train loss:0.0019058568841810106\n",
      "train loss:0.0011585524385409759\n",
      "train loss:0.00022992198946911548\n",
      "train loss:0.0009451813187480054\n",
      "train loss:0.0005506946599168358\n",
      "train loss:0.0007053728839798062\n",
      "train loss:0.004456378105385191\n",
      "train loss:0.0003743190852127487\n",
      "train loss:0.00026828882794374236\n",
      "train loss:0.003861968529589132\n",
      "train loss:0.0009286179080209195\n",
      "train loss:0.0002973310072865547\n",
      "train loss:0.003268005865193267\n",
      "train loss:0.004157859901047425\n",
      "train loss:0.005715031130052089\n",
      "train loss:0.01243026036774889\n",
      "train loss:0.0009983601994933088\n",
      "train loss:0.0007277351447778171\n",
      "train loss:0.0020228283972595355\n",
      "train loss:0.0002069876774814836\n",
      "train loss:0.00016542408683203384\n",
      "train loss:0.0012586835857984814\n",
      "train loss:0.00043411159282602773\n",
      "train loss:0.0009260605115230672\n",
      "train loss:0.005945314966367811\n",
      "train loss:0.001154902319371298\n",
      "train loss:0.0011017803399861353\n",
      "train loss:0.001145562689108673\n",
      "train loss:0.0005719297607475153\n",
      "train loss:0.0001312508618687226\n",
      "train loss:0.00039910720043868984\n",
      "train loss:0.0003891693038713362\n",
      "train loss:0.0029665445816446315\n",
      "train loss:0.0005195922409925261\n",
      "train loss:0.0013182761998342088\n",
      "train loss:0.0005928743995680325\n",
      "train loss:0.003032532098543892\n",
      "train loss:0.0026492290595271865\n",
      "train loss:0.000976043046809266\n",
      "train loss:0.0012027622728646133\n",
      "train loss:0.006927397343685373\n",
      "train loss:0.0013168073068637641\n",
      "train loss:0.0008238809523119988\n",
      "train loss:0.0003520193311572866\n",
      "train loss:0.0003425346134892661\n",
      "train loss:0.0003226089910241538\n",
      "train loss:0.004816717618766227\n",
      "train loss:0.0002510063191701377\n",
      "train loss:0.00022406908830425104\n",
      "train loss:0.0012131631711729432\n",
      "train loss:0.0007525556233095081\n",
      "train loss:0.00029438155014696116\n",
      "train loss:0.007507326251286723\n",
      "train loss:0.006790448137582586\n",
      "train loss:0.0009704858600963297\n",
      "train loss:0.00010799910157774434\n",
      "train loss:0.00515335023876261\n",
      "train loss:0.007769418971831501\n",
      "train loss:0.00014108201953161382\n",
      "train loss:0.008042839590957142\n",
      "train loss:0.0007283544372079445\n",
      "train loss:0.012824862409383766\n",
      "train loss:0.011190161413446814\n",
      "train loss:0.005827289372269507\n",
      "train loss:0.0005473073488403471\n",
      "train loss:0.0014944531695720515\n",
      "train loss:0.018219661462177617\n",
      "train loss:4.637977840787533e-05\n",
      "train loss:0.0017082826145972197\n",
      "train loss:0.0014717033664606311\n",
      "train loss:0.00032552821634188695\n",
      "train loss:0.0008748195818702591\n",
      "train loss:0.0005185885081571674\n",
      "train loss:0.0014965591611244221\n",
      "train loss:0.0021997729403298426\n",
      "train loss:0.0012824719782080043\n",
      "train loss:0.002324417585204582\n",
      "train loss:0.002556861188170182\n",
      "train loss:0.001616199847650609\n",
      "train loss:0.0026926389777738717\n",
      "train loss:0.004409950181873226\n",
      "train loss:0.009405388957376326\n",
      "train loss:0.0026494898612237673\n",
      "train loss:0.0013142360566685317\n",
      "train loss:0.0013732286845315248\n",
      "train loss:0.0050657754566617195\n",
      "train loss:0.001023757633056219\n",
      "train loss:0.0006506800952980738\n",
      "train loss:0.002332987878714612\n",
      "train loss:0.0035631286228377666\n",
      "train loss:0.002194963159052974\n",
      "train loss:0.001948473727105018\n",
      "train loss:0.0004450461116541958\n",
      "train loss:0.0011568872594945117\n",
      "train loss:0.00026329640923412727\n",
      "train loss:0.001305717369282973\n",
      "train loss:0.0028170262208129836\n",
      "train loss:0.00023761593410945552\n",
      "train loss:0.001341882871056001\n",
      "train loss:0.00114561825537344\n",
      "train loss:0.001121049970348863\n",
      "train loss:0.0005785750099155236\n",
      "train loss:0.0005310927871223841\n",
      "train loss:0.0005209188635942803\n",
      "train loss:0.001264839236397989\n",
      "train loss:0.00019634438743068575\n",
      "train loss:0.003075081254622018\n",
      "train loss:0.00017086322274483033\n",
      "train loss:0.005155549914478454\n",
      "train loss:0.00012666373822251613\n",
      "train loss:0.00031559147226955587\n",
      "train loss:0.0012193081121042032\n",
      "train loss:0.03265180054142098\n",
      "train loss:0.0016904918696581852\n",
      "train loss:0.0016284123428162197\n",
      "train loss:0.0008934997886181563\n",
      "train loss:0.00025248866973473613\n",
      "train loss:0.0037083942016148752\n",
      "train loss:9.887176266617918e-05\n",
      "train loss:0.007028177239617065\n",
      "train loss:0.003454907869587155\n",
      "train loss:0.0019279020385967378\n",
      "train loss:0.007226991419143165\n",
      "train loss:0.0005307574957893192\n",
      "train loss:0.0027531420348971118\n",
      "train loss:0.003278603386731467\n",
      "train loss:0.00021802172810544057\n",
      "train loss:0.01135456822014909\n",
      "train loss:0.005177132686396905\n",
      "train loss:0.001787396741918127\n",
      "train loss:0.0007759586028033039\n",
      "train loss:0.0011186890775067182\n",
      "train loss:0.007713182957070867\n",
      "train loss:0.0038466557784587845\n",
      "train loss:0.0006979123128581888\n",
      "train loss:0.011387551068326961\n",
      "train loss:0.002387267163891105\n",
      "train loss:0.0010474887882967194\n",
      "train loss:0.00158444004804544\n",
      "train loss:0.000539115745356643\n",
      "train loss:0.004300388925418025\n",
      "train loss:0.0030066962450765538\n",
      "train loss:0.001507027978438144\n",
      "train loss:0.0005849188692693162\n",
      "train loss:0.004309674568876998\n",
      "train loss:0.00045466035975270176\n",
      "train loss:0.0006398319257840737\n",
      "train loss:0.0071250267065717235\n",
      "train loss:0.004919447248847837\n",
      "train loss:0.00982877958496023\n",
      "train loss:0.0034684687143222044\n",
      "train loss:0.0268344564723125\n",
      "train loss:0.0037086883963940082\n",
      "train loss:0.0002350852682649092\n",
      "train loss:0.0030576822206091997\n",
      "train loss:0.0060208205892559544\n",
      "train loss:0.003487379988806462\n",
      "train loss:0.00041644683011467785\n",
      "train loss:0.0018083767003676196\n",
      "train loss:0.0024049359941045083\n",
      "train loss:0.010448044788965338\n",
      "train loss:0.0002697988614594992\n",
      "train loss:0.0016765452744705598\n",
      "train loss:0.0008608613798295625\n",
      "train loss:0.0028932912308414697\n",
      "train loss:0.013534166284568447\n",
      "train loss:0.0018287495874127037\n",
      "train loss:0.000417353726424132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.018180720418838624\n",
      "train loss:0.0026579775185489544\n",
      "train loss:0.001347678287768738\n",
      "train loss:0.0010742460157945795\n",
      "train loss:0.0014295760343135974\n",
      "train loss:0.0003332091813279938\n",
      "train loss:0.00015445644046887635\n",
      "train loss:0.000203534188629364\n",
      "train loss:0.0013813921069711654\n",
      "train loss:0.0010975325213899978\n",
      "train loss:0.0007137587836503859\n",
      "train loss:0.000840211064999194\n",
      "train loss:0.008897527332766504\n",
      "train loss:0.0028398782958912626\n",
      "train loss:0.0046521933872656004\n",
      "train loss:0.004004284687451274\n",
      "train loss:0.0004157576414110304\n",
      "train loss:0.0003273039662246267\n",
      "train loss:0.0010104296725352705\n",
      "train loss:0.0004462038139538714\n",
      "train loss:0.0010842314830158154\n",
      "train loss:0.00018411951369253431\n",
      "train loss:0.0006458615712552237\n",
      "train loss:0.031900705952883836\n",
      "train loss:0.0008851584633032669\n",
      "train loss:0.0006493424017693322\n",
      "train loss:0.0018689805540925378\n",
      "train loss:0.0038644235077504117\n",
      "train loss:0.001429864525440116\n",
      "train loss:0.0015091824470970696\n",
      "train loss:0.008005770514410867\n",
      "train loss:0.005747686003245894\n",
      "train loss:0.0012562218303144351\n",
      "train loss:0.0002287100240604768\n",
      "train loss:0.0002736152832283308\n",
      "train loss:0.001499960273997501\n",
      "train loss:0.0006025635479449277\n",
      "train loss:0.0027455621388020462\n",
      "train loss:0.002959867270304757\n",
      "train loss:0.0005383036750942462\n",
      "train loss:0.00018575944350174913\n",
      "train loss:0.006901498970492687\n",
      "train loss:0.001968823906177599\n",
      "train loss:0.0009499691396348389\n",
      "train loss:0.0005530051298226117\n",
      "train loss:0.0007731863550924721\n",
      "train loss:0.002812415144462506\n",
      "train loss:0.0020934005933923\n",
      "train loss:0.0015986545669841102\n",
      "train loss:0.0038832096345763163\n",
      "train loss:0.0023967080198421045\n",
      "train loss:0.015292416404429814\n",
      "train loss:0.0007138411936801522\n",
      "train loss:0.0015770891435833883\n",
      "train loss:0.0010236605251069924\n",
      "train loss:0.0001805577832641436\n",
      "train loss:0.00039180429136555846\n",
      "train loss:0.007440781740325768\n",
      "train loss:0.0015075389942151715\n",
      "train loss:0.004187294684579811\n",
      "train loss:0.0007526165481596389\n",
      "train loss:0.001123508710047623\n",
      "train loss:0.002656820867548727\n",
      "train loss:0.0007910716561488305\n",
      "train loss:0.0008022674354980399\n",
      "train loss:0.0033751218572418305\n",
      "train loss:0.005904841802828638\n",
      "train loss:0.003774755330887243\n",
      "train loss:0.004254076535156067\n",
      "train loss:0.00035607110665674904\n",
      "train loss:0.002780113766744575\n",
      "train loss:0.00013162862886670516\n",
      "train loss:0.00015268404352953595\n",
      "train loss:0.0012280915026245524\n",
      "train loss:0.0020532044474946953\n",
      "train loss:0.0007027268818622584\n",
      "train loss:0.003108274486700835\n",
      "train loss:0.0022784065012293945\n",
      "train loss:0.0008885323533398509\n",
      "train loss:0.011016603240618145\n",
      "train loss:0.0009550711958646909\n",
      "=== epoch:17, train acc:1.0, test acc:0.988 ===\n",
      "train loss:0.00020958507422772448\n",
      "train loss:0.0017782439817446607\n",
      "train loss:0.003402585877289814\n",
      "train loss:0.026833945058841492\n",
      "train loss:0.001968672390075348\n",
      "train loss:0.00019463418933608984\n",
      "train loss:0.010577137845898827\n",
      "train loss:0.0073704149298325\n",
      "train loss:0.001415767146666606\n",
      "train loss:0.003561272016701078\n",
      "train loss:0.0013627668674126387\n",
      "train loss:0.00707468011188822\n",
      "train loss:0.00044447720341184356\n",
      "train loss:0.00047185556488650083\n",
      "train loss:0.004942486952999018\n",
      "train loss:0.003946720696240855\n",
      "train loss:0.0029372867782589507\n",
      "train loss:0.010432641267425245\n",
      "train loss:0.0016450429878825737\n",
      "train loss:0.0006785968510722231\n",
      "train loss:0.0015880283987780932\n",
      "train loss:0.0008498180570839232\n",
      "train loss:0.0013411115329754766\n",
      "train loss:0.0022496059778793335\n",
      "train loss:0.016142936068422365\n",
      "train loss:0.0011912392106582421\n",
      "train loss:0.0003719039274635452\n",
      "train loss:0.00300649212017485\n",
      "train loss:0.004627575687627933\n",
      "train loss:0.0036720109360381874\n",
      "train loss:0.0005523634557908529\n",
      "train loss:0.0012454937093235917\n",
      "train loss:0.0038625608305702342\n",
      "train loss:0.00027892403446209404\n",
      "train loss:0.003706465042942797\n",
      "train loss:0.008056734787422186\n",
      "train loss:0.0012176316258624552\n",
      "train loss:0.004545940354110754\n",
      "train loss:0.010402943205039454\n",
      "train loss:0.0009158436539462074\n",
      "train loss:0.005053583940535017\n",
      "train loss:0.0003947144557719775\n",
      "train loss:0.0007305341501706996\n",
      "train loss:0.0007429756343188653\n",
      "train loss:6.367213520184822e-05\n",
      "train loss:0.0022093381185273563\n",
      "train loss:0.0029305574842098505\n",
      "train loss:0.002094944443995168\n",
      "train loss:0.0004226374451454664\n",
      "train loss:0.002430570574799032\n",
      "train loss:0.018477794221561747\n",
      "train loss:0.0019525405700159223\n",
      "train loss:0.002653615099276625\n",
      "train loss:0.0002584160166312334\n",
      "train loss:0.0014171518496195914\n",
      "train loss:0.0001793021075819422\n",
      "train loss:0.0009548910789015336\n",
      "train loss:0.0015150938711130104\n",
      "train loss:0.0016911053554062585\n",
      "train loss:0.0038797406213015597\n",
      "train loss:0.0030185242633256593\n",
      "train loss:0.0010342014150868115\n",
      "train loss:0.0007385354644744746\n",
      "train loss:0.0002616278978242193\n",
      "train loss:0.0004584336844059016\n",
      "train loss:4.3972953403194065e-05\n",
      "train loss:0.0001769394236708818\n",
      "train loss:0.0002772691074064017\n",
      "train loss:0.0005862499974371026\n",
      "train loss:0.0002946642051735173\n",
      "train loss:0.0008644314333949387\n",
      "train loss:0.0008315996361779816\n",
      "train loss:0.00031577483822504504\n",
      "train loss:0.0011994347498115274\n",
      "train loss:0.00233422514568034\n",
      "train loss:0.001149341185299847\n",
      "train loss:0.0003325023301036212\n",
      "train loss:0.0008216761419267012\n",
      "train loss:0.0030048991602898505\n",
      "train loss:0.0007353247470106225\n",
      "train loss:0.0030590401722957344\n",
      "train loss:0.0003743346806517333\n",
      "train loss:6.443487921448709e-05\n",
      "train loss:0.0010967119426873839\n",
      "train loss:0.00032597000817077176\n",
      "train loss:3.2713935670275936e-05\n",
      "train loss:0.00585384136614035\n",
      "train loss:0.0028637705299645296\n",
      "train loss:0.002575541190243354\n",
      "train loss:0.0010924789843790841\n",
      "train loss:0.0013260282591766959\n",
      "train loss:8.90569112757911e-05\n",
      "train loss:0.0019436138722089974\n",
      "train loss:0.0016836861112215036\n",
      "train loss:0.004375159897469058\n",
      "train loss:0.0009378871350212057\n",
      "train loss:0.0027735420034827474\n",
      "train loss:0.0005112807428358526\n",
      "train loss:0.004839629008985586\n",
      "train loss:0.0002669770044097333\n",
      "train loss:0.00990032407328306\n",
      "train loss:0.006082563846889096\n",
      "train loss:0.006092736748512723\n",
      "train loss:0.010415648023463708\n",
      "train loss:0.002087444586856951\n",
      "train loss:0.0002066986684217498\n",
      "train loss:0.007315356982780897\n",
      "train loss:0.0017607004769693145\n",
      "train loss:0.0011119812905781143\n",
      "train loss:0.001259916640946557\n",
      "train loss:0.0007149359417987826\n",
      "train loss:0.00042760983236414426\n",
      "train loss:0.0015111344107445093\n",
      "train loss:0.004236720107008406\n",
      "train loss:0.00708875796030452\n",
      "train loss:0.005983835711732325\n",
      "train loss:0.002253290998477617\n",
      "train loss:0.0008243814573103191\n",
      "train loss:0.0005704191836247458\n",
      "train loss:0.0038114573659024475\n",
      "train loss:0.012864970542601608\n",
      "train loss:0.00472491765128925\n",
      "train loss:0.0032341470145286298\n",
      "train loss:0.0001990735727317633\n",
      "train loss:0.0007967424064940698\n",
      "train loss:0.011317675240842626\n",
      "train loss:0.000994859478029765\n",
      "train loss:0.005880043165710344\n",
      "train loss:0.0022754426419794756\n",
      "train loss:0.00317961148279859\n",
      "train loss:0.003299642963637424\n",
      "train loss:0.0026068919998766316\n",
      "train loss:0.0014151512003339692\n",
      "train loss:0.005546636761390001\n",
      "train loss:0.0021764889342345815\n",
      "train loss:0.002055579983870956\n",
      "train loss:0.001593918913972178\n",
      "train loss:0.0015487203339861937\n",
      "train loss:0.012675131211727794\n",
      "train loss:0.005356383022858906\n",
      "train loss:0.0023315110085521274\n",
      "train loss:0.008009211719054714\n",
      "train loss:0.0018113377273507889\n",
      "train loss:0.0014221808136277159\n",
      "train loss:0.0003196763454569616\n",
      "train loss:0.023118218882081792\n",
      "train loss:0.0017650246342677304\n",
      "train loss:0.0015289319906249166\n",
      "train loss:0.046566229180428687\n",
      "train loss:0.0003189295886044357\n",
      "train loss:0.004265314834695649\n",
      "train loss:0.0013331403955882664\n",
      "train loss:0.00043286518543196595\n",
      "train loss:0.0064968794255840226\n",
      "train loss:0.001582231398734258\n",
      "train loss:0.0005046795036932402\n",
      "train loss:0.00020902726446223887\n",
      "train loss:0.003109036097874878\n",
      "train loss:0.0011826683357264018\n",
      "train loss:0.0032019172608608922\n",
      "train loss:0.0007547426273206643\n",
      "train loss:0.0009298667592276469\n",
      "train loss:0.0009084382387922582\n",
      "train loss:0.0015463124894363445\n",
      "train loss:0.007424742068826512\n",
      "train loss:0.00171824990771544\n",
      "train loss:0.002730675886830178\n",
      "train loss:0.002147980733071854\n",
      "train loss:0.0009016537606607604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.005973507751566035\n",
      "train loss:0.0036530088881340087\n",
      "train loss:0.005202928467512385\n",
      "train loss:0.002023943693389002\n",
      "train loss:0.005483183805538459\n",
      "train loss:0.0024839546726207083\n",
      "train loss:0.010763132307199878\n",
      "train loss:0.005125590429922881\n",
      "train loss:0.00456373320339351\n",
      "train loss:0.0003590966748285662\n",
      "train loss:0.0010302792708287126\n",
      "train loss:0.003139435108112931\n",
      "train loss:0.0011305396811771588\n",
      "train loss:0.0004092406828456045\n",
      "train loss:0.004697293804147768\n",
      "train loss:0.0029157985904182827\n",
      "train loss:0.0007133104783484106\n",
      "train loss:0.00037339073651355617\n",
      "train loss:0.0002570027029436448\n",
      "train loss:0.00016339041834127966\n",
      "train loss:0.003710291215505009\n",
      "train loss:0.003916986969696962\n",
      "train loss:0.0005690415069744782\n",
      "train loss:0.0018922612532598709\n",
      "train loss:0.005464075107671544\n",
      "train loss:0.0013315263919376635\n",
      "train loss:0.00030450734147756273\n",
      "train loss:0.003720053309836603\n",
      "train loss:0.0024880161460041354\n",
      "train loss:0.00277560168306657\n",
      "train loss:9.239234428727518e-05\n",
      "train loss:0.0013062954680011555\n",
      "train loss:0.00017563295006815685\n",
      "train loss:0.0017312108115388502\n",
      "train loss:0.014555724392940275\n",
      "train loss:0.004797110883096788\n",
      "train loss:0.0021895566966315897\n",
      "train loss:0.014605488115086702\n",
      "train loss:0.002374703812720454\n",
      "train loss:0.002644211665608848\n",
      "train loss:0.0026830277542468388\n",
      "train loss:0.004051182590048206\n",
      "train loss:0.003181659543230267\n",
      "train loss:0.0015379414730762864\n",
      "train loss:0.0024571116341685316\n",
      "train loss:0.001144757415690438\n",
      "train loss:0.00648481256102662\n",
      "train loss:0.0012741090333094505\n",
      "train loss:0.006285513021976396\n",
      "train loss:0.003604435402835066\n",
      "train loss:0.00021459053001552742\n",
      "train loss:0.056906032960463794\n",
      "train loss:0.00740691886465632\n",
      "train loss:0.002068160267438061\n",
      "train loss:0.0038228655074829293\n",
      "train loss:0.0015946499420791045\n",
      "train loss:0.00104541864921717\n",
      "train loss:0.008052177710556566\n",
      "train loss:0.0005430191410274452\n",
      "train loss:0.06786734760683151\n",
      "train loss:0.0014583902128607754\n",
      "train loss:0.00570748344633294\n",
      "train loss:0.00035615786230684606\n",
      "train loss:0.0007253670162034983\n",
      "train loss:0.0019335654297444485\n",
      "train loss:0.000541506381417167\n",
      "train loss:0.0024106988655559045\n",
      "train loss:0.0008626995220848351\n",
      "train loss:0.0024898003316391666\n",
      "train loss:0.007683276844822382\n",
      "train loss:0.0005991708621932284\n",
      "train loss:0.004419800351194412\n",
      "train loss:0.0012344070406660349\n",
      "train loss:0.0005594116074746988\n",
      "train loss:0.007612332555978058\n",
      "train loss:0.007902411438365904\n",
      "train loss:0.0004673561580190611\n",
      "train loss:0.0008154023303427643\n",
      "train loss:0.007313820640106463\n",
      "train loss:0.0001068554013512434\n",
      "train loss:0.0024279570379156807\n",
      "train loss:0.008309024413513593\n",
      "train loss:0.00786781539967845\n",
      "train loss:0.008736389619155382\n",
      "train loss:0.0014369861088447497\n",
      "train loss:0.0011223605392750807\n",
      "train loss:0.00397318938120554\n",
      "train loss:0.001473702028160593\n",
      "train loss:0.00935422551673452\n",
      "train loss:0.0024515144278462143\n",
      "train loss:0.061436055277924694\n",
      "train loss:0.0014520929315608145\n",
      "train loss:0.002057052304575466\n",
      "train loss:0.0013517432717250946\n",
      "train loss:0.0015492619684317113\n",
      "train loss:0.0031613061692939287\n",
      "train loss:0.008694527926340947\n",
      "train loss:0.0015180347092091275\n",
      "train loss:0.001320540525290987\n",
      "train loss:0.002126919315947638\n",
      "train loss:0.002688989132278833\n",
      "train loss:0.008108078103040237\n",
      "train loss:0.00032617091336844395\n",
      "train loss:0.0014886812358790478\n",
      "train loss:0.0007389221369007768\n",
      "train loss:0.0008238707451069127\n",
      "train loss:0.0012074191317085411\n",
      "train loss:0.00014124350325161192\n",
      "train loss:0.0047747302850015185\n",
      "train loss:0.0024221915901216384\n",
      "train loss:0.003369804140614411\n",
      "train loss:0.00027119698342316085\n",
      "train loss:0.0013047303524179987\n",
      "train loss:0.0011366803592753444\n",
      "train loss:0.0004688725134205901\n",
      "train loss:0.001659913971707958\n",
      "train loss:0.0011186493565248376\n",
      "train loss:0.004269873060579198\n",
      "train loss:0.0036309910504506195\n",
      "train loss:0.0022931739221333882\n",
      "train loss:0.001374728885537915\n",
      "train loss:0.0006186228648429608\n",
      "train loss:0.005191869684851976\n",
      "train loss:0.00077250337505126\n",
      "train loss:0.005462583816710299\n",
      "train loss:0.001887439372276813\n",
      "train loss:0.0018034251926442948\n",
      "train loss:0.0006749378369893408\n",
      "train loss:0.0004969240570256759\n",
      "train loss:0.0027813835820725235\n",
      "train loss:0.0005885110209991364\n",
      "train loss:0.0009586687221787002\n",
      "train loss:0.003532096460443055\n",
      "train loss:0.0070198198734836405\n",
      "train loss:0.00023467033467833783\n",
      "train loss:0.02016691196921519\n",
      "train loss:0.002404019101810212\n",
      "train loss:0.009266446610969115\n",
      "train loss:0.002393977006510901\n",
      "train loss:0.010579635132252765\n",
      "train loss:0.00026328594315187104\n",
      "train loss:0.0013060385463445554\n",
      "train loss:0.002283819409744989\n",
      "train loss:0.0019225012806343908\n",
      "train loss:0.00014952819330949692\n",
      "train loss:0.0037507218531350195\n",
      "train loss:0.0009375304845513083\n",
      "train loss:0.0026207717621411974\n",
      "train loss:0.0021787216373115904\n",
      "train loss:0.0013856782319479361\n",
      "train loss:0.00344355871752597\n",
      "train loss:0.006816274193391024\n",
      "train loss:0.003411764787937255\n",
      "train loss:0.00033634903026494355\n",
      "train loss:0.003647696052008706\n",
      "train loss:8.84543831273018e-05\n",
      "train loss:0.013964333124639879\n",
      "train loss:0.0009693878538890929\n",
      "train loss:0.0019688062118088418\n",
      "train loss:0.0009459137978647985\n",
      "train loss:0.00033127217963413084\n",
      "train loss:0.005622269290241752\n",
      "train loss:0.0006739110241418935\n",
      "train loss:0.0016674107098079692\n",
      "train loss:0.003863657643186439\n",
      "train loss:0.0007982469222792028\n",
      "train loss:0.004761875602326143\n",
      "train loss:0.000648944659950212\n",
      "train loss:0.006199930802702773\n",
      "train loss:0.0003850268278229995\n",
      "train loss:0.00643486793185561\n",
      "train loss:0.0029389466833899987\n",
      "train loss:0.00015654822467791126\n",
      "train loss:0.0021468161307196558\n",
      "train loss:0.04344699757135058\n",
      "train loss:0.003891768204960305\n",
      "train loss:0.0004409231262258879\n",
      "train loss:0.001478366171249449\n",
      "train loss:0.002472267945766502\n",
      "train loss:0.00015092947211659623\n",
      "train loss:0.0023852721342801806\n",
      "train loss:0.0009004546489010341\n",
      "train loss:0.0017392119421302445\n",
      "train loss:0.0013235288157900141\n",
      "train loss:0.0005761656674831381\n",
      "train loss:0.0004275048769859395\n",
      "train loss:0.00389231115149534\n",
      "train loss:0.0025220992503878596\n",
      "train loss:0.0004858053532700993\n",
      "train loss:0.0009676751190365027\n",
      "train loss:0.0007805156610569478\n",
      "train loss:0.00023740539186938066\n",
      "train loss:0.0025844324357452075\n",
      "train loss:0.0012868444663952027\n",
      "train loss:0.0005789557501087401\n",
      "train loss:0.004266196406960549\n",
      "train loss:0.0023265880336942168\n",
      "train loss:0.006912544601009757\n",
      "train loss:0.015298509268669522\n",
      "train loss:0.003232008181880343\n",
      "train loss:0.0018131055064704348\n",
      "train loss:0.009063226782825045\n",
      "train loss:0.0018322436688992553\n",
      "train loss:0.0025959253260646026\n",
      "train loss:0.0007411891882448554\n",
      "train loss:0.00383852712528733\n",
      "train loss:0.003753718986502938\n",
      "train loss:0.00047121110541810016\n",
      "train loss:0.029738687407817897\n",
      "train loss:0.0003776728313085408\n",
      "train loss:0.0019799047037084238\n",
      "train loss:0.001599629831817604\n",
      "train loss:0.0004028969695031861\n",
      "train loss:0.0048023693053911975\n",
      "train loss:0.00021072476804967429\n",
      "train loss:0.0018899274063057386\n",
      "train loss:0.0016478258299373561\n",
      "train loss:0.0005358196933854553\n",
      "train loss:0.0004881246608698435\n",
      "train loss:0.0009626713569754389\n",
      "train loss:0.00013062409515803586\n",
      "train loss:0.002587159300392964\n",
      "train loss:0.004085313865513817\n",
      "train loss:0.0021114256824467737\n",
      "train loss:0.006284390589723869\n",
      "train loss:0.0030386923813099697\n",
      "train loss:0.003777170903759477\n",
      "train loss:0.0038273750399515976\n",
      "train loss:0.0016141660242047248\n",
      "train loss:0.0011514201717765593\n",
      "train loss:0.00025211616642467345\n",
      "train loss:0.0030345315801720957\n",
      "train loss:0.0017560492215454694\n",
      "train loss:0.0022607993055903247\n",
      "train loss:0.0004056204193836597\n",
      "train loss:0.01514984711100076\n",
      "train loss:0.000128390059074057\n",
      "train loss:0.001383778187180455\n",
      "train loss:0.00026244525276500417\n",
      "train loss:0.003208583233289012\n",
      "train loss:0.005241491575096937\n",
      "train loss:0.00011544682350567184\n",
      "train loss:0.001968838591651632\n",
      "train loss:0.00030332118690888865\n",
      "train loss:0.004187351768878882\n",
      "train loss:0.004784798657196129\n",
      "train loss:0.0019628921491510088\n",
      "train loss:0.0013503947185471371\n",
      "train loss:8.201889026595994e-05\n",
      "train loss:0.000915229775724275\n",
      "train loss:0.0004245112100496836\n",
      "train loss:8.615927339317901e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0027390344119147454\n",
      "train loss:0.0006904286961904163\n",
      "train loss:0.00013822058704413156\n",
      "train loss:0.001972187930779927\n",
      "train loss:0.0008822618053846728\n",
      "train loss:5.784003169620625e-05\n",
      "train loss:0.0019564525665214734\n",
      "train loss:3.9191537832930886e-05\n",
      "train loss:8.354485909360825e-05\n",
      "train loss:0.004835744619202876\n",
      "train loss:0.0031740881399238423\n",
      "train loss:0.0038256675056725325\n",
      "train loss:0.002238956898900008\n",
      "train loss:0.012213064917206884\n",
      "train loss:0.000560889823689713\n",
      "train loss:0.00015954134491807378\n",
      "train loss:0.0011280759321496937\n",
      "train loss:0.003280058893435155\n",
      "train loss:0.0016635540539409622\n",
      "train loss:0.0015734707314011961\n",
      "train loss:0.003927163697024277\n",
      "train loss:0.001448071641084608\n",
      "train loss:0.001157705034698442\n",
      "train loss:0.0015676488200113442\n",
      "train loss:0.021430360112963856\n",
      "train loss:0.0013081535379410652\n",
      "train loss:0.0004828045592191543\n",
      "train loss:0.00044046134083273715\n",
      "train loss:0.00022930064920037037\n",
      "train loss:0.0011409192955254626\n",
      "train loss:0.005486200034086067\n",
      "train loss:0.002907190933714809\n",
      "train loss:0.0004900780676601233\n",
      "train loss:0.0008242539019731952\n",
      "train loss:0.0015840813804329096\n",
      "train loss:0.03335562629738895\n",
      "train loss:0.006834694196219917\n",
      "train loss:0.00010351729626912216\n",
      "train loss:0.0036596928872284873\n",
      "train loss:0.005053733503534333\n",
      "train loss:0.015437571698411643\n",
      "train loss:0.022525810028185704\n",
      "train loss:0.0013586015228176877\n",
      "train loss:0.0012996538341540327\n",
      "train loss:0.0012083002171044616\n",
      "train loss:0.003009296350980122\n",
      "train loss:0.003245233670674707\n",
      "train loss:0.0009825348416458443\n",
      "train loss:0.0026968767322107525\n",
      "train loss:0.0004763348783098754\n",
      "train loss:0.00045221642109820646\n",
      "train loss:0.006784961706385341\n",
      "train loss:0.0014211651817842771\n",
      "train loss:0.0022591729218689096\n",
      "train loss:0.0032090835438211805\n",
      "train loss:0.006677465451982705\n",
      "train loss:0.017715128130352826\n",
      "train loss:0.0005461013126183858\n",
      "train loss:0.0013215035200241586\n",
      "train loss:0.0003518444888123453\n",
      "train loss:0.0012065149530011497\n",
      "train loss:0.0004052531053952427\n",
      "train loss:0.0002195429691646902\n",
      "train loss:0.004373567346638345\n",
      "train loss:0.0005452231904339674\n",
      "train loss:0.003060536877870025\n",
      "train loss:0.0009417509854524128\n",
      "train loss:0.004788095869848855\n",
      "train loss:0.0003709904413906094\n",
      "train loss:0.002122981714064468\n",
      "train loss:0.019947083200137376\n",
      "train loss:9.587779861758185e-05\n",
      "train loss:0.0014597761681462407\n",
      "train loss:0.0011595653757035375\n",
      "train loss:0.0025157728863229294\n",
      "train loss:0.0034075246379530293\n",
      "train loss:0.00024666039010627796\n",
      "train loss:0.0008731556730512916\n",
      "train loss:0.0010158226826557314\n",
      "train loss:0.0027980194559948423\n",
      "train loss:0.0006232530008755262\n",
      "train loss:0.0030549383528240197\n",
      "train loss:0.0008231984674706085\n",
      "train loss:0.002403804383311126\n",
      "train loss:0.017551104678613736\n",
      "train loss:0.004060882673867711\n",
      "train loss:0.002180164948319704\n",
      "train loss:0.0018708567971200787\n",
      "train loss:0.0030745260747738995\n",
      "train loss:0.002670113411400056\n",
      "train loss:0.0007945522444247869\n",
      "train loss:0.0012648108469047804\n",
      "train loss:0.0005712769861552786\n",
      "train loss:0.001941126856872068\n",
      "train loss:0.02781460876733198\n",
      "train loss:0.0006325722312007951\n",
      "train loss:0.002948573160910642\n",
      "train loss:0.0010810155998492456\n",
      "train loss:0.00621620659921934\n",
      "train loss:0.0009288317755750577\n",
      "train loss:0.014157831865597213\n",
      "train loss:0.004063047804538564\n",
      "train loss:0.000975668552431947\n",
      "train loss:0.012737516381217052\n",
      "train loss:8.308998040207642e-05\n",
      "train loss:0.0019605949458040514\n",
      "train loss:0.0002520040521190347\n",
      "train loss:0.0016653546059906725\n",
      "train loss:0.0025583619478773234\n",
      "train loss:0.01067080837235693\n",
      "train loss:0.004001776885441327\n",
      "train loss:0.000428065943226351\n",
      "train loss:0.004325881125157742\n",
      "train loss:0.0018245703635003526\n",
      "train loss:0.0013367269468630629\n",
      "train loss:0.0032018639018873717\n",
      "train loss:0.0038451452731946413\n",
      "train loss:0.001508022217767001\n",
      "train loss:0.0003364674717115309\n",
      "train loss:0.0006932632919235064\n",
      "train loss:0.0062543110718967666\n",
      "train loss:0.0036022898795525816\n",
      "train loss:0.004489808779609805\n",
      "train loss:0.0002849723581218259\n",
      "train loss:0.001044959741633179\n",
      "train loss:0.00017332530373566753\n",
      "train loss:0.0042252024962083886\n",
      "train loss:0.00018893768482201198\n",
      "train loss:0.0028335888383338977\n",
      "train loss:0.00036868773831461444\n",
      "train loss:0.0009629422728893943\n",
      "train loss:0.0001874255051189293\n",
      "train loss:4.302432216899243e-05\n",
      "train loss:0.03785988182940202\n",
      "train loss:0.0027892313594919766\n",
      "train loss:0.00035150523779084436\n",
      "train loss:4.0671487352838076e-05\n",
      "train loss:0.00048676438304423727\n",
      "train loss:0.00031084797331788777\n",
      "train loss:0.0019412430301174036\n",
      "train loss:0.00018775521507409705\n",
      "train loss:0.0027879814033869386\n",
      "train loss:0.0109311677507801\n",
      "train loss:0.003783783636869857\n",
      "train loss:0.00046763417863262066\n",
      "train loss:0.00044334281204913404\n",
      "train loss:0.0007267335604843275\n",
      "train loss:0.00010705991384888255\n",
      "train loss:0.0036711041017102306\n",
      "train loss:0.008945508405462389\n",
      "train loss:0.0003382171524849817\n",
      "train loss:0.00012151164303560765\n",
      "train loss:0.00016318515801909885\n",
      "train loss:0.0047799054178893005\n",
      "train loss:0.004244691307105726\n",
      "train loss:0.000567260644171935\n",
      "train loss:0.0005153930062816897\n",
      "train loss:0.0005060820529344267\n",
      "train loss:0.0026218979436017404\n",
      "train loss:0.00028702278945208574\n",
      "train loss:0.00043647672823297104\n",
      "train loss:0.0005730527540380351\n",
      "train loss:0.0016996965849031393\n",
      "train loss:0.001439181714751319\n",
      "train loss:0.0009106013779515913\n",
      "train loss:0.0025662989254213674\n",
      "train loss:0.006481874644749649\n",
      "train loss:0.0003021525906672276\n",
      "train loss:0.0006237107938876583\n",
      "train loss:0.002014568426092245\n",
      "train loss:0.00032932845253806614\n",
      "train loss:0.0015997776786235605\n",
      "train loss:0.0003144301550042562\n",
      "train loss:0.0011499123152125844\n",
      "train loss:0.001082421093865812\n",
      "train loss:0.0007140998987091271\n",
      "train loss:0.0022226145996038795\n",
      "train loss:0.003129338098361196\n",
      "train loss:0.0018823209173192224\n",
      "=== epoch:18, train acc:0.999, test acc:0.982 ===\n",
      "train loss:0.00406710774306435\n",
      "train loss:5.029027179066805e-05\n",
      "train loss:0.00028838663453338576\n",
      "train loss:0.0012112659187211027\n",
      "train loss:0.002974618593056743\n",
      "train loss:0.0015811969238643156\n",
      "train loss:0.0006917672417734611\n",
      "train loss:0.0014996964498715042\n",
      "train loss:0.003229344328959766\n",
      "train loss:0.004251785641159016\n",
      "train loss:0.00010356677564637047\n",
      "train loss:0.004534494618074031\n",
      "train loss:0.0026509123255127557\n",
      "train loss:0.004596156879252386\n",
      "train loss:0.0006493409226337138\n",
      "train loss:0.00019909205440273817\n",
      "train loss:0.00010359358753732197\n",
      "train loss:0.00023838438372931507\n",
      "train loss:0.004500014970253484\n",
      "train loss:0.0005382391599871676\n",
      "train loss:0.0012208955852682885\n",
      "train loss:0.0013332777763289633\n",
      "train loss:0.00036895518341038055\n",
      "train loss:0.002266107408795537\n",
      "train loss:0.0007029281656972529\n",
      "train loss:0.0014949189174966679\n",
      "train loss:0.00855997483544629\n",
      "train loss:0.0033819935468814427\n",
      "train loss:0.0001992860225857079\n",
      "train loss:0.0018694160531577925\n",
      "train loss:0.001103610724712861\n",
      "train loss:0.0003344462146333695\n",
      "train loss:0.0009366561182216276\n",
      "train loss:0.0001718177687403084\n",
      "train loss:0.0008357645813499488\n",
      "train loss:0.0006583575994418288\n",
      "train loss:0.00038199689537195844\n",
      "train loss:0.0001078655247244648\n",
      "train loss:0.0009426305433939264\n",
      "train loss:0.002042620843189553\n",
      "train loss:0.007057751306639985\n",
      "train loss:0.0034637362681192614\n",
      "train loss:0.00019001833809260274\n",
      "train loss:0.0023557075543097976\n",
      "train loss:0.0007166711425353117\n",
      "train loss:0.00017577030715258725\n",
      "train loss:0.0010704212472750247\n",
      "train loss:0.0012230018536659802\n",
      "train loss:0.0004220330707492943\n",
      "train loss:0.012384863484448142\n",
      "train loss:0.0008357102371325916\n",
      "train loss:0.001936880507144131\n",
      "train loss:0.00033618804291046025\n",
      "train loss:0.0007903883531924525\n",
      "train loss:0.0005639601866479406\n",
      "train loss:0.002285778947711899\n",
      "train loss:0.002973677033034172\n",
      "train loss:0.0009512669078554524\n",
      "train loss:0.004245180760942993\n",
      "train loss:0.006512071389562767\n",
      "train loss:0.00812836506914442\n",
      "train loss:0.0004266150721127657\n",
      "train loss:0.014220100087806817\n",
      "train loss:0.0038736292574594777\n",
      "train loss:0.0009012273712888141\n",
      "train loss:0.0005327235370409754\n",
      "train loss:0.009636463031912772\n",
      "train loss:0.005362302131540243\n",
      "train loss:0.0029024468306703168\n",
      "train loss:0.0010454266387924403\n",
      "train loss:0.001135548831016965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0004840050584161294\n",
      "train loss:0.0015934814292118205\n",
      "train loss:0.00037167470168728017\n",
      "train loss:0.001458699711124438\n",
      "train loss:0.0002310598899368085\n",
      "train loss:0.0008874530977942588\n",
      "train loss:0.00044947252188509744\n",
      "train loss:0.00048013136618065423\n",
      "train loss:0.0021388813452460163\n",
      "train loss:0.0046120046101995145\n",
      "train loss:0.004279318726447475\n",
      "train loss:0.0008775486180303038\n",
      "train loss:0.0011436866093044598\n",
      "train loss:0.00021870817862705487\n",
      "train loss:0.002169659514884379\n",
      "train loss:0.010644392071214355\n",
      "train loss:7.3978560664496e-06\n",
      "train loss:0.0012438530608378613\n",
      "train loss:0.0009164372739909815\n",
      "train loss:0.003953515760929798\n",
      "train loss:0.0049517697085477855\n",
      "train loss:0.001622799694016225\n",
      "train loss:0.010211531223694786\n",
      "train loss:0.009950750552500534\n",
      "train loss:0.003115794204136598\n",
      "train loss:0.005079292087509493\n",
      "train loss:0.00014745454900387346\n",
      "train loss:0.004284289740314397\n",
      "train loss:0.0020617374965778363\n",
      "train loss:0.0034463387511231357\n",
      "train loss:0.0025568098919555365\n",
      "train loss:0.0009019816771425441\n",
      "train loss:0.0008234033431193297\n",
      "train loss:0.0019133162010594104\n",
      "train loss:0.0038989497138675095\n",
      "train loss:0.0022411350850807432\n",
      "train loss:0.0021927104081778492\n",
      "train loss:0.020961280958115763\n",
      "train loss:0.000350462426500717\n",
      "train loss:0.0005433224727463587\n",
      "train loss:0.0002367303270265704\n",
      "train loss:0.001375958315128589\n",
      "train loss:0.002134902901138383\n",
      "train loss:0.0003725909764873136\n",
      "train loss:0.0013285609735272932\n",
      "train loss:0.0015412412905612025\n",
      "train loss:0.026058564026988962\n",
      "train loss:0.012555705117673503\n",
      "train loss:0.008234848037944375\n",
      "train loss:0.00185220604247482\n",
      "train loss:0.0034035070039699143\n",
      "train loss:4.0173533183715186e-05\n",
      "train loss:0.007259183680912829\n",
      "train loss:0.0016082755443106941\n",
      "train loss:0.005110797258199815\n",
      "train loss:0.0004732470483684915\n",
      "train loss:0.0004227372107648517\n",
      "train loss:0.0026564064196865924\n",
      "train loss:0.002213468717577316\n",
      "train loss:0.0014518368003249512\n",
      "train loss:0.0004069419808543107\n",
      "train loss:0.0011987773273750535\n",
      "train loss:0.0006508561772005565\n",
      "train loss:0.0019030281743843843\n",
      "train loss:0.0002522114233341655\n",
      "train loss:0.0009493802823146952\n",
      "train loss:0.007966375772326326\n",
      "train loss:0.0067296363519199175\n",
      "train loss:0.0018529293720441816\n",
      "train loss:0.00026488380177461945\n",
      "train loss:0.0015325790067791181\n",
      "train loss:0.003603134089066115\n",
      "train loss:0.0021081964964443133\n",
      "train loss:0.00016167945122784016\n",
      "train loss:0.0030489021270022386\n",
      "train loss:0.0021898338731851192\n",
      "train loss:0.0003801491079165504\n",
      "train loss:0.0010003142954912029\n",
      "train loss:0.0005736763942339253\n",
      "train loss:0.0019206640912831669\n",
      "train loss:0.0017675835572044665\n",
      "train loss:0.031759364979467915\n",
      "train loss:0.0008513748465374211\n",
      "train loss:0.0003690143634808727\n",
      "train loss:0.000433574445423292\n",
      "train loss:0.0018968622041686666\n",
      "train loss:8.009079742885345e-05\n",
      "train loss:8.181947412114882e-05\n",
      "train loss:0.004884512237148381\n",
      "train loss:0.03475062341593085\n",
      "train loss:2.13034729977671e-05\n",
      "train loss:0.0016714117059363415\n",
      "train loss:0.0006064599242302066\n",
      "train loss:0.0011013939596182006\n",
      "train loss:0.0010281721137489017\n",
      "train loss:0.0007753253132278176\n",
      "train loss:0.0029000187163661676\n",
      "train loss:0.0009709376842850951\n",
      "train loss:0.001816257572148868\n",
      "train loss:0.001211583146587838\n",
      "train loss:0.0006854896858174592\n",
      "train loss:0.016019349980353346\n",
      "train loss:0.0012314751014392334\n",
      "train loss:0.002773716130752407\n",
      "train loss:0.00024172850653513613\n",
      "train loss:0.0014263683001527068\n",
      "train loss:0.0005392325731562182\n",
      "train loss:0.0011390262339230016\n",
      "train loss:0.00172944450641114\n",
      "train loss:0.00043859764683244507\n",
      "train loss:0.0011593659521202504\n",
      "train loss:0.00018161653749324008\n",
      "train loss:0.0006231462536100225\n",
      "train loss:0.0006925818003187191\n",
      "train loss:2.5478990705034813e-05\n",
      "train loss:0.0004397446238774531\n",
      "train loss:0.00025273642340760904\n",
      "train loss:0.0017209828714597614\n",
      "train loss:0.0005842876645563008\n",
      "train loss:8.41683827209574e-05\n",
      "train loss:0.00014427684151565033\n",
      "train loss:0.0010761825655316347\n",
      "train loss:0.0009069345112219103\n",
      "train loss:0.00027696019675812144\n",
      "train loss:0.002068873756589143\n",
      "train loss:0.020201210845475166\n",
      "train loss:0.0008233884995380842\n",
      "train loss:4.570930010745936e-05\n",
      "train loss:0.0007669690099305263\n",
      "train loss:0.0008660769064638754\n",
      "train loss:0.00037362489944707354\n",
      "train loss:0.0004447171199136672\n",
      "train loss:0.001466678234791559\n",
      "train loss:0.0017311922112457335\n",
      "train loss:0.0022719786421621277\n",
      "train loss:0.0028284632244918676\n",
      "train loss:0.0018038903804461024\n",
      "train loss:0.0005699663621036063\n",
      "train loss:0.0011523638600744598\n",
      "train loss:0.001194533458857855\n",
      "train loss:0.00020964288801731667\n",
      "train loss:9.618227346943562e-05\n",
      "train loss:0.00010432715117216054\n",
      "train loss:0.001258793209755115\n",
      "train loss:0.0022525908640137486\n",
      "train loss:0.019072793484017744\n",
      "train loss:0.006310466794598488\n",
      "train loss:0.0004336863651108358\n",
      "train loss:0.0020036996495775296\n",
      "train loss:0.0029874928589431\n",
      "train loss:0.0013939250364978732\n",
      "train loss:0.0020281435565274544\n",
      "train loss:0.0004403667972491306\n",
      "train loss:0.0007475083263424169\n",
      "train loss:0.0036386146751532116\n",
      "train loss:0.007890788549427794\n",
      "train loss:0.0009745656779323642\n",
      "train loss:0.002088666608914956\n",
      "train loss:0.0007293764391911055\n",
      "train loss:0.001857691886724595\n",
      "train loss:0.0015619602791504877\n",
      "train loss:0.0006899084055614722\n",
      "train loss:0.0006674575309292448\n",
      "train loss:0.000541264997040077\n",
      "train loss:0.0016738338088920176\n",
      "train loss:0.001543696425738206\n",
      "train loss:0.002744389127068845\n",
      "train loss:0.0015592477144881183\n",
      "train loss:0.00015490240541379027\n",
      "train loss:0.05807879686516909\n",
      "train loss:0.0014746995563744883\n",
      "train loss:0.0003837751747417753\n",
      "train loss:0.0001214288028788606\n",
      "train loss:0.0008805758043749343\n",
      "train loss:0.0007650184462491045\n",
      "train loss:0.0003752204196735071\n",
      "train loss:0.0009838105937239217\n",
      "train loss:0.0022022292364989966\n",
      "train loss:5.9745424888290693e-05\n",
      "train loss:8.888738966096576e-05\n",
      "train loss:0.00017499572957286217\n",
      "train loss:0.0009866421386549211\n",
      "train loss:3.2127062993925295e-05\n",
      "train loss:0.0003433894940102274\n",
      "train loss:0.005027637504921082\n",
      "train loss:0.001005209944766657\n",
      "train loss:0.0035579202152731387\n",
      "train loss:0.0004549574042508605\n",
      "train loss:0.0016179620929598338\n",
      "train loss:0.0007106644980336225\n",
      "train loss:0.00012048440978568766\n",
      "train loss:0.0011779821905223186\n",
      "train loss:7.987784881598898e-05\n",
      "train loss:0.010002248454417075\n",
      "train loss:0.000603990454795041\n",
      "train loss:0.00027543741415641805\n",
      "train loss:0.0006597487279384122\n",
      "train loss:0.0013436646624971297\n",
      "train loss:0.0004710694735158446\n",
      "train loss:0.00016327266539455497\n",
      "train loss:0.0007635102532972761\n",
      "train loss:0.0007060513223493409\n",
      "train loss:0.00413728206178691\n",
      "train loss:0.002950649435849814\n",
      "train loss:0.0002559777455711246\n",
      "train loss:0.00013742458287785487\n",
      "train loss:0.0014789571348551224\n",
      "train loss:0.02636363463329088\n",
      "train loss:0.00015144607938384223\n",
      "train loss:0.0006744122785575392\n",
      "train loss:0.0003591441498500911\n",
      "train loss:0.004890098764590238\n",
      "train loss:0.00028279228791414703\n",
      "train loss:0.0030766625316440443\n",
      "train loss:0.002584292152479454\n",
      "train loss:0.0015584138345577758\n",
      "train loss:0.0008544614716951341\n",
      "train loss:0.001918135991647419\n",
      "train loss:0.0002346179524882129\n",
      "train loss:0.00021755453469339686\n",
      "train loss:0.0009092005805007891\n",
      "train loss:0.0019186462372636074\n",
      "train loss:0.00033462808889627707\n",
      "train loss:0.001176070314845197\n",
      "train loss:0.006593954005267428\n",
      "train loss:0.0009862082420005421\n",
      "train loss:0.0019104813238833057\n",
      "train loss:0.003286863701820478\n",
      "train loss:0.0005235680410829844\n",
      "train loss:0.00040028047741406107\n",
      "train loss:0.0011236483038331968\n",
      "train loss:0.00030282340440030515\n",
      "train loss:0.005838997526398629\n",
      "train loss:0.009858463880873558\n",
      "train loss:0.0020807375097841273\n",
      "train loss:0.0017632768319683209\n",
      "train loss:0.00018256493348897597\n",
      "train loss:0.0007484867934976243\n",
      "train loss:0.0001324627721639265\n",
      "train loss:0.006341242532334643\n",
      "train loss:0.004112313218645539\n",
      "train loss:0.0009490193840205593\n",
      "train loss:0.00033026747717548844\n",
      "train loss:0.002112142310868862\n",
      "train loss:5.8344133210781275e-05\n",
      "train loss:0.0016899388771802596\n",
      "train loss:0.00027118758334083125\n",
      "train loss:0.001525009742736406\n",
      "train loss:0.0004888489938430874\n",
      "train loss:0.0012346363488289125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0011642655481180746\n",
      "train loss:0.0008559227680292361\n",
      "train loss:0.0018277023135991916\n",
      "train loss:0.0013441038556918977\n",
      "train loss:0.002826286243991105\n",
      "train loss:0.021180011919408773\n",
      "train loss:0.00047457798892524156\n",
      "train loss:0.002855812231182756\n",
      "train loss:0.0013700867170169321\n",
      "train loss:0.0003127522311582683\n",
      "train loss:0.0009814869093437689\n",
      "train loss:0.002142679390649294\n",
      "train loss:0.0003819697305896977\n",
      "train loss:0.0014881536565018666\n",
      "train loss:0.006686979939178906\n",
      "train loss:0.0016085121294778487\n",
      "train loss:0.006980322836798063\n",
      "train loss:0.0009197835912988699\n",
      "train loss:0.000553526222525074\n",
      "train loss:0.0002058602937739893\n",
      "train loss:0.0010957814766770757\n",
      "train loss:0.0004653386848609909\n",
      "train loss:0.0005237228277354344\n",
      "train loss:0.004166939673073088\n",
      "train loss:0.0025155680367060984\n",
      "train loss:0.0009908289890667129\n",
      "train loss:0.0010806057970731728\n",
      "train loss:0.003841066347176744\n",
      "train loss:0.007859250904390672\n",
      "train loss:0.0017147451373469\n",
      "train loss:0.0002918559011945132\n",
      "train loss:0.0003499761400826147\n",
      "train loss:0.0005436145991168382\n",
      "train loss:0.004248522932016169\n",
      "train loss:0.00033310860361384037\n",
      "train loss:0.0016769727754904452\n",
      "train loss:0.000775784062695911\n",
      "train loss:0.0008266861186179514\n",
      "train loss:0.002048734133204284\n",
      "train loss:0.0008380198897147057\n",
      "train loss:0.00028559179109873635\n",
      "train loss:0.00027871865211849794\n",
      "train loss:0.0011274472420971045\n",
      "train loss:0.002760256967360798\n",
      "train loss:0.002433158651665571\n",
      "train loss:0.0015618439713042706\n",
      "train loss:0.0009805617449165592\n",
      "train loss:0.000356423368458176\n",
      "train loss:0.0002598350075458122\n",
      "train loss:0.002137254031218004\n",
      "train loss:0.0002498540792567317\n",
      "train loss:0.002737771738340252\n",
      "train loss:0.00034848808139243076\n",
      "train loss:0.00011546718383539879\n",
      "train loss:0.0013885381046899689\n",
      "train loss:0.0003411447769853511\n",
      "train loss:0.0006130261690655976\n",
      "train loss:0.000335895976113308\n",
      "train loss:0.00039650469199571404\n",
      "train loss:0.0019522167667013115\n",
      "train loss:0.001418199633427728\n",
      "train loss:0.0002363941013538358\n",
      "train loss:0.002323140452019855\n",
      "train loss:0.0003622704775856064\n",
      "train loss:0.0003959921807293179\n",
      "train loss:0.0010847977467729197\n",
      "train loss:0.005055536370810164\n",
      "train loss:0.0005233478529578853\n",
      "train loss:0.004230783922371751\n",
      "train loss:0.00016179353890465772\n",
      "train loss:0.0004156992716565238\n",
      "train loss:0.00028908739640079716\n",
      "train loss:0.0017499298734439664\n",
      "train loss:0.00029557278042407716\n",
      "train loss:0.00032420175940581485\n",
      "train loss:0.0020478503489827727\n",
      "train loss:4.610241565354459e-05\n",
      "train loss:0.0004963518746676943\n",
      "train loss:0.000291546815372729\n",
      "train loss:0.0008107018898814932\n",
      "train loss:0.0006430188265421913\n",
      "train loss:0.0005263445165638394\n",
      "train loss:0.000154647628688999\n",
      "train loss:0.026519137782590237\n",
      "train loss:0.006999982766905912\n",
      "train loss:0.02139593749858575\n",
      "train loss:7.755762958038362e-05\n",
      "train loss:0.0005757831766629183\n",
      "train loss:2.383368143339816e-05\n",
      "train loss:3.231617610685843e-05\n",
      "train loss:0.0014662647277682793\n",
      "train loss:0.0024971803078644313\n",
      "train loss:0.0003532033349436557\n",
      "train loss:0.00021706654657206785\n",
      "train loss:0.0025288297204581916\n",
      "train loss:0.0016818110714387573\n",
      "train loss:0.0175032711713987\n",
      "train loss:0.0015462350426075624\n",
      "train loss:8.289928740749702e-05\n",
      "train loss:0.0021953640900029352\n",
      "train loss:8.558889715143879e-05\n",
      "train loss:0.001057489010007134\n",
      "train loss:0.00011907242933272606\n",
      "train loss:0.0012278067178706574\n",
      "train loss:0.0005358262638227647\n",
      "train loss:0.0007724680829624944\n",
      "train loss:0.0003716394316144978\n",
      "train loss:0.00023922224119591863\n",
      "train loss:0.0030436907936648634\n",
      "train loss:0.0010419725937438127\n",
      "train loss:0.0032125939618044856\n",
      "train loss:0.000613903343787354\n",
      "train loss:0.003746981216863285\n",
      "train loss:0.0015544167142452917\n",
      "train loss:0.0011841251964057398\n",
      "train loss:0.0011547570955860965\n",
      "train loss:0.004397650769635801\n",
      "train loss:0.0006706930315282397\n",
      "train loss:0.0002411334253152857\n",
      "train loss:0.0007010419526713065\n",
      "train loss:0.0007043075480530323\n",
      "train loss:0.004500525710322702\n",
      "train loss:0.00041809845067173224\n",
      "train loss:0.0017166622849938624\n",
      "train loss:0.0002393879604624956\n",
      "train loss:0.0012536429756153694\n",
      "train loss:0.00034607685629228937\n",
      "train loss:0.00015381757106228515\n",
      "train loss:0.0001349964827040448\n",
      "train loss:0.0012708712992941945\n",
      "train loss:0.0018524388039890852\n",
      "train loss:0.00048624046375245455\n",
      "train loss:0.00033011178929068306\n",
      "train loss:0.0008955824874455785\n",
      "train loss:0.0021817474671143974\n",
      "train loss:0.0025018289274431615\n",
      "train loss:0.0011925252212856997\n",
      "train loss:0.0006554848044040247\n",
      "train loss:0.002120482739268291\n",
      "train loss:0.00018403982525122154\n",
      "train loss:0.00017461055329263345\n",
      "train loss:5.930841220332831e-05\n",
      "train loss:0.0004432443208735106\n",
      "train loss:0.00019201934539750246\n",
      "train loss:0.0005851649665369172\n",
      "train loss:0.0028250541896882243\n",
      "train loss:0.0019344811310573842\n",
      "train loss:0.0009664213552088068\n",
      "train loss:0.0013973419427397887\n",
      "train loss:0.0004282181535551599\n",
      "train loss:0.001957318934928291\n",
      "train loss:0.00019503932248395305\n",
      "train loss:0.001748804576156386\n",
      "train loss:0.0008245073254059121\n",
      "train loss:0.00040482982975327374\n",
      "train loss:0.0005971502895100486\n",
      "train loss:2.1547417193457737e-05\n",
      "train loss:0.004702636104567805\n",
      "train loss:0.00219265496201122\n",
      "train loss:0.00018972896277622806\n",
      "train loss:0.00044774407378666\n",
      "train loss:0.032126089254705656\n",
      "train loss:0.0028735857627355133\n",
      "train loss:0.0021703354518303576\n",
      "train loss:0.00021888552632337926\n",
      "train loss:0.0009109070843665347\n",
      "train loss:9.512404406499714e-05\n",
      "train loss:0.005345446227135153\n",
      "train loss:0.0007854152827718586\n",
      "train loss:0.0013602967224623991\n",
      "train loss:0.002296115580211025\n",
      "train loss:0.00033989495414072084\n",
      "train loss:0.000142193683997216\n",
      "train loss:0.01001950735553261\n",
      "train loss:0.0014082603195482049\n",
      "train loss:0.00021340444027305425\n",
      "train loss:0.0009165853020714727\n",
      "train loss:7.760171894643052e-05\n",
      "train loss:0.0009335647051719647\n",
      "train loss:0.0008765418606041904\n",
      "train loss:0.0032764003467952918\n",
      "train loss:0.0009333196800268685\n",
      "train loss:0.0014261425077918854\n",
      "train loss:0.0009188725154436392\n",
      "train loss:0.0006354432571453678\n",
      "train loss:0.0010122249180501597\n",
      "train loss:6.080722046808996e-05\n",
      "train loss:0.0003511802017942628\n",
      "train loss:0.00014273247519159675\n",
      "train loss:0.00024197626463478642\n",
      "train loss:0.0007524850719156862\n",
      "train loss:0.001849655117305555\n",
      "train loss:0.002423653382261436\n",
      "train loss:0.00031435336225338486\n",
      "train loss:0.0010092406470780663\n",
      "train loss:0.0010563290679472663\n",
      "train loss:4.046188071202551e-05\n",
      "train loss:0.0017042074237692867\n",
      "train loss:0.0025739455094151075\n",
      "train loss:0.0005947344049093537\n",
      "train loss:5.810546183407618e-05\n",
      "train loss:0.00025391446815889993\n",
      "train loss:7.559838200487911e-05\n",
      "train loss:6.123067260158674e-05\n",
      "train loss:0.00015333115032287382\n",
      "train loss:0.00016062878166712058\n",
      "train loss:0.0002575694589133688\n",
      "train loss:0.0028365734824608237\n",
      "train loss:0.0004985476840889473\n",
      "train loss:0.0023340934731142446\n",
      "train loss:7.994251468966067e-05\n",
      "train loss:0.00017896787772359285\n",
      "train loss:0.0009591398120289637\n",
      "train loss:0.00046658126542383947\n",
      "train loss:0.000489722248771219\n",
      "train loss:0.0001567252965492419\n",
      "train loss:0.001962928489016664\n",
      "train loss:6.636089564262366e-05\n",
      "train loss:0.0018916096368375294\n",
      "train loss:0.0004556611009550606\n",
      "train loss:0.010236117465339185\n",
      "train loss:0.0006320574934963221\n",
      "train loss:0.0012118288943635338\n",
      "train loss:0.0004395840204924702\n",
      "train loss:0.0003004982668794717\n",
      "train loss:0.0003922361575805266\n",
      "train loss:0.02016837836803326\n",
      "train loss:0.004247506122238733\n",
      "train loss:0.0009043591179479361\n",
      "train loss:0.0010450100825196889\n",
      "train loss:0.00021311063918720856\n",
      "train loss:0.00019568945477950068\n",
      "train loss:0.008043286022993503\n",
      "train loss:0.0001657015066083216\n",
      "train loss:0.004771704522422018\n",
      "train loss:0.0003453005566085457\n",
      "train loss:0.00041036502652774836\n",
      "train loss:0.0019004210321406049\n",
      "train loss:0.002212082645214918\n",
      "train loss:0.0008921069964661349\n",
      "train loss:0.0022057795061703332\n",
      "train loss:0.0002572695636348202\n",
      "train loss:0.0056248738331201205\n",
      "train loss:0.0008070459924889846\n",
      "train loss:0.0023254512665703757\n",
      "train loss:0.0005458596065950932\n",
      "train loss:0.00843367549889936\n",
      "train loss:0.003995867771332318\n",
      "train loss:0.0011628177755728596\n",
      "train loss:0.002181741500582455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.001242211848928163\n",
      "train loss:0.0004902878155257976\n",
      "train loss:0.0016287820855819582\n",
      "train loss:0.00012068566252581696\n",
      "train loss:0.0006369962550320918\n",
      "train loss:0.00030943217675156385\n",
      "train loss:0.00041920567174025936\n",
      "train loss:0.0029919577427811594\n",
      "train loss:0.00010885763910862284\n",
      "train loss:0.00398494595444951\n",
      "train loss:0.0016674075371461206\n",
      "train loss:0.002103181894066752\n",
      "train loss:0.0004707468530311312\n",
      "train loss:0.0022848596241287753\n",
      "train loss:0.00012105272650022584\n",
      "train loss:0.0002331301441369413\n",
      "train loss:0.0013234966088429242\n",
      "train loss:0.0001921110841295683\n",
      "train loss:0.0010635890221769042\n",
      "train loss:0.00010586692004103971\n",
      "train loss:0.029020017040757472\n",
      "train loss:1.2264051427087588e-05\n",
      "train loss:0.005406920899840088\n",
      "train loss:0.00045918413769509856\n",
      "train loss:0.0016196648733463306\n",
      "train loss:0.0001386530556755907\n",
      "train loss:0.0005038553465277025\n",
      "train loss:0.0055130910317027215\n",
      "train loss:0.0009725327009850449\n",
      "=== epoch:19, train acc:0.999, test acc:0.985 ===\n",
      "train loss:0.0030322818231867804\n",
      "train loss:0.00222791396747765\n",
      "train loss:0.0013767413711054436\n",
      "train loss:0.0008581397446306326\n",
      "train loss:0.016920948109338578\n",
      "train loss:3.2987017472002626e-05\n",
      "train loss:0.0002600128270005517\n",
      "train loss:0.00019730993074068508\n",
      "train loss:0.0011343011619972228\n",
      "train loss:0.0010207948926775066\n",
      "train loss:0.008002937615728088\n",
      "train loss:0.0018090977206204843\n",
      "train loss:0.00048582158896744816\n",
      "train loss:0.004444645897308221\n",
      "train loss:0.00031625341933045974\n",
      "train loss:0.0005023618480269816\n",
      "train loss:0.0002990720978915194\n",
      "train loss:0.003291526947610927\n",
      "train loss:2.760062270474651e-05\n",
      "train loss:0.00035673953542469017\n",
      "train loss:0.0005221505574206412\n",
      "train loss:0.0020289720867470206\n",
      "train loss:0.0024847653484648645\n",
      "train loss:0.0006609985732312926\n",
      "train loss:0.008774929136316618\n",
      "train loss:6.176114914853908e-05\n",
      "train loss:0.0021073587975715398\n",
      "train loss:0.0008031167125687927\n",
      "train loss:0.0006415061723990157\n",
      "train loss:0.00024059242893461117\n",
      "train loss:0.0018407039982797524\n",
      "train loss:2.2467548358328353e-05\n",
      "train loss:0.0038787332030313106\n",
      "train loss:0.00035689560911457787\n",
      "train loss:0.0012081896255274874\n",
      "train loss:0.0021733739600835727\n",
      "train loss:0.00314443463755865\n",
      "train loss:0.0018225443400364302\n",
      "train loss:0.0004031067247667824\n",
      "train loss:0.0007908103554586728\n",
      "train loss:0.0003757037036466041\n",
      "train loss:0.000293265681168961\n",
      "train loss:0.009088503311288488\n",
      "train loss:0.0018973822999147614\n",
      "train loss:0.0034797936703738637\n",
      "train loss:0.003730176981403861\n",
      "train loss:0.040706742734772046\n",
      "train loss:0.0005622266625251455\n",
      "train loss:0.00617946149620911\n",
      "train loss:0.0009659188545863095\n",
      "train loss:0.002344918697933775\n",
      "train loss:0.0012216513725354734\n",
      "train loss:0.00022012073857528042\n",
      "train loss:2.0684891247995653e-05\n",
      "train loss:0.00019615537790737433\n",
      "train loss:0.0029259126831658842\n",
      "train loss:0.002949117883890422\n",
      "train loss:6.435582114257355e-05\n",
      "train loss:0.002781810009289506\n",
      "train loss:0.00032116064159664004\n",
      "train loss:0.0009106101681610318\n",
      "train loss:0.002125253290956978\n",
      "train loss:0.00034097399977301595\n",
      "train loss:1.0871390324572105e-05\n",
      "train loss:0.0006368072889397732\n",
      "train loss:0.00048006856771325157\n",
      "train loss:0.00012199079766976713\n",
      "train loss:0.004058360585349748\n",
      "train loss:0.001740126119475245\n",
      "train loss:0.0005525730489226915\n",
      "train loss:0.0036377676116619074\n",
      "train loss:0.0021512324136713423\n",
      "train loss:0.0003827735031901081\n",
      "train loss:0.0009967046616596788\n",
      "train loss:0.0007451912714681284\n",
      "train loss:0.0003889553984801097\n",
      "train loss:0.0027137249389431657\n",
      "train loss:0.0013130938751601568\n",
      "train loss:0.002867811087404358\n",
      "train loss:0.0002141016770733241\n",
      "train loss:0.0031611599009949594\n",
      "train loss:0.0009481450879059468\n",
      "train loss:0.0011166074588679926\n",
      "train loss:0.0005081773744650801\n",
      "train loss:0.00027444927299758283\n",
      "train loss:0.0006162162432558356\n",
      "train loss:0.0006247478650972547\n",
      "train loss:0.000574136338246473\n",
      "train loss:0.002879361604284014\n",
      "train loss:0.00017165384993177323\n",
      "train loss:0.00032338347110334666\n",
      "train loss:0.0003994128316659122\n",
      "train loss:0.0002739824153377002\n",
      "train loss:0.0004596076175173791\n",
      "train loss:0.0063077574728416476\n",
      "train loss:0.0001141663987538843\n",
      "train loss:0.0034840624735909347\n",
      "train loss:0.0008525666455045611\n",
      "train loss:0.00038180428581404134\n",
      "train loss:0.00028459148195630093\n",
      "train loss:0.001979630111989795\n",
      "train loss:0.0007473878450925341\n",
      "train loss:0.0002366487550196\n",
      "train loss:0.0025606326375677358\n",
      "train loss:0.00020920214670881995\n",
      "train loss:0.0003509547869860969\n",
      "train loss:0.0001740432272688738\n",
      "train loss:0.0033596215912526416\n",
      "train loss:0.0003992548060242028\n",
      "train loss:0.0003552245034410059\n",
      "train loss:0.0015671979123172764\n",
      "train loss:0.0016399055416018893\n",
      "train loss:0.00022722453910833568\n",
      "train loss:0.00015622420350389494\n",
      "train loss:3.3902299000357976e-05\n",
      "train loss:0.0022047318486281106\n",
      "train loss:0.0012499038631137225\n",
      "train loss:0.0029945360395441174\n",
      "train loss:0.00020744081915858589\n",
      "train loss:0.0034922579940813857\n",
      "train loss:0.0013522743588757562\n",
      "train loss:0.00031954560741528763\n",
      "train loss:0.0010000225171834544\n",
      "train loss:0.0008631226959250466\n",
      "train loss:0.005078204939619107\n",
      "train loss:0.00298855380823673\n",
      "train loss:0.0008411443729196129\n",
      "train loss:0.0016436886320466111\n",
      "train loss:0.00022733783629933033\n",
      "train loss:0.0018172463038761695\n",
      "train loss:0.0011393373273977723\n",
      "train loss:0.0032813138544097805\n",
      "train loss:0.0010671236773423937\n",
      "train loss:0.0013408846352832614\n",
      "train loss:0.00015383724641711378\n",
      "train loss:0.00011783302298662046\n",
      "train loss:0.0006734168762685428\n",
      "train loss:2.9596951143240577e-05\n",
      "train loss:0.003952954512367529\n",
      "train loss:0.0007579381052664508\n",
      "train loss:0.004121334545135446\n",
      "train loss:0.001339180884791796\n",
      "train loss:0.0001782998180759559\n",
      "train loss:0.0012210501027103094\n",
      "train loss:0.006603765707208543\n",
      "train loss:0.0019448174412644058\n",
      "train loss:0.0019220125966887536\n",
      "train loss:0.0010724499138973357\n",
      "train loss:0.005700577752867547\n",
      "train loss:0.00015873503989669894\n",
      "train loss:0.0012942081320311111\n",
      "train loss:9.60166929810927e-05\n",
      "train loss:0.0033758505495599138\n",
      "train loss:0.0013804665722803129\n",
      "train loss:0.0005795513123081132\n",
      "train loss:0.0007519531139804885\n",
      "train loss:0.0008364779173060963\n",
      "train loss:0.00016652248625310988\n",
      "train loss:0.0011172503584213722\n",
      "train loss:0.001809150356047786\n",
      "train loss:0.005642315144854645\n",
      "train loss:0.00028426821523981307\n",
      "train loss:0.0012534245790611941\n",
      "train loss:0.0006780305223939416\n",
      "train loss:0.00015872205661781583\n",
      "train loss:0.009739716618784693\n",
      "train loss:0.0012386328010375103\n",
      "train loss:0.0003957096702154261\n",
      "train loss:0.002821723718267632\n",
      "train loss:0.002898604969580905\n",
      "train loss:0.0005579324465021052\n",
      "train loss:0.005380911926565793\n",
      "train loss:0.0032352027846345678\n",
      "train loss:0.00016867524639656447\n",
      "train loss:0.0005261811726590648\n",
      "train loss:0.000595564272037744\n",
      "train loss:0.008087493095560538\n",
      "train loss:0.0009929666689243665\n",
      "train loss:0.0019333994511143017\n",
      "train loss:0.0012727146594157027\n",
      "train loss:0.0006872550609687448\n",
      "train loss:0.0036543476623718237\n",
      "train loss:0.00042852459283280146\n",
      "train loss:0.001200853592580053\n",
      "train loss:0.0012544516442618814\n",
      "train loss:0.0021408013059831792\n",
      "train loss:0.0006620594266743714\n",
      "train loss:0.0033949702811401374\n",
      "train loss:0.0019292099015793154\n",
      "train loss:0.000778105527261272\n",
      "train loss:0.00029206212291127973\n",
      "train loss:0.009868323488608317\n",
      "train loss:0.0004626744116017361\n",
      "train loss:5.7511036972235005e-05\n",
      "train loss:0.0009829555191125379\n",
      "train loss:0.0006725560150222003\n",
      "train loss:0.00025293884676691643\n",
      "train loss:7.241840726167906e-05\n",
      "train loss:0.004088214656594126\n",
      "train loss:0.00011279683826946137\n",
      "train loss:0.0002566208388117506\n",
      "train loss:0.0012762227786175422\n",
      "train loss:0.0001721648769166832\n",
      "train loss:7.82998525761122e-05\n",
      "train loss:0.0008043559142465977\n",
      "train loss:0.0009046685938348667\n",
      "train loss:8.570948394438241e-05\n",
      "train loss:0.0033250918664431616\n",
      "train loss:0.0005048198305851416\n",
      "train loss:0.0038569241805378146\n",
      "train loss:0.0003553608161945598\n",
      "train loss:0.0002920550880985246\n",
      "train loss:0.00011666055767277921\n",
      "train loss:0.0017607069458637995\n",
      "train loss:0.0001037999400978832\n",
      "train loss:0.0008183825130888788\n",
      "train loss:0.0004026683748157205\n",
      "train loss:0.000654566354529141\n",
      "train loss:0.00029482844420558244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.003120055734711793\n",
      "train loss:0.006021260300140123\n",
      "train loss:4.1732369567316724e-05\n",
      "train loss:0.0007893113717346681\n",
      "train loss:0.0007106292255489681\n",
      "train loss:0.0015070764693797432\n",
      "train loss:0.0004173390411430728\n",
      "train loss:0.00032929300238973536\n",
      "train loss:0.000442467171823435\n",
      "train loss:0.0022152307994541816\n",
      "train loss:0.0005389727982753258\n",
      "train loss:0.0006922158953693639\n",
      "train loss:0.0017029092864020523\n",
      "train loss:0.003422827436989876\n",
      "train loss:0.0020783351069311395\n",
      "train loss:0.00030862516861686286\n",
      "train loss:0.0001581465293811466\n",
      "train loss:0.0013755684255706718\n",
      "train loss:0.001814083767816809\n",
      "train loss:0.0010279227553405162\n",
      "train loss:0.0014776944855223334\n",
      "train loss:0.0006367704808400418\n",
      "train loss:0.0019267134342533835\n",
      "train loss:0.0009528934566174733\n",
      "train loss:0.00019917060034063563\n",
      "train loss:0.0020040238767065613\n",
      "train loss:0.002640315437070515\n",
      "train loss:0.0008375725011331178\n",
      "train loss:0.0011622020759100326\n",
      "train loss:0.0012170309116665801\n",
      "train loss:0.0006661475576444976\n",
      "train loss:0.0027185871936778656\n",
      "train loss:0.00062880638164987\n",
      "train loss:0.062357298428136414\n",
      "train loss:0.0019800367486294784\n",
      "train loss:0.0005966098554609458\n",
      "train loss:0.001521432987293047\n",
      "train loss:3.763500288929315e-05\n",
      "train loss:4.3858898455738294e-05\n",
      "train loss:0.0006430694414165008\n",
      "train loss:5.948788607338105e-05\n",
      "train loss:0.00043765004001087855\n",
      "train loss:0.0017982251402082956\n",
      "train loss:0.003234729993821174\n",
      "train loss:0.0023727477112707577\n",
      "train loss:0.00446185489650817\n",
      "train loss:0.0013835061366537285\n",
      "train loss:0.003653218068091352\n",
      "train loss:0.0040345009698939385\n",
      "train loss:0.0005521090612079523\n",
      "train loss:5.802294243095064e-05\n",
      "train loss:0.005448129063886917\n",
      "train loss:0.010123755537078616\n",
      "train loss:0.00033343658196828394\n",
      "train loss:3.0131322959371004e-05\n",
      "train loss:0.0013463071565301854\n",
      "train loss:0.00188139798539294\n",
      "train loss:0.0012702761825609781\n",
      "train loss:0.004204306660246298\n",
      "train loss:0.0020241205598156243\n",
      "train loss:0.00036812820688443673\n",
      "train loss:0.0029581626984060854\n",
      "train loss:0.007978307002187468\n",
      "train loss:0.0009777677401938743\n",
      "train loss:1.6613372212434954e-05\n",
      "train loss:0.00040299261676072026\n",
      "train loss:0.003396199672613943\n",
      "train loss:4.367788957687685e-05\n",
      "train loss:0.0018530818461597062\n",
      "train loss:0.00011323567200053513\n",
      "train loss:0.00011143682547909558\n",
      "train loss:0.0004057879346936288\n",
      "train loss:0.0013368020859450356\n",
      "train loss:0.0006468697609869627\n",
      "train loss:0.0012989397653769494\n",
      "train loss:0.0010518683453828402\n",
      "train loss:0.001669218161044636\n",
      "train loss:0.0011807002399800773\n",
      "train loss:0.0031547562749016955\n",
      "train loss:0.0005666510076385911\n",
      "train loss:0.0013891250474765584\n",
      "train loss:0.0008351066062064729\n",
      "train loss:0.0015422930424027017\n",
      "train loss:0.00040253763612337914\n",
      "train loss:0.00035327847065991865\n",
      "train loss:0.00022525168563406335\n",
      "train loss:0.00011568753627941289\n",
      "train loss:0.00660758944094739\n",
      "train loss:0.0004740772525395065\n",
      "train loss:0.0011227403041604678\n",
      "train loss:0.0008766821790656528\n",
      "train loss:0.00011405875472457866\n",
      "train loss:0.0009433986369196788\n",
      "train loss:0.0022535685093488817\n",
      "train loss:4.601195774878155e-05\n",
      "train loss:0.0005740832211790844\n",
      "train loss:0.0017457557773689276\n",
      "train loss:0.0005256808153272217\n",
      "train loss:0.001040238543390044\n",
      "train loss:3.0608276180079656e-05\n",
      "train loss:0.0003210466210482569\n",
      "train loss:0.0018708390737817565\n",
      "train loss:0.003657399331081467\n",
      "train loss:0.014782021006051322\n",
      "train loss:0.0005949782587374344\n",
      "train loss:0.001984433934837989\n",
      "train loss:0.00146748538620435\n",
      "train loss:0.000990686693213865\n",
      "train loss:0.0009780167698832832\n",
      "train loss:0.0007500848483739043\n",
      "train loss:0.0009514698895790174\n",
      "train loss:0.002260111029429333\n",
      "train loss:0.0009789859728882796\n",
      "train loss:0.002053294609460981\n",
      "train loss:0.0029465761640121786\n",
      "train loss:0.006603720915566516\n",
      "train loss:0.028586704065480317\n",
      "train loss:0.0012621517900772187\n",
      "train loss:0.0007858780344374328\n",
      "train loss:0.0007722214104146248\n",
      "train loss:0.0003572949697062543\n",
      "train loss:0.0025445638733614124\n",
      "train loss:0.0003720712087301304\n",
      "train loss:0.0002333734792640134\n",
      "train loss:0.00015682470854024456\n",
      "train loss:2.5523048385212652e-05\n",
      "train loss:0.0031047310836089448\n",
      "train loss:0.0027821404008312217\n",
      "train loss:0.0026964229185003903\n",
      "train loss:0.007750090818780597\n",
      "train loss:0.004370392133179833\n",
      "train loss:0.0033299196887929456\n",
      "train loss:0.001548055236491004\n",
      "train loss:0.0004650157631887359\n",
      "train loss:0.0016375823034136421\n",
      "train loss:0.0015487548985711444\n",
      "train loss:0.0001387321783431258\n",
      "train loss:0.002782143335483557\n",
      "train loss:0.0039150834948314655\n",
      "train loss:0.0019095070833858607\n",
      "train loss:0.0008817690355942752\n",
      "train loss:0.004902209189780356\n",
      "train loss:0.0005450615014200066\n",
      "train loss:0.0012168547179501733\n",
      "train loss:0.0030250391755041865\n",
      "train loss:0.0024845892953601124\n",
      "train loss:0.0018758901530107589\n",
      "train loss:0.0021977193239633933\n",
      "train loss:0.000698625178999799\n",
      "train loss:0.00010701248718070218\n",
      "train loss:0.0017378205229181152\n",
      "train loss:0.002791190047779621\n",
      "train loss:0.00030224727009675384\n",
      "train loss:0.0021938791977151747\n",
      "train loss:0.0010252104074404543\n",
      "train loss:0.004814588288086497\n",
      "train loss:0.0010491323774083133\n",
      "train loss:0.0004878698617721144\n",
      "train loss:7.115869016375191e-05\n",
      "train loss:0.0023059727452063965\n",
      "train loss:0.029300254451823174\n",
      "train loss:0.003604472392525458\n",
      "train loss:0.0024105475374042056\n",
      "train loss:0.0010194385346717464\n",
      "train loss:0.001299188344206227\n",
      "train loss:0.0021724779733359443\n",
      "train loss:0.004125112420451054\n",
      "train loss:3.567296996223811e-05\n",
      "train loss:7.541130415851451e-05\n",
      "train loss:5.597817985008401e-05\n",
      "train loss:0.0021332719359554555\n",
      "train loss:0.001213698843841226\n",
      "train loss:0.0010276466579840625\n",
      "train loss:0.012042195130189549\n",
      "train loss:0.0009360261067257002\n",
      "train loss:0.0016990338944805102\n",
      "train loss:0.0009174250958493711\n",
      "train loss:0.018906638335973093\n",
      "train loss:0.0003073403578756602\n",
      "train loss:0.004490839196044813\n",
      "train loss:0.0011922004289330807\n",
      "train loss:0.0012295180430212333\n",
      "train loss:0.0023930812854785098\n",
      "train loss:0.00012192838472853892\n",
      "train loss:0.0003985683391322572\n",
      "train loss:0.0005540602134626485\n",
      "train loss:0.0015145613797021317\n",
      "train loss:0.00017892880034650206\n",
      "train loss:0.0005092527062300686\n",
      "train loss:0.000412781330037098\n",
      "train loss:0.0010589635463286967\n",
      "train loss:0.0010800116314634504\n",
      "train loss:0.006147617503856223\n",
      "train loss:0.0020007264793644396\n",
      "train loss:0.00672940758844627\n",
      "train loss:0.00029841464805810603\n",
      "train loss:0.000547351890434845\n",
      "train loss:0.0028827389246957845\n",
      "train loss:0.0005500659022610525\n",
      "train loss:0.024836451784099177\n",
      "train loss:0.004922861173757222\n",
      "train loss:0.0009003255514986742\n",
      "train loss:0.002342461727403626\n",
      "train loss:0.004398735426858509\n",
      "train loss:0.0029804602947687475\n",
      "train loss:0.000985545866205768\n",
      "train loss:0.0022623653933033934\n",
      "train loss:0.0025493728672167297\n",
      "train loss:0.0013966421530417833\n",
      "train loss:0.0032112983368023755\n",
      "train loss:0.0005941989464662113\n",
      "train loss:0.0010156318450743288\n",
      "train loss:0.0005025671268009592\n",
      "train loss:0.001153529674661029\n",
      "train loss:0.005039602211587046\n",
      "train loss:0.007991578341777064\n",
      "train loss:0.00086065132227353\n",
      "train loss:0.007789662760636779\n",
      "train loss:0.0007317756461614781\n",
      "train loss:0.0005114976708808867\n",
      "train loss:0.0003990958449484283\n",
      "train loss:0.00015437084581401852\n",
      "train loss:0.0004626101460406713\n",
      "train loss:0.0014234449532605131\n",
      "train loss:0.0003597764679926847\n",
      "train loss:0.011848080514155025\n",
      "train loss:0.001336660755818316\n",
      "train loss:0.0015732917530188956\n",
      "train loss:0.00024947099910328714\n",
      "train loss:0.0023557172491745905\n",
      "train loss:0.002936604235152869\n",
      "train loss:0.00024996670141832816\n",
      "train loss:0.0005629550744151827\n",
      "train loss:0.0010686811292761316\n",
      "train loss:0.0018082667490278653\n",
      "train loss:0.0009315741161272753\n",
      "train loss:0.0026435679029114608\n",
      "train loss:0.0009616135305208047\n",
      "train loss:0.0008992465554954545\n",
      "train loss:0.022633922640519444\n",
      "train loss:0.002586604337354455\n",
      "train loss:0.00469310624780913\n",
      "train loss:0.001209934008470418\n",
      "train loss:0.0006070265746405956\n",
      "train loss:0.0015099878502958905\n",
      "train loss:0.005672775458369167\n",
      "train loss:0.004946491331480297\n",
      "train loss:0.000586649407323159\n",
      "train loss:0.019826195942656964\n",
      "train loss:1.0786065435666114e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0013857359538839526\n",
      "train loss:0.0003110413417656436\n",
      "train loss:0.00601803222707516\n",
      "train loss:0.0020146637897045057\n",
      "train loss:0.00030422490483810247\n",
      "train loss:0.02385000403989197\n",
      "train loss:0.007252340475591046\n",
      "train loss:0.00019169446661211002\n",
      "train loss:0.0031459053608038816\n",
      "train loss:0.00038222870300136006\n",
      "train loss:0.0015268238993441314\n",
      "train loss:0.00015840753492535656\n",
      "train loss:0.0028447156107244908\n",
      "train loss:0.000894649157487207\n",
      "train loss:0.001421322200945776\n",
      "train loss:0.002144021058638412\n",
      "train loss:0.0009154997239909974\n",
      "train loss:0.001508335224854353\n",
      "train loss:5.309339241654658e-05\n",
      "train loss:0.0014627782901867844\n",
      "train loss:0.00024084811053661368\n",
      "train loss:0.007218183208183578\n",
      "train loss:0.0011140634951068771\n",
      "train loss:0.003110762261590935\n",
      "train loss:0.008141121192001517\n",
      "train loss:0.000981282356302402\n",
      "train loss:0.00015116607008131073\n",
      "train loss:0.009517499302542323\n",
      "train loss:0.001718493200578152\n",
      "train loss:7.719607607625322e-05\n",
      "train loss:0.00286888097155111\n",
      "train loss:5.9548673062344316e-05\n",
      "train loss:0.0018852628284296046\n",
      "train loss:0.0004986758843654792\n",
      "train loss:0.001552038295123691\n",
      "train loss:0.0034786815566141397\n",
      "train loss:0.0004985548382453478\n",
      "train loss:0.002485767544133561\n",
      "train loss:0.0008614323216110595\n",
      "train loss:0.0013134711037297929\n",
      "train loss:0.0028150362382587516\n",
      "train loss:0.0015015439221072505\n",
      "train loss:0.0010628459028952245\n",
      "train loss:0.0006114424257958233\n",
      "train loss:0.0003253598108749319\n",
      "train loss:0.0001703997869054031\n",
      "train loss:0.0008957162039214123\n",
      "train loss:0.000151729820619727\n",
      "train loss:7.186097822240706e-05\n",
      "train loss:0.003213072536163245\n",
      "train loss:0.0003595587343514273\n",
      "train loss:0.0008566161872070513\n",
      "train loss:0.0011128008163976825\n",
      "train loss:0.0024729397974751046\n",
      "train loss:0.0003112433677350082\n",
      "train loss:0.0012039605331046856\n",
      "train loss:0.0011602621438439034\n",
      "train loss:0.000273564204341419\n",
      "train loss:0.004425625387374403\n",
      "train loss:0.0018682635377937269\n",
      "train loss:0.0002585099034057684\n",
      "train loss:0.0037807090433896605\n",
      "train loss:3.028826552859372e-05\n",
      "train loss:8.111089154968819e-05\n",
      "train loss:0.00047594807079805906\n",
      "train loss:3.275495596889633e-05\n",
      "train loss:0.0013403074798919953\n",
      "train loss:0.002202142546228382\n",
      "train loss:0.00805032263895358\n",
      "train loss:7.569396792801238e-05\n",
      "train loss:0.0008146956249261825\n",
      "train loss:7.933806086402231e-05\n",
      "train loss:0.00016048018332473054\n",
      "train loss:0.0016641662932312654\n",
      "train loss:0.00033101621141263295\n",
      "train loss:0.0001917945499545792\n",
      "train loss:0.0013738106090416524\n",
      "train loss:0.002400996856804403\n",
      "train loss:0.0033518801862659573\n",
      "train loss:0.00030088113959297365\n",
      "train loss:0.0001482152382349749\n",
      "train loss:0.00014227755447882096\n",
      "train loss:0.0009343884592056812\n",
      "train loss:0.0012666530965253588\n",
      "train loss:0.00015929032885595565\n",
      "train loss:0.0030575335819531153\n",
      "train loss:0.0010539855355065862\n",
      "train loss:0.0011317722506518084\n",
      "train loss:0.001673054964622743\n",
      "train loss:0.005555325009847592\n",
      "train loss:0.0024599623636504917\n",
      "train loss:0.000744408979835623\n",
      "train loss:0.0008631013048926799\n",
      "train loss:0.00010889351234307797\n",
      "train loss:0.003372268707253715\n",
      "train loss:0.00010957017607033677\n",
      "train loss:0.0033041849911230044\n",
      "train loss:0.00041003710823193804\n",
      "train loss:0.0017209938695369523\n",
      "train loss:0.00032415133282314356\n",
      "train loss:0.0018950358186729127\n",
      "train loss:4.739019071779927e-05\n",
      "train loss:0.0010532363039533518\n",
      "train loss:0.00023476888080392136\n",
      "train loss:0.00021097282360827896\n",
      "train loss:0.00022142372325409214\n",
      "train loss:0.014677439581276873\n",
      "train loss:0.007387183180993415\n",
      "train loss:0.004497737810775558\n",
      "train loss:0.0011028677821512734\n",
      "train loss:0.00023544124404889833\n",
      "train loss:0.0007573522911863297\n",
      "train loss:0.0025654660144288255\n",
      "train loss:0.00036482931815033106\n",
      "train loss:0.00022588806435625878\n",
      "train loss:0.0007613096483648146\n",
      "train loss:2.7068457464287895e-05\n",
      "train loss:0.00017040326415946567\n",
      "train loss:0.00011786742556643855\n",
      "train loss:0.0004069738588208249\n",
      "train loss:0.0024825895580696516\n",
      "train loss:6.334715289693341e-05\n",
      "train loss:0.0003938086330879962\n",
      "train loss:0.0003475673768886352\n",
      "train loss:0.0011429493240079605\n",
      "train loss:0.00016413117281518973\n",
      "train loss:0.002764003382028451\n",
      "train loss:0.0011494286243690195\n",
      "train loss:6.436787489818321e-05\n",
      "train loss:5.412572366056457e-05\n",
      "train loss:0.0005336834551706913\n",
      "=== epoch:20, train acc:1.0, test acc:0.983 ===\n",
      "train loss:0.0008785757058375084\n",
      "train loss:2.5523112264890942e-05\n",
      "train loss:0.00010180106176861691\n",
      "train loss:0.0015654394436416088\n",
      "train loss:0.002553621965186288\n",
      "train loss:0.001728627625237076\n",
      "train loss:0.0014102491288985684\n",
      "train loss:6.603496687342395e-05\n",
      "train loss:0.00011545531378532837\n",
      "train loss:0.00015619417975506095\n",
      "train loss:0.0001967072607046134\n",
      "train loss:0.0003497599152748523\n",
      "train loss:0.00030874153553061575\n",
      "train loss:0.00015277468755414396\n",
      "train loss:0.0002895968229942912\n",
      "train loss:0.0004187557953321062\n",
      "train loss:0.001152388047923083\n",
      "train loss:0.0008406491060467521\n",
      "train loss:0.0015796609129566277\n",
      "train loss:0.0020569566085932136\n",
      "train loss:0.002784407046826513\n",
      "train loss:0.002242150485817546\n",
      "train loss:0.00020251898403446307\n",
      "train loss:4.7856535934625926e-05\n",
      "train loss:0.00164460564184243\n",
      "train loss:0.00023116810631577895\n",
      "train loss:1.832393321101365e-05\n",
      "train loss:0.0002943767793316263\n",
      "train loss:0.0014132294128586785\n",
      "train loss:0.0009175458933915224\n",
      "train loss:0.00011492199684354328\n",
      "train loss:0.0011488657986465966\n",
      "train loss:0.0012222548448657699\n",
      "train loss:0.0003466217068598701\n",
      "train loss:0.005220677457774357\n",
      "train loss:0.003155135222833996\n",
      "train loss:0.0007545893371249577\n",
      "train loss:0.0007086728468128048\n",
      "train loss:0.0017745038212987699\n",
      "train loss:0.0009284163449677477\n",
      "train loss:0.002916825257217511\n",
      "train loss:3.195457411128643e-05\n",
      "train loss:9.236935883736223e-05\n",
      "train loss:0.00018494239696767258\n",
      "train loss:0.001467329947244552\n",
      "train loss:0.00013732130743881354\n",
      "train loss:0.009778576086557926\n",
      "train loss:8.763356558908999e-05\n",
      "train loss:0.0010589676379570388\n",
      "train loss:0.002401275090681444\n",
      "train loss:0.0009099918311297094\n",
      "train loss:0.006221471305380879\n",
      "train loss:6.611858722761162e-05\n",
      "train loss:3.0040575126724267e-05\n",
      "train loss:0.0005843330276997202\n",
      "train loss:0.00076471099828581\n",
      "train loss:0.0012009180805107019\n",
      "train loss:0.00019019985478794458\n",
      "train loss:0.0032989305703396473\n",
      "train loss:5.208435119665359e-05\n",
      "train loss:0.001024712881227406\n",
      "train loss:0.023148146590897235\n",
      "train loss:0.0005517246020764711\n",
      "train loss:0.00020760839755973206\n",
      "train loss:0.00044761439819620955\n",
      "train loss:0.0004369433836944752\n",
      "train loss:0.0005311337326555197\n",
      "train loss:8.68673182284419e-05\n",
      "train loss:0.010459333007831988\n",
      "train loss:0.0008632560676596218\n",
      "train loss:0.00040332827975740235\n",
      "train loss:0.0024975035642274697\n",
      "train loss:0.0012124126835612117\n",
      "train loss:0.00036464826937371205\n",
      "train loss:0.00040187500253230005\n",
      "train loss:0.001887662937951975\n",
      "train loss:0.00010353499450833129\n",
      "train loss:0.001173711468170702\n",
      "train loss:0.001831252641195846\n",
      "train loss:8.417101765280354e-05\n",
      "train loss:0.000588290557599175\n",
      "train loss:0.0008515531768609395\n",
      "train loss:0.00299637971085077\n",
      "train loss:0.0006171338358572348\n",
      "train loss:0.0016293127207052077\n",
      "train loss:0.0009688993930598139\n",
      "train loss:0.0013674397588778395\n",
      "train loss:0.0008812364996934106\n",
      "train loss:0.0040130298621849315\n",
      "train loss:0.0004889323102477309\n",
      "train loss:0.00040206455111054044\n",
      "train loss:0.0017346436541765184\n",
      "train loss:0.0008691003641710123\n",
      "train loss:0.0005084500879548939\n",
      "train loss:0.0008973062785337997\n",
      "train loss:0.00225744947050865\n",
      "train loss:9.554104723023929e-05\n",
      "train loss:4.0500966203347346e-05\n",
      "train loss:0.0006364664642557734\n",
      "train loss:0.0015781607964966478\n",
      "train loss:0.0015369017734962514\n",
      "train loss:0.000854878260285369\n",
      "train loss:0.0020990276958186894\n",
      "train loss:9.447626649613737e-05\n",
      "train loss:0.000269757767126179\n",
      "train loss:0.0023363708635048925\n",
      "train loss:0.0005406683777934596\n",
      "train loss:0.0002958295147229473\n",
      "train loss:0.0005553278644120997\n",
      "train loss:0.0016290556009626379\n",
      "train loss:7.813317450505389e-05\n",
      "train loss:0.001182038259142602\n",
      "train loss:0.0005700156595605125\n",
      "train loss:0.0024807489379152655\n",
      "train loss:0.0024701508243904612\n",
      "train loss:0.00240528822208719\n",
      "train loss:0.005931150843715514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00037137282290795214\n",
      "train loss:0.0009925863046693116\n",
      "train loss:0.0024776921670284836\n",
      "train loss:0.00073729517635961\n",
      "train loss:0.00556606817760006\n",
      "train loss:0.0005245558532191155\n",
      "train loss:0.004497119652901824\n",
      "train loss:0.0015719996293957128\n",
      "train loss:0.0009190676634225199\n",
      "train loss:0.005774163049358251\n",
      "train loss:0.007156581327532564\n",
      "train loss:0.011420383222807862\n",
      "train loss:4.7887901560136744e-05\n",
      "train loss:0.0022849664935910593\n",
      "train loss:0.0009461840575366948\n",
      "train loss:0.0014382232192295264\n",
      "train loss:0.0011357928829727425\n",
      "train loss:0.00204006801635786\n",
      "train loss:0.0004191740111603188\n",
      "train loss:0.0018361919053740325\n",
      "train loss:0.0008903023351086788\n",
      "train loss:0.0005799100447962155\n",
      "train loss:0.00133634321630355\n",
      "train loss:0.0015966594539007335\n",
      "train loss:0.0016910607982678271\n",
      "train loss:1.3527308143622651e-05\n",
      "train loss:0.0012777779443640745\n",
      "train loss:0.003449635394084105\n",
      "train loss:0.0021189292711261904\n",
      "train loss:0.003125735137623025\n",
      "train loss:0.0006739167298477239\n",
      "train loss:0.001123955423479797\n",
      "train loss:0.0016801839664261492\n",
      "train loss:0.0015742444830572546\n",
      "train loss:0.0009117458456543153\n",
      "train loss:0.00020800361556051096\n",
      "train loss:0.0008457970343779425\n",
      "train loss:0.0005832823180114288\n",
      "train loss:0.00172709826646732\n",
      "train loss:0.0005761694230790931\n",
      "train loss:0.0004952402032244906\n",
      "train loss:0.002719463361542629\n",
      "train loss:0.019111552911790678\n",
      "train loss:0.0010592224397289634\n",
      "train loss:0.0018857329651824602\n",
      "train loss:0.002044709091701659\n",
      "train loss:0.00015818385593263374\n",
      "train loss:0.0008775753939363393\n",
      "train loss:0.007216038700147982\n",
      "train loss:0.00571930077029094\n",
      "train loss:0.004983878751067378\n",
      "train loss:0.0005628353253502869\n",
      "train loss:0.000900180204559287\n",
      "train loss:0.00595931173650038\n",
      "train loss:0.0002784173074576009\n",
      "train loss:0.0015209557610874464\n",
      "train loss:0.004985555735630789\n",
      "train loss:0.002565335340701077\n",
      "train loss:0.00011579165211335289\n",
      "train loss:0.0015324297502766267\n",
      "train loss:0.0025328070463581476\n",
      "train loss:0.0007055798829056699\n",
      "train loss:0.0019309035335012478\n",
      "train loss:0.002457255361683872\n",
      "train loss:0.002711814018505766\n",
      "train loss:0.0038006443617382463\n",
      "train loss:0.0018875595846822885\n",
      "train loss:0.002321790999382148\n",
      "train loss:0.0009076910010682006\n",
      "train loss:4.741809926528377e-05\n",
      "train loss:0.0005350923929767224\n",
      "train loss:0.0004402699404893029\n",
      "train loss:0.0011199409364713287\n",
      "train loss:0.0002574433037882088\n",
      "train loss:6.923273852727417e-05\n",
      "train loss:0.0017533660768968957\n",
      "train loss:0.0020412493804073837\n",
      "train loss:0.019092948507929072\n",
      "train loss:3.82955235584801e-05\n",
      "train loss:0.0019661382339501905\n",
      "train loss:0.0005589937471725149\n",
      "train loss:7.913998113219592e-05\n",
      "train loss:0.00024118927120615006\n",
      "train loss:4.327355704953061e-05\n",
      "train loss:0.0019416466235708063\n",
      "train loss:9.573928946166296e-05\n",
      "train loss:0.0007997626742264593\n",
      "train loss:0.0006479124749580891\n",
      "train loss:0.0034744724611930787\n",
      "train loss:0.000971542565112201\n",
      "train loss:0.00021785911890141973\n",
      "train loss:4.0652718228515524e-05\n",
      "train loss:0.0008182813399872192\n",
      "train loss:0.00015774144295307974\n",
      "train loss:0.0006533774704822263\n",
      "train loss:8.52939529715501e-05\n",
      "train loss:3.4426899163215854e-05\n",
      "train loss:0.0016721739422220273\n",
      "train loss:0.0017678062510779202\n",
      "train loss:0.0012066011823261072\n",
      "train loss:0.0009208011770868941\n",
      "train loss:7.952206974855165e-05\n",
      "train loss:0.00025313766777061643\n",
      "train loss:0.00015529662217733613\n",
      "train loss:0.000192523358655672\n",
      "train loss:0.001301598436203932\n",
      "train loss:0.0005613846028688007\n",
      "train loss:0.0003041686804832549\n",
      "train loss:0.0012681475418968944\n",
      "train loss:4.0517307947395786e-05\n",
      "train loss:0.0002811347186741837\n",
      "train loss:0.0002538600941783308\n",
      "train loss:0.0006405584575553853\n",
      "train loss:0.002231179432508797\n",
      "train loss:0.00027803888347842907\n",
      "train loss:0.0001722708154573712\n",
      "train loss:0.000317755500150501\n",
      "train loss:0.0036619678458120475\n",
      "train loss:0.0007861442049394486\n",
      "train loss:0.00080284817332949\n",
      "train loss:0.00017574757491645342\n",
      "train loss:0.0006683842365545383\n",
      "train loss:0.0011145509979009665\n",
      "train loss:0.00019247431973533016\n",
      "train loss:5.564452659565322e-05\n",
      "train loss:0.0014883048831315462\n",
      "train loss:0.0002675543339238624\n",
      "train loss:0.004862497577581678\n",
      "train loss:0.0014309893204842471\n",
      "train loss:0.0003801496352651331\n",
      "train loss:0.0009153439497892464\n",
      "train loss:0.00010957088187385836\n",
      "train loss:0.0011939778650783875\n",
      "train loss:0.0006170806183659496\n",
      "train loss:0.0002132193725910377\n",
      "train loss:0.0015394096165652115\n",
      "train loss:0.0005671782996806481\n",
      "train loss:0.0003204253153245766\n",
      "train loss:0.000372770757954047\n",
      "train loss:0.0027944966281800766\n",
      "train loss:0.00045989604771698846\n",
      "train loss:0.0007883150515631354\n",
      "train loss:0.00028733465093624396\n",
      "train loss:0.00011623315171385346\n",
      "train loss:0.0057820960131141\n",
      "train loss:0.00031792988520617496\n",
      "train loss:0.0007631362856210289\n",
      "train loss:0.0009473229609125168\n",
      "train loss:0.00011718905293688645\n",
      "train loss:0.00019815568488945398\n",
      "train loss:4.91860054697754e-05\n",
      "train loss:0.00010184511538373636\n",
      "train loss:0.0003120458218533924\n",
      "train loss:0.0003808702475783452\n",
      "train loss:0.00031242285064273667\n",
      "train loss:4.12427179736553e-05\n",
      "train loss:0.00019570342553225294\n",
      "train loss:0.00034524444185344785\n",
      "train loss:0.0018892194601189\n",
      "train loss:0.0010868329054370659\n",
      "train loss:0.016387885862790275\n",
      "train loss:0.0017919340719063715\n",
      "train loss:0.0030525134573825973\n",
      "train loss:0.004595604740026974\n",
      "train loss:0.0011607417406457002\n",
      "train loss:5.496964269313149e-05\n",
      "train loss:0.0018901480684287884\n",
      "train loss:0.0009000448489872705\n",
      "train loss:0.0004081492102832747\n",
      "train loss:0.0018990113145311753\n",
      "train loss:0.0002121908809964539\n",
      "train loss:0.0032546219732846452\n",
      "train loss:0.0002532343680047794\n",
      "train loss:0.00037072272319021776\n",
      "train loss:0.0026911794057032897\n",
      "train loss:1.7323170301023094e-05\n",
      "train loss:0.0045906357454659034\n",
      "train loss:0.0027860403081921264\n",
      "train loss:7.542156786706086e-05\n",
      "train loss:0.0009141504908177044\n",
      "train loss:0.002881416496944503\n",
      "train loss:0.0016755272772501306\n",
      "train loss:2.2582981428246406e-05\n",
      "train loss:0.0014963734374981207\n",
      "train loss:0.0035521824337924955\n",
      "train loss:0.011938708935647295\n",
      "train loss:0.0012434066696664823\n",
      "train loss:0.00014159268195430624\n",
      "train loss:0.0017662766331696118\n",
      "train loss:0.00028676350380739196\n",
      "train loss:0.0019359586107389715\n",
      "train loss:0.007030051063597006\n",
      "train loss:7.095394882169497e-05\n",
      "train loss:0.001130104318495428\n",
      "train loss:0.0017913662166867662\n",
      "train loss:0.0013666901202722232\n",
      "train loss:0.00027339216594498585\n",
      "train loss:0.00029886261345339995\n",
      "train loss:0.0034102203394943465\n",
      "train loss:0.0013059594439449654\n",
      "train loss:0.0004095503014502257\n",
      "train loss:0.0006011631199103111\n",
      "train loss:0.0019278802440814266\n",
      "train loss:0.0002059601561959332\n",
      "train loss:7.421877704283887e-05\n",
      "train loss:0.001740807482173224\n",
      "train loss:0.00044121536426303386\n",
      "train loss:0.00010944463011691881\n",
      "train loss:0.0023891621129641183\n",
      "train loss:0.0013535134461174088\n",
      "train loss:0.0003902497639760775\n",
      "train loss:0.0021735900597845506\n",
      "train loss:0.001263560868334658\n",
      "train loss:0.00023649782371515051\n",
      "train loss:4.3624977596049753e-05\n",
      "train loss:0.0014410421096845167\n",
      "train loss:4.856945347566756e-05\n",
      "train loss:0.00031642404403370197\n",
      "train loss:0.0018861392742541493\n",
      "train loss:0.002061227649142306\n",
      "train loss:0.0022267101871354085\n",
      "train loss:0.001286829099084795\n",
      "train loss:0.0026702257918066086\n",
      "train loss:0.0006989034363527494\n",
      "train loss:5.7563608487024525e-05\n",
      "train loss:0.0012117881604971784\n",
      "train loss:0.0003002765955234096\n",
      "train loss:0.0002462531213019775\n",
      "train loss:0.00014360650267270265\n",
      "train loss:0.00024203012858718708\n",
      "train loss:0.019375371634150273\n",
      "train loss:0.0005782171837670034\n",
      "train loss:8.117659124326284e-05\n",
      "train loss:9.87055352699423e-05\n",
      "train loss:0.012131937998474585\n",
      "train loss:0.0027863417085764812\n",
      "train loss:7.902924374487588e-05\n",
      "train loss:0.00024187419322657642\n",
      "train loss:0.0010902977906699577\n",
      "train loss:0.005988119288109751\n",
      "train loss:0.0015162037422514056\n",
      "train loss:0.0008301778202145152\n",
      "train loss:0.0007019674379697704\n",
      "train loss:0.002432586986496877\n",
      "train loss:0.0016359426826172638\n",
      "train loss:0.005857315608118827\n",
      "train loss:0.000391452170793658\n",
      "train loss:0.002180125233697829\n",
      "train loss:0.00025412927517081394\n",
      "train loss:0.0007648262089222159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.011264811819481597\n",
      "train loss:0.0007683045341468611\n",
      "train loss:0.00069751512560306\n",
      "train loss:0.0004053133526140349\n",
      "train loss:0.0013521473440107263\n",
      "train loss:0.000563037290454646\n",
      "train loss:0.000927613506492372\n",
      "train loss:0.003701866711037877\n",
      "train loss:0.000958907904077007\n",
      "train loss:0.00034377813078848387\n",
      "train loss:0.001308802500470864\n",
      "train loss:0.003451782787142354\n",
      "train loss:0.00453557254128142\n",
      "train loss:0.0005740980854792348\n",
      "train loss:0.00030697838529728406\n",
      "train loss:6.776218727559124e-05\n",
      "train loss:0.00032702699909573855\n",
      "train loss:0.0019710498449613655\n",
      "train loss:0.0013656220474408689\n",
      "train loss:0.007463902045009242\n",
      "train loss:0.019433414395356497\n",
      "train loss:0.0012681418030410189\n",
      "train loss:0.00020421642798706694\n",
      "train loss:4.647440262065864e-05\n",
      "train loss:0.046742255292381615\n",
      "train loss:0.00028381677178242534\n",
      "train loss:7.097690206682188e-05\n",
      "train loss:0.0007678681735721027\n",
      "train loss:0.00972236325048408\n",
      "train loss:0.0006344046318010519\n",
      "train loss:0.0020033274996436885\n",
      "train loss:0.013524533925208915\n",
      "train loss:0.0009416513659056493\n",
      "train loss:0.00019914082828387754\n",
      "train loss:0.0005588357386020586\n",
      "train loss:0.03448419275047507\n",
      "train loss:0.0003008436925039723\n",
      "train loss:0.0023411769553999605\n",
      "train loss:0.0008000492472606334\n",
      "train loss:0.0002977915175191505\n",
      "train loss:0.0020355002724398567\n",
      "train loss:0.0007721514703850559\n",
      "train loss:0.020550617487640567\n",
      "train loss:0.0020018435754668345\n",
      "train loss:0.004508160862949487\n",
      "train loss:0.0001493714871409985\n",
      "train loss:0.0003751679142019137\n",
      "train loss:0.0009536175572626475\n",
      "train loss:0.011903319580616207\n",
      "train loss:0.00018787925637816775\n",
      "train loss:0.0011217801545447637\n",
      "train loss:0.008502635544392198\n",
      "train loss:0.0005439646215060117\n",
      "train loss:0.0003380596883451682\n",
      "train loss:0.001147091626973605\n",
      "train loss:0.0003931929367621692\n",
      "train loss:0.002693424957428262\n",
      "train loss:0.0022399703783607568\n",
      "train loss:0.0014706768482968735\n",
      "train loss:0.041807856253941615\n",
      "train loss:8.420038330323209e-05\n",
      "train loss:0.0015031034725512783\n",
      "train loss:0.002281709246542892\n",
      "train loss:0.00044242079945138303\n",
      "train loss:0.0001368946415497127\n",
      "train loss:0.0006061278692326308\n",
      "train loss:0.0009774495236448111\n",
      "train loss:0.002290366522033684\n",
      "train loss:0.00022492923684629278\n",
      "train loss:0.011529934456027926\n",
      "train loss:0.004578543429139384\n",
      "train loss:0.0002690840616816758\n",
      "train loss:0.0006895251468662857\n",
      "train loss:0.0009886813152428719\n",
      "train loss:0.004892282570823234\n",
      "train loss:0.0007972091308155823\n",
      "train loss:0.0013932425733395883\n",
      "train loss:0.0003801574335767916\n",
      "train loss:0.000943239986682982\n",
      "train loss:0.0045583999173754225\n",
      "train loss:0.0007551656719292977\n",
      "train loss:0.0015638746417507483\n",
      "train loss:0.002824837566722435\n",
      "train loss:0.0024660859022978664\n",
      "train loss:0.0007415852238528861\n",
      "train loss:0.0005648299364974022\n",
      "train loss:0.0006117113880865787\n",
      "train loss:0.0014352038207883868\n",
      "train loss:0.009078489093489272\n",
      "train loss:0.00032788422604925865\n",
      "train loss:0.0009537138208838676\n",
      "train loss:0.004489939109761048\n",
      "train loss:0.0006886534787537688\n",
      "train loss:2.695154265063502e-05\n",
      "train loss:0.0004123131161562634\n",
      "train loss:0.012011128757454794\n",
      "train loss:0.0005711278072107791\n",
      "train loss:0.003502407095546574\n",
      "train loss:0.002862782080708523\n",
      "train loss:0.0009504295857650795\n",
      "train loss:0.00027819446221218915\n",
      "train loss:0.001413829997899252\n",
      "train loss:0.003236395459554192\n",
      "train loss:0.0001002443818887656\n",
      "train loss:0.0004945903103372361\n",
      "train loss:0.0007796612924667758\n",
      "train loss:0.004985387365679426\n",
      "train loss:0.0019553828281945428\n",
      "train loss:0.0003590867026193397\n",
      "train loss:0.0003679621111372362\n",
      "train loss:0.0015421022653821382\n",
      "train loss:0.0048600681048526825\n",
      "train loss:0.0001833792406235336\n",
      "train loss:0.0017575929294966267\n",
      "train loss:0.0002685984404849809\n",
      "train loss:0.0015000008194636296\n",
      "train loss:0.000810640433357001\n",
      "train loss:0.00035396792983103704\n",
      "train loss:0.0020740052956735285\n",
      "train loss:0.0017261163314519364\n",
      "train loss:0.007458043611936968\n",
      "train loss:0.010572173948705112\n",
      "train loss:0.00016506378661938976\n",
      "train loss:0.0027710588603034273\n",
      "train loss:0.002208059008205731\n",
      "train loss:0.0011280909477662732\n",
      "train loss:0.00010877536057061345\n",
      "train loss:0.0010514776256786698\n",
      "train loss:0.003113155486419718\n",
      "train loss:0.00011288797012908532\n",
      "train loss:0.001497480109466384\n",
      "train loss:0.0024255427526860395\n",
      "train loss:0.0016116145109871356\n",
      "train loss:0.0007943045150723672\n",
      "train loss:0.0029934416896568086\n",
      "train loss:0.0009453859466212406\n",
      "train loss:0.0031968218845723666\n",
      "train loss:0.0011047647423101355\n",
      "train loss:0.046990297426234645\n",
      "train loss:0.0006774127485367452\n",
      "train loss:0.001152948787109864\n",
      "train loss:0.0047940223561966\n",
      "train loss:0.00010617671809566942\n",
      "train loss:0.0017917270432260691\n",
      "train loss:0.0006043608037934684\n",
      "train loss:0.001347906624561027\n",
      "train loss:0.001894321853689516\n",
      "train loss:0.00041433548272268723\n",
      "train loss:0.0006823327831140806\n",
      "train loss:0.0030694368372950085\n",
      "train loss:0.0020796697263768143\n",
      "train loss:4.799578128180663e-05\n",
      "train loss:0.0019069346705694599\n",
      "train loss:0.0022413175164758352\n",
      "train loss:0.001414253204057767\n",
      "train loss:0.00019793388495444522\n",
      "train loss:0.002410495119498112\n",
      "train loss:0.0019987429990264978\n",
      "train loss:0.007634564054666552\n",
      "train loss:0.0007086003069109948\n",
      "train loss:0.0027577512338475465\n",
      "train loss:0.0002619130677223048\n",
      "train loss:0.0009363889815948832\n",
      "train loss:0.0009141709208943749\n",
      "train loss:0.0007114293416064829\n",
      "train loss:0.0006912489385155283\n",
      "train loss:0.00026386467663853485\n",
      "train loss:0.0007229978511724663\n",
      "train loss:0.0002798204587754484\n",
      "train loss:0.004524007828415571\n",
      "train loss:0.0007042714913107562\n",
      "train loss:0.00021121917161460283\n",
      "train loss:0.00018370897848409947\n",
      "train loss:0.0013601591289876221\n",
      "train loss:0.00029254704634120343\n",
      "train loss:0.0026289778085623423\n",
      "train loss:0.003525289142649526\n",
      "train loss:0.0007407540198625171\n",
      "train loss:0.001970835038097646\n",
      "train loss:0.0007925733557532398\n",
      "train loss:0.00017425529368057157\n",
      "train loss:0.0009582519284719869\n",
      "train loss:0.001974525562894941\n",
      "train loss:0.00027321648147004483\n",
      "train loss:0.00012922420137933676\n",
      "train loss:0.0020041798765895245\n",
      "train loss:0.0031637578951363367\n",
      "train loss:4.3628146796547855e-05\n",
      "train loss:0.001332640383119471\n",
      "train loss:0.000349316728916536\n",
      "train loss:0.00019382772764449746\n",
      "train loss:0.0014346095311668222\n",
      "train loss:0.0003027074105952572\n",
      "train loss:3.472045009165722e-05\n",
      "train loss:0.00041457694248362613\n",
      "train loss:3.579673294766535e-05\n",
      "train loss:0.00025656097978793055\n",
      "train loss:0.0008056567073095323\n",
      "train loss:0.0016385056296929512\n",
      "train loss:0.00019966294871527043\n",
      "train loss:0.002724519465437628\n",
      "train loss:0.002671470526492474\n",
      "train loss:0.00037429462890861394\n",
      "train loss:0.001013210282353479\n",
      "train loss:0.0007401433923748288\n",
      "train loss:0.0002791524931202702\n",
      "train loss:0.00014018646759918622\n",
      "train loss:0.00011160503694156923\n",
      "train loss:0.0007997805613644413\n",
      "train loss:0.001048950791823805\n",
      "train loss:0.0010816159562048\n",
      "train loss:0.0017683182027285408\n",
      "train loss:0.0011927564780447042\n",
      "train loss:0.0007319825904476764\n",
      "train loss:0.005184297425068091\n",
      "train loss:0.0006888237227020353\n",
      "train loss:0.0015516479736532455\n",
      "train loss:0.00015403399187435357\n",
      "train loss:0.0005046418292696299\n",
      "train loss:4.639661495079943e-05\n",
      "train loss:9.50252152161069e-06\n",
      "train loss:0.0006333113265123741\n",
      "train loss:0.000738408803965565\n",
      "train loss:0.014055274566189869\n",
      "train loss:1.869780416480559e-05\n",
      "train loss:0.0023330300914121316\n",
      "train loss:0.001247856320867552\n",
      "train loss:0.0024246728218645284\n",
      "train loss:0.004526584033999136\n",
      "train loss:0.0008350535768589817\n",
      "train loss:0.0035015793457809953\n",
      "train loss:0.001347051482811392\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9889\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir(\"/Users/choeunsol/deep-learning-from-scratch-master/ch07\")\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten = False)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b958f9b",
   "metadata": {},
   "source": [
    "## CNN 시각화하기\n",
    "\n",
    "CNN을 구성하는 합성곱 계층은 입력으로 받은 이미지 데이터에서 '무엇을 보고 있는' 것일까?\n",
    "\n",
    "이번 절에서는 합성곱 계층을 시각화해서 CNN이 보고 있는 것이 무엇인지 보도록 하겠다.\n",
    "\n",
    "### 1번째 층의 가중치 시각화하기\n",
    "\n",
    "조금 앞에서 MNIST 데이터셋으로 간단한 CNN 학습을 해보았고, 그 때 1번째 층의 합성곱 계층의 가중치는 그 형상이 $(30, 1, 5, 5)$ 이다. 필터의 크기가 5 x 5이고, 채널이 1개라는 것은 이 필터를 1채널의 회색조 이미지로 시각화할 수 있다는 뜻이다.\n",
    "\n",
    "그럼 합성곱 계층 (1층 째) 필터를 이미지로 나타내보자. 그 결과는 이처럼 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d6e7276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualize_filter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d01db",
   "metadata": {},
   "source": [
    "학습 전의 필터(위의 이미지들)는 무작위로 초기화되고 있어 흑백의 정도에 규칙성이 없다.\n",
    "\n",
    "한편, 학습을 마친 필터(아래 이미지들)는 규칙성 있는 이미지가 되었다. 흰색에서 검은색으로 점차 변화하는 필터와 덩어리가 진 필터 등, 규칙을 띄는 필터로 바꾸었다.\n",
    "\n",
    "그렇다면 아래 필터는 무엇을 보고 잇는 것일까? 그것은 **에지(색상이 바뀐 경계선)와 블롭(국소적으로 덩어리진 영역)** 등을 보고 있다. 가령 왼쪽 절반이 흰색이고 오른쪽 절반이 검은색인 필터는 세로 방향의 에지에 반응하는 필터이다.\n",
    "\n",
    "이처럼 합성곱 계층의 필터는 에지나 블롭 등의 원시적인 정보를 추출할 수 있다. 이런 **원시적인 정보가 뒷단 계층에 전달된다는 것이 앞에서 구현한 CNN에서 일어나는 일**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b62e0",
   "metadata": {},
   "source": [
    "### 층 깊이에 따른 추출 정보 변화\n",
    "\n",
    "앞 절의 결과는 1번째 층의 합성곱 계층을 대상으로 한 것이다.\n",
    "\n",
    "이 층은 에지와 블롭 등의 저수준 정보가 추출된다 치고, 그럼 CNN의 각 계층에서는 어떤 정보가 추출될까?\n",
    "\n",
    "딥러닝 시각화에 관한 연구에 따르면, 계층이 깊어질수록 추출되는 정보(정확히는 강하게 반응하는 뉴런)는 더 추상화된다는 것을 알 수 있다.\n",
    "\n",
    "[그림 7-26]은 일반 사물 인ㅅ기을 수행한 8층의 CNN이다. 이 네트워크 구조는 AlexNet이라고 하는데, 합성곱 계층과 풀링 계층을 여러 겹 쌓고, 마지막으로 완전연결 계층을 거쳐 결과를 출력하는 구조이다.\n",
    "\n",
    "[그림 7-26]에서 블록으로 나타낸 것은 중간 데이터이며, 그 중간 데이터에 합성곱 연산을 연속해서 적용한다.\n",
    "\n",
    "<font color = blue> [257페이지 그림 7-26 참고] </font>\n",
    "\n",
    "딥러닝의 흥미로운 점은 [그림 7-26]과 같이 합성곱 계층을 여러 겹 쌓으면, 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다는 것이다. 처음 층은 단순한 에지에 반응하고, 이어서 텍스처에 반응하고, 더 복잡한 사물의 일부에 반응하도록 변화한다.\n",
    "\n",
    "즉, **층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 고급 정보로 변화**해간다.\n",
    "\n",
    "다시 말하면 사물의 의미를 이해하도록 변화하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18feb638",
   "metadata": {},
   "source": [
    "## 대표적인 CNN\n",
    "\n",
    "이번 절에서는 CNN에서 특히 중요한 네트워크를 두 개 소개한다.\n",
    "\n",
    "하나는 CNN의 원조인 LaNet이고, 다른 하나는 딥러닝이 주목받도록 이끈 AlexNet이다.\n",
    "\n",
    "### LaNet\n",
    "\n",
    "LaNet은 손글씨 숫자를 인식하는 네트워크로, 합성곱 계층과 풀링 계층(정확히는 단순히 **'원소를 줄이기'만 하는** 서브샘플링 계층)을 반복하고, 마지막으로 완전연결 계층을 거치면서 결과를 출력한다.\n",
    "\n",
    "<font color = blue> [258페이지 그림 7-27 참고] </font>\n",
    "\n",
    "LaNet과 현재의 CNN을 비교하면 몇 가지 면에서 차이가 있따.\n",
    "\n",
    "첫 번째 차이는 활성화 함수이다. LaNet은 시그모이드 함수를 사용하고, 현재는 ReLU를 주로 사용한다.\n",
    "\n",
    "또한, 원래의 LaNet은 서브샘플링을 하여 중간 데이터를 크기를 줄이지만 현재는 최대 풀링이 주류이다.\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "AlexNet은 합성곱 계층과 풀링 계층을 거듭하여 마지막으로 완전연결 계층을 거쳐 결과를 출력한다.\n",
    "\n",
    "LaNet과 다른 점은;\n",
    "\n",
    "    - 활성화 함수로 ReLU를 이용한다.\n",
    "    - LRN이라는 국소적 정규화를 실시하는 계층을 이용한다.\n",
    "    - 드롭아웃을 사용한다.\n",
    "    \n",
    "이상에서 보듯 네트워크 구성 면에서는 LaNet과 AlexNet에 큰 차이는 없다. 하지만 컴퓨터 기술이 발전하면서 대량의 연산을 고속으로 수행할 수 있게 되었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
