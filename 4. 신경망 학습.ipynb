{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2585e2",
   "metadata": {},
   "source": [
    "# 4. 신경망 학습\n",
    "\n",
    "이번 장의 주제는 신경망 학습\n",
    "\n",
    "여기서 학습이란 **훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것**을 뜻한다.\n",
    "\n",
    "이번 장에서는 신경망이 학습할 수 있도록 해주는 지표인 **손실 함수**를 소개한다.\n",
    "\n",
    "이 <font color = red> **손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표** </font> 이다.\n",
    "\n",
    "이번 장에서는 손실 함수의 값을 가급적 작게 만드는 기법으로, 함수의 기울기를 활용하는 경사법을 소개한다.\n",
    "\n",
    "\n",
    "## 데이터에서 학습한다!\n",
    "\n",
    "신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다. 데이터에서 학습한다는 것은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻.\n",
    "\n",
    "이를테면 2장의 퍼셉트론 예에서는 진리표를 보면서 사람이 수작업으로 매개변수 값을 설정했고, 이 때 매개변수가 3개였다.\n",
    "\n",
    "하지만 실제 신경망에서는 매개변수가 수천에서 수만, 층을 깊게 할 경우 수억까지 이를 수도 있기 때문에 **매개변수를 수작업으로 정한다는 것은 아예 불가능** 하다.\n",
    "\n",
    "이번 장에서는 신경망 학습(**데이터로부터 매개변수의 값을 정하는 방법**)에 대해 설명하고 파이썬으로 MNIST 데이터셋의 손글씨 숫자를 학습하는 코드를 구현해 볼 것이다.\n",
    "\n",
    "<font color = blue> NOTE. 2장의 퍼셉트론도 선형 분리 가능 문제라면 데이터로부터 자동으로 학습할 수 있다. 하지만 비선형 분리 문제는 자동으로 학습할 수 없다. </font>\n",
    "\n",
    "\n",
    "### 데이터 주도 학습\n",
    "\n",
    "기계학습은 데이터가 생명이다. 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는 것이 기계학습.\n",
    "\n",
    "그래서 데이터 없이는 아무것도 시작되지 않는다. 즉, **기계학습의 중심에는 데이터가 존재** 한다.\n",
    "\n",
    "따라서 기계학습은 인간의 개입을 최소화하며, 수집한 데이터로부터 패턴을 찾으려 시도한다.\n",
    "\n",
    "게다가 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지녔다. 여기서 말하는 특징은 **입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기**를 가리킨다.\n",
    "\n",
    "예를 들어, 사람마다 필체가 다르기 때문에 5를 제대로 분류하는 프로그램을 고안해 설계하기란 의외로 어려운 문제이다. 사람이라면 어렵지 않게 인식하지만, 그 안에 숨은 규칙성을 명확한 로직으로 풀기가 힘들다.\n",
    "따라서 5를 인식하는 알고리즘을 밑바닥부터 '설계하는' 대신, **주어진 데이터를 잘 활용**해서 해결하고 싶어질 것이다.\n",
    "\n",
    "이런 방법의 하나로, **이미지에서 특징을 추출하고, 그 특징의 패턴을 기계학습 기술로 학습**하는 방법이 있다.\n",
    "\n",
    "컴퓨터 비전 분야에서는 SIFT, SURF, HOG 등의 특징을 사용, 이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습할 수 있따.\n",
    "\n",
    "기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 기계가 담당한다. 다만, 이미지를 벡터로 변환할 때 사용하는 특징은 **여전히 사람이 설계하는 것**임에 주의해야 한다. 즉, 특징과 기계학습을 활용한 접근에도 문제에 따라서는 사람이 적절한 특징을 생각해내야 하는 것이다.\n",
    "\n",
    "그렇지만 신경망은 이미지를 **\"있는 그대로\" 학습** 한다. 두 번째 접근 방식에서는 특징을 사람이 설계했지만, 신경망은 이미지에 중요한 특징까지도 기계가 스스로 학습할 것이다.\n",
    "\n",
    "신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점에 있다. 예를 들어 5를 인식하는 문제든, 개를 인식하는 문제든, 사람의 얼굴을 인식하든 문제든, 세부사항과 상관없이 신경망은 주어진 데이터를 온전ㄴ히 학습하고 주어진 문제의 패턴을 발견하러 시도한다.\n",
    "\n",
    "즉, 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end to end'로 학습할 수 있다.\n",
    "\n",
    "\n",
    "### 훈련 데이터와 시험 데이터\n",
    "\n",
    "기계학습 문제는 데이터를 훈련 데이터와 시험 데이터(training data and test data)로 나눠 학습과 실험을 수행하는 것이 일반적\n",
    "\n",
    "우선 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾는다. 그런 다음 시험 데이터를 사용하여 훈련된 모델의 실력을 평가한다.\n",
    "\n",
    "훈련 데이터와 시험 데이터를 나누는 이유는 우리가 원하는 것이 **범용적으로 사용할 수 있는 모델**이기 때문이다. 이 범용 능력을 제대로 평가하기 위해 훈련 데이터와 시험 데이터를 분리한다.\n",
    "\n",
    "범용 능력이란 아직 보지 못한 데이터로도 문제를 올바르게 풀어내는 능력이다. 이 **범용 능력을 획득**하는 것이 기계학습의 최종 목표가 될 것이다.\n",
    "\n",
    "데이터셋 하나로만 매개변수의 학습과 평가를 수행하면 올바른 평가가 될 수 없다. 수중의 데이터셋은 제대로 맞히더라도 다른 데이터셋에는 엉망인 일이 벌어지기 때문이다. 한 데이터셋에 지나치게 최적화된 상태를 **오버 피팅** 이라고 하는데, 오버피팅을 피하는 것은 기계학습의 중요한 과제이기도 하다.\n",
    "\n",
    "\n",
    "## 손실 함수\n",
    "\n",
    "만약 사람들에게 \"지금 얼마나 행복하나요?\"라고 물으면 뭐라고 대답할까?\n",
    "\n",
    "\"아주 행복하죠\" 나 \"그리 행복한 거 같진 않아요\"라는 막연한 답이 돌아올 것이다. 그런데 \"현재 내 행복 지수는 10.23입니다\"라고 대답한다면 질문한 사람이 당황할 것이다. 하나의 지표를 가지고 행복을 수치적으로 판단했다는 것이기 때문이다.\n",
    "\n",
    "신경망 학습에서도 이와 같은 일을 수행한다. 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현하고, 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 과정이다.\n",
    "\n",
    "행복 지표를 가진 사람이 그 지표를 근거로 '최적의 인생'을 탐색하듯, 신경망도 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색한다.\n",
    "\n",
    "신경망 학습에서 사용하는 지표는 **손실 함수**라고 한다. 이 손실 함수는 임의의 함수를 사용할 수도 있지민 일반적으로는 **오차제곱합과 교차 엔트로피 오차**를 사용한다.\n",
    "\n",
    "\n",
    "### 오차제곱합\n",
    "\n",
    "가장 많이 쓰이는 손실 함수는 **오차 제곱합(SSE)** 이다. 수식으로는 다음과 같다.\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{k}{(y_k - t_k)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df3043",
   "metadata": {},
   "source": [
    "$y_k$는 신경망의 출력(신경망이 추정한 값), $t_k$는 정답 레이블, k는 데이터의 차원 수를 나타낸다.\n",
    "\n",
    "예를 들어, 손글씨 숫자 인식 예에서 $y_k$와 $t_k$는 다음과 같은 원소 10개짜리 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e556d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc6187",
   "metadata": {},
   "source": [
    "이 배열들의 원소는 첫 번째 인덱스부터 순서대로 숫자 0, 1, 2, ... 일 때의 값이다.\n",
    "\n",
    "여기에서 신경망의 출력 y는 소프트맥스 함수의 출력이다. 소프트맥스 함수의 출력은 확률로 해석할 수 있으므로,\n",
    "이 예에서는 이미지가 0일 확률은 0.1, 1일 확률은 0.05, 2일 확률은 0.6, ... 이라고 해석된다.\n",
    "\n",
    "정답 레이블인 t는 정답을 가리키는 위치의 원소는 1로, 그 외에는 0으로 표기한다.\n",
    "여기에서는 숫자 '2'에 해당하는 원소의 값이 1이므로 정답이 2임을 알 수 있다.\n",
    "\n",
    "이처럼 **한 원소만 1로 하고 그 외는 0으로 나타내는 표기법을 원-핫 인코딩** 이라 한다.\n",
    "\n",
    "오차제곱합은 위의 식과 같이 각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차를 제곱한 후, 그 총합을 구한다.\n",
    "\n",
    "이 오차제곱합을 파이썬으로 구현해보자.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6065d3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 예1 : '2'일 확률이 가장 높다고 추정함\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2422956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예2 : '7'일 확률이 가장 높다고 추정함\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0268a",
   "metadata": {},
   "source": [
    "이 실험의 결과로 첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작은 것을 알 수 있다.\n",
    "\n",
    "오차제곱합 기준으로는 첫 번째 추정 결과가 오차가 더 작으니 정답에 더 가까울 것으로 판단할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d90c82",
   "metadata": {},
   "source": [
    "### 교차 엔트로피 오차\n",
    "\n",
    "또 다른 손실 함수로서 교차 엔트로피 오차도 자주 이용한다.\n",
    "교차 엔트로피 오차의 수식은 다음과 같다.\n",
    "\n",
    "$$E = -{\\sum}_{k}{t_klog y_k}$$\n",
    "\n",
    "여기서 $log$는 밑이 $e$인 자연로그이다.\n",
    "\n",
    "$y_k$는 신경망의 출력, $t_k$는 정답 레이블이다.\n",
    "$t_k$는 정답에 해당하는 인덱스의 원소만 1이고 나머지가 0인 원-핫 인코딩이다.\n",
    "\n",
    "그래서 위의 식은 실질적으로 정답일 때의 추정($t_k$가 1일 때의 $y_k$)의 자연로그를 계산하느 식이 된다.\n",
    "\n",
    "예를 들어, 정답 레이블은 2가 정답이라 하고 이때의 신경망 출력이 0.6이라면 교차 엔트로피 오차는 $-log0.6 = 0.51$이 된다.\n",
    "\n",
    "또한, 같은 조건에서 신경망 출력이 0.1이라면 $-log0.1 = 2.30$이 된다.\n",
    "\n",
    "**즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e01bdd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/8zsbg3r51rs_vl832mx18q_00000gn/T/ipykernel_52796/2713818593.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  y = np.log(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-5.0, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAasUlEQVR4nO3de7SUdb3H8fcXFVEoRElRQbmIKCiIbBFR0xC8p5Z6RC3zUsg60e2YmQeXHdNMS0+mnDS81OkcizK1vCOkhiciQEGUO3gDRATFSyC4N/t7/vhu1oAOMPsy85t55vNa61mz957HmS+PMJ/9uz7m7oiIiHxcq9QFiIhIeVJAiIhIXgoIERHJSwEhIiJ5KSBERCQvBYSIiOSVNCDM7EQzm29mi8zs+ylrERGRzVmqdRBmth2wABgGLAWmAee6+5wkBYmIyGZStiAGAovc/WV3/wgYB5yesB4REdnE9gnfe29gySbfLwUO//hJZjYCGAHQtm3bAQcccEBpqhMRKWP19bB+ff7jo4/goIOgdes497nnnlvl7p9p7HukDAjL87NP9He5+1hgLEBNTY1Pnz692HWJiCTnDsuXw+LFcbz88uaPK1dufn779rDfftC9O3TrBt/+Nuy1VzxnZq81pYaUAbEU6LLJ952BNxLVIiJScnV1sGQJLFoUx+LFuceXX4a1a3PntmoFXbpAjx5wxhkRAj16xNG9O3To0PL1pQyIaUBPM+sGLAOGA+clrEdEpMXV1cGrr8YH/8KFmz+++irU1ubObdMmPuz32w+GDcu1CHr0gH33zXUZlUqygHD3OjMbBYwHtgPucffZqeoREWmqDRuiJbBwYRwLFuS+fuWVCImN2rWLD/6+feHMM+PrHj2gZ0/Yc89oKZSLlC0I3P0x4LGUNYiIFMI9+v0XLMgd8+fH4+LFMTi80c47xwd+v35w1lnx9cZj993B8o3AlqGkASEiUm7WrYvf/OfP/+Tx3nu581q3jt/+e/aEU07ZPAT22qtyQmBrFBAiUnXc4a23YN68Tx6vvRbPb9S5M/TqBeefD/vvH1/vv3+MCWy3Xbo/QykoIEQkszZsiIHguXPjmDcv97h6de68nXeOD/5Bg+DCC+PrjUHQtm2q6tNTQIhIxautjVlBc+bkjrlzo1to3brceXvsAQceCMOHwwEH5I7OnctrcLhcKCBEpGLU1sb4wOzZccyZE48LFmw+U6hr1wiCoUPjceNRjLUCWaaAEJGys2FDTA996aXcMXt2tAg2rhswizUCffrAaadB797xda9e1d0t1JIUECKSjDu8+Sa8+GIcL70Uj3PmwIcf5s7r1i32Fjr11AiBjUGw887paq8GCggRKYm1a6MVMGtWHC++GI9vv507Z889IwhGjoSDD44g6N07FpdJ6SkgRKRFuceq4hdeiGPWrHhcuDA3fXTnnSMIvvCFWFF88MHxfceOaWuXzSkgRKTJamujO2jmzDheeCEeN51C2r17rCg+99wIg379ostIs4bKnwJCRAryz39GAMyYkTtmz457D0BsNNe3b2wtccghEQQHHwyf/nTSsqUZFBAi8gnvvQfPP7/5MX9+rouoY0fo3z/uOXDIIXH07Anb6xMlU/S/U6TKvftuBMD06fDcc3EsXpx7vksXOPTQWFx26KERDHvvnY29hmTrFBAiVWTNmgiDadMiEKZNixXIG+27L9TUwCWXRBgceih8ptE3qpSsUECIZFRtbawrmDo1d8yZE/cyhmgZ1NTE3kOHHRZhoFlEsikFhEgGuMcupP/4R+54/vncPkS77QYDB8IXvxhhUFMDnTqlrVnKnwJCpAKtWRNdRH//O0yZEseKFfFcmzbRGhg5Eg4/PI6uXTVmII2ngBApc+6xZfXkyREIkyfH4rMNG+L5nj3h+OMjCAYNiqmmO+yQtGTJCAWESJmprY01Bn/7WxyTJ8Py5fFcu3bRVfT978MRR0Qg7LZb2noluxQQIol98EG0DP7v/+KYMiW3UV23bjBkCBx5JAweHNtRZP0uZlI+FBAiJbZqFTz7LEyaFMfMmTGzqFWrWGMwYkQEwpFHxr2NRVJRQIgU2Ztvwl//mjvmzImft2kTXUSjR8PRR8fXn/pU2lpFNqWAEGlhK1ZEEDz9NDzzTNz/GGL84Mgj4Utfgs9+Nqaa7rhj0lJFtkoBIdJMq1dHIDz1VByzZ8fP27WLlsFFF8Gxx8bUU+1VJJVEf11FGmnduhhM/stfYOLEWJBWXw877QRHHQXnnx8DywMGKBCksumvr8g21NfHuoMnn4QJEyIc1q2LD//DD4erroLjjouv1WUkWaKAEMljxYoIhI2hsHGVcp8+sUJ56FA45hjdClOyTQEhAtTVxVqExx+HJ56IhWoQO5kOGxYrlYcN07RTqS4KCKlaK1ZEIDz6aLQS3nsvFqENHgw/+hGceGLcCEe3xpRqpYCQquEeLYOHH4ZHHonN7iBaBWedBSedFF1H7dunrVOkXCggJNM+/DCmnj70UITCG2/ErqaDBsF118Epp8S9k7XTqcgnKSAkc95+O7qN/vQnGD8e1q6NweQTToDPfx5OPll3SRMphAJCMmHpUnjwwTgmTYqtsPfeO+6WdtppsVBNU1BFGkcBIRVr8WK4//44pk6Nn/XuHVthn3FGLFRT15FI0ykgpKIsWgR/+AP88Y+5qagDBsD118ftNHv1SlufSJYkCQgzOxv4D+BAYKC7T09Rh1SGV16JUPj973OhcMQRcPPNEQpduyYtTySzUrUgXgK+CPwy0ftLmVu+PEJh3Li4gQ7EVhY33wxnnw1duqStT6QaJAkId58LYOoglk28/z488ADce29MTa2vj4VqN9wA55yjloJIqZX9GISZjQBGAOyzzz6Jq5GWVlcX+x39z//EtNR166B797iJzrnnwoEHpq5QpHoVLSDMbCLQKc9To939z4W+jruPBcYC1NTUeAuVJ4m9+CL8+tfRWlixAnbdFS6+OG6mM2iQZh+JlIOiBYS7Dy3Wa0tlWr06AuFXv4p7KOywA5x6KlxwQSxea906dYUisqmy72KSylZfH7fdvOuuGF9Yvz7GFX7+czjvPOjYMXWFIrIlqaa5fgG4DfgM8KiZzXT3E1LUIsXx1lvRUrjzzljQtssu8LWvwSWXRECISPlLNYvpQeDBFO8txeMe21z84hex5UVtbdxU55pr4MwzoU2b1BWKSGOoi0ma7f334Te/gdtvhzlzoEMHGDUKRoyAAw5IXZ2INJUCQpps/nwYMwb++7/hgw/gsMOiW+mcc2CnnVJXJyLNpYCQRnGPu6/dckvcja116wiEb3wjAkJEskMBIQVZty6mqP7sZzB7NuyxB/zwh9GNtMceqasTkWJQQMhWrV4dYwu33hoL2vr1iwVuw4fr/goiWaeAkLyWLYP//E/45S9hzZq4G9vll8OQIVrlLFItFBCymUWL4MYbY+C5vj72Q7r8cujbN3VlIlJqCggBYN68uOnOvffGFhhf/WoEQ7duqSsTkVQUEFVu3rwYbB43Lqamfuc78N3vQqd82yyKSFVRQFSpRYsiGO69N4Lh8svhsstg991TVyYi5UIBUWWWLYtguPvu6Er6znfge99TMIjIJykgqsTq1XFntltvhQ0bYOTIuCnPnnumrkxEypUCIuPWr4/N8669Ft59N27Ic801GnwWkW1rlboAKQ53uO8+6N0b/u3fYhuMGTNiUz2Fg4gUQgGRQTNmxDbb//Iv0LYtPPEEjB8fq6BFRAqlgMiQVatib6QBA2Du3FgFPWNGrIIWEWksjUFkwIYNcUvPK6+Mbbe//W24+uq4i5uISFMpICrc88/HjKRp06Jb6b/+C/r0SV2ViGSBupgq1Jo1sbDtsMPg9dfhf/8Xnn5a4SAiLUctiAo0fjxceim89lo83nCDupNEpOWpBVFB3n0XLr4YTjwxtseYNAnuuEPhICLFoYCoEI8/DgcdFNtwX3llzE46+ujUVYlIlikgytyaNTEIffLJ0VKYMiW25W7TJnVlIpJ1CogyNm0a9O8PY8fGbqvPPReD0iIipaCAKEP19fDjH8MRR8C6dfDUU/CTn+ge0CJSWprFVGZWrIAvfxkmTIitMu64Azp0SF2ViFQjBUQZefppOO+8mK00dmzc9tMsdVUiUq3UxVQG3OHGG2Ho0BiInjoVvvY1hYOIpKUWRGIffAAXXQT33x9dSnffDe3apa5KREQBkdTChXD66bBgAdx0U9y3Qa0GESkXCohEnn4azjwTWrWKAenPfS51RSIim9MYRAJjx8Lxx8f9oKdOVTiISHlSQJRQfX0seLv00hiQnjwZundPXZWISH4KiBL56KNY33DTTfD1r8PDD0P79qmrEhHZsiQBYWY/NbN5ZjbLzB40s11S1FEq778Pp5wCv/1tbM19222wvUZ/RKTMpWpBTAAOcve+wALgykR1FN3KlXDssfDMM7ET6xVXaKaSiFSGJAHh7k+6e13Dt1OAzinqKLblyyMc5s6Fhx6CCy5IXZGISOHKYQziYuDxLT1pZiPMbLqZTV+5cmUJy2qeJUviHtGvvRb3cjjppNQViYg0TtF6ws1sItApz1Oj3f3PDeeMBuqAe7f0Ou4+FhgLUFNT40UotcW9+mpMXX3nnVjjcMQRqSsSEWm8ogWEuw/d2vNm9hXgVOA4d6+ID/5CLFsGxx0XG+499RQMGJC6IhGRpkkyl8bMTgSuAI5x97UpaiiGFSsiHFauhIkTFQ4iUtlSTbYcA+wITLCY0jPF3UcmqqVFvP02DBsWYw/jx8PAgakrEhFpniQB4e77pXjfYlm7NtY5LFgAjz4KRx2VuiIRkebTcq1mqquD4cPj/tH33x9dTCIiWaCAaAZ3+OY3Y9uMMWPgjDNSVyQi0nLKYR1ExbrxRrj99lgd/fWvp65GRKRlKSCa6MEH4cor4dxz4frrU1cjItLyFBBNMHt2bJsxcCDcc0/c9EdEJGv00dZI77wTtwlt1w4eeADatEldkYhIcWiQuhE2bIgupddfj91Z9947dUUiIsWjgGiEH/4QnnwS7rwTBg9OXY2ISHGpi6lAzzwD110HX/kKfPWrqasRESk+BUQBVq2C88+H/faL9Q4iItVAXUzb4A4XXxwh8cgjMTgtIlINFBDbMGZMrJS+5Rbo3z91NSIipaMupq1YvDhWSZ90UmypISJSTRQQW1BfH4PRO+wQs5ZiV3IRkeqhLqYtuPPOmLk0dqzWO4hIddpmC8LMRplZh1IUUy6WLIHLL4chQzSlVUSqVyFdTJ2AaWb2BzM70SzbnS3uMHJkrJpW15KIVLNtBoS7XwX0BO4GLgQWmtn1ZtajyLUl8dBD8NhjsSiue/fU1YiIpFPQILW7O/Bmw1EHdAD+aGY/KWJtJbd+PVx2GfTuDaNGpa5GRCStbQ5Sm9k3ga8Aq4C7gMvdvdbMWgELge8Vt8TS+fnPY2rr+PExe0lEpJoVMoupI/BFd39t0x+6e72ZnVqcskrvzTfh2mvh85+H449PXY2ISHrbDAh3v3orz81t2XLSGT06uphuvjl1JSIi5UEL5YAZM+BXv4JvfQt69kxdjYhIeVBAAFdfDR06wFVXpa5ERKR8VH1ATJ8eu7Redhm0b5+6GhGR8lH1AXHNNbDrrprWKiLycVUdEJu2Hj796dTViIiUl6oOCLUeRES2rGoDQq0HEZGtq9qAuO46tR5ERLamKgPi1VdjU76RI9V6EBHZkqoMiNtvh1atIiBERCS/qguIDz+Eu+6CM86ALl1SVyMiUr6qLiDGjYN33tHYg4jItlRVQLjDbbdBnz5wzDGpqxERKW9JAsLMrjWzWWY208yeNLO9SvG+U6bExnyjRulWoiIi25KqBfFTd+/r7ocAjwBb3FK8JY0ZE/stfelLpXg3EZHKliQg3P39Tb5tC3ix33PVKrjvPrjwQmjXrtjvJiJS+Qq5o1xRmNmPgAuA94DPbeW8EcAIgH322afJ73f//VBbCxdd1OSXEBGpKuZenF/ezWwi0CnPU6Pd/c+bnHcl0Mbdf7Ct16ypqfHp06c3qZ5jj4UVK2DOHI0/iEh1MbPn3L2msf9d0VoQ7j60wFN/CzwKbDMgmmrZMpg0CX7wA4WDiEihUs1i2vTGnqcB84r5fvfdF1Nchw8v5ruIiGRLqjGIG8ysF1APvAYUddOL3/0O+veHXr2K+S4iItmSJCDc/cxSvdfLL8PUqXDjjaV6RxGRbMj8Surf/z4ezzknbR0iIpUm8wExbhwMHgz77pu6EhGRypLpgJgzB2bN0uC0iEhTZDogHnwwHs8+O20dIiKVKNMBMXEiHHIIdMq3XE9ERLYqswGxZg387W8wbFjqSkREKlNmA+LZZ2PvpaGFrucWEZHNZDYgJkyAHXeEo49OXYmISGXKbEBMnAhHHgk77ZS6EhGRypTJgFixIqa3avxBRKTpMhkQf/lLPGr8QUSk6TIZEBMmwK67xgZ9IiLSNJkLCPcYfxgyBLbbLnU1IiKVK3MBMX8+LF2q8QcRkebKXEBMnBiPGn8QEWmeTAZE9+5xiIhI02UuIKZNi+29RUSkeTIVEG+9BW+8odlLIiItIVMBMWNGPCogRESaL1MBMXNmPPbrl7QMEZFMyFRAzJgRtxbdddfUlYiIVL5MBcTMmepeEhFpKZkJiH/+ExYsiDvIiYhI82UmIGbNim021IIQEWkZmQmIjQPUCggRkZaRmYCYMSMGpzt3Tl2JiEg2ZCog+vcHs9SViIhkQyYCorYWXnpJ3UsiIi0pEwExbx6sX68ZTCIiLSkTAaEBahGRlpeJgJgxA3baCXr1Sl2JiEh2ZCYgDj5YtxgVEWlJFR8Q7tpiQ0SkGCo+IJYtg3ffhb59U1ciIpItSQPCzL5rZm5mHZv6Gq+/Ho/durVUVSIiAgkDwsy6AMOA15vzOkuXxqNWUIuItKyULYifAd8DvDkvsmRJPHbp0vyCREQkJ0lAmNlpwDJ3f6GAc0eY2XQzm75y5cpPPL90KbRtC+3bF6NSEZHqtX2xXtjMJgKd8jw1Gvh34PhCXsfdxwJjAWpqaj7R2li6NFoP2oNJRKRlFS0g3H1ovp+b2cFAN+AFi0/1zsDzZjbQ3d9s7PssWaLxBxGRYih5F5O7v+juu7t7V3fvCiwFDm1KOEC0IBQQIiItr6LXQdTVwfLlGqAWESmGonUxFaqhFdEky5dDfb1aECIixVDRLQhNcRURKZ6KDggtkhMRKZ5MBIRaECIiLa+iA2LJEi2SExEplooOiI1TXLVITkSk5VV0QCxZou4lEZFiqeiA0CI5EZHiqdiA0CI5EZHiqtiA0CI5EZHiqtiA0BoIEZHiqtiA0CpqEZHiqtiAUAtCRKS4KjYgNi6S22WX1JWIiGRTxQaEFsmJiBRXxQaEFsmJiBRXxQaEFsmJiBRXRQbExkVyCggRkeKpyIDYuEhOXUwiIsVTkQGhKa4iIsVXkQGhRXIiIsVXkQGhFoSISPFtn7qApjjhBGjXTovkRESKqSIDok+fOEREpHgqsotJRESKTwEhIiJ5KSBERCQvBYSIiOSlgBARkbwUECIikpcCQkRE8lJAiIhIXgoIERHJSwEhIiJ5KSBERCQvBYSIiOSVJCDM7D/MbJmZzWw4Tk5Rh4iIbFnK3Vx/5u43JXx/ERHZCnUxiYhIXilbEKPM7AJgOnCZu6/Od5KZjQBGNHy73sxeKlWBZa4jsCp1EWVC1yJH1yJH1yKnV1P+I3P3li4kXthsItApz1OjgSnE/zgHrgX2dPeLC3jN6e5e06KFVihdixxdixxdixxdi5ymXouitSDcfWgh55nZncAjxapDRESaJtUspj03+fYLgLqNRETKTKoxiJ+Y2SFEF9OrwKUF/ndji1VQBdK1yNG1yNG1yNG1yGnStSjaGISIiFQ2TXMVEZG8FBAiIpJXWQaEmZ1oZvPNbJGZfT/P82ZmtzY8P8vMDk1RZykUcC3Ob7gGs8xsspn1S1FnsW3rOmxy3mFmtsHMziplfaVUyLUws2MbtrGZbWZ/LXWNpVLAv4/2Zvawmb3QcC0uSlFnKZjZPWb21pbWijXpc9Pdy+oAtgMWA92B1sALQO+PnXMy8DhgwCDgH6nrTngtBgMdGr4+KYvXopDrsMl5TwGPAWelrjvh34ldgDnAPg3f75667oTX4t+BGxu+/gzwDtA6de1Fuh6fBQ4FXtrC843+3CzHFsRAYJG7v+zuHwHjgNM/ds7pwG88TAF2+djU2azY5rVw98meW4U+Behc4hpLoZC/EwDfAO4H3iplcSVWyLU4D3jA3V8HcPesXo9CroUDnzIzA9oRAVFX2jJLw90nEX++LWn052Y5BsTewJJNvl/a8LPGnpMFjf1zXkL8hpA127wOZrY3sabmjhLWlUIhfyf2BzqY2TNm9lzDljZZVMi1GAMcCLwBvAh8y93rS1Ne2Wn052bKvZi2xPL87ONzcQs5JwsK/nOa2eeIgDiqqBWlUch1uAW4wt03xC+LmVXItdgeGAAcB+wE/N3Mprj7gmIXV2KFXIsTgJnAEKAHMMHMnnX394tcWzlq9OdmOQbEUqDLJt93JtK/sedkQUF/TjPrC9wFnOTub5eotlIq5DrUAOMawqEjcLKZ1bn7n0pSYekU+u9jlbuvAdaY2SSgH5C1gCjkWlwE3ODRCb/IzF4BDgCmlqbEstLoz81y7GKaBvQ0s25m1hoYDjz0sXMeAi5oGJUfBLzn7stLXWgJbPNamNk+wAPAlzP4G+JG27wO7t7N3bu6e1fgj8C/ZjAcoLB/H38Gjjaz7c1sZ+BwYG6J6yyFQq7F60RLCjPbg9jV9OWSVlk+Gv25WXYtCHevM7NRwHhilsI97j7bzEY2PH8HMUvlZGARsJb4LSFzCrwWVwO7Ab9o+O25zjO2g2WB16EqFHIt3H2umT0BzALqgbvcPXP7nRX49+Ja4Ndm9iLRxXKFu2dyC3Az+x1wLNDRzJYCPwB2gKZ/bmqrDRERyascu5hERKQMKCBERCQvBYSIiOSlgBARkbwUECIikpcCQkRE8lJAiIhIXgoIkWZouP/ELDNrY2ZtG+45cFDqukRaghbKiTSTmV0HtCE2xlvq7j9OXJJIi1BAiDRTwz5A04B1wGB335C4JJEWoS4mkebblbgZzaeIloRIJqgFIdJMZvYQcTezbsCe7j4qcUkiLaLsdnMVqSQNd2urc/ffmtl2wGQzG+LuT6WuTaS51IIQEZG8NAYhIiJ5KSBERCQvBYSIiOSlgBARkbwUECIikpcCQkRE8lJAiIhIXv8PM0b7Pz0WKckAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 자연로그 y = logx 의 그래프\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(0, 1.0, 0.01)\n",
    "y = np.log(x)\n",
    "\n",
    "plt.plot(x, y, color = \"blue\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(-5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936ae55",
   "metadata": {},
   "source": [
    "위의 그림에서 보듯이 x가 1일 때 y는 0이 되고, x가 0에 가까워질수록 y의 값은 점점 작아진다.\n",
    "\n",
    "마찬가지로, 위 식도 **정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 된다.** 반대로 정답일 때의 출력이 작아질수록 오차는 커진다.\n",
    "\n",
    "그렇다면 파이썬으로 교차 엔트로피 오차를 구현해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e72694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206faf17",
   "metadata": {},
   "source": [
    "- y와 t는 넘파이 배열이다.\n",
    "\n",
    "- delta에 대한 설명 : np.log() 함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31463592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "639993a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a0e67",
   "metadata": {},
   "source": [
    "오차 값이 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단한다.\n",
    "\n",
    "앞서 오차제곱합의 판단과 일치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b4de64",
   "metadata": {},
   "source": [
    "### 미니배치 학습\n",
    "\n",
    "기계학습 문제는 훈련 데이터를 사용해 학습한다. 더 구체적으로 말하면, 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 줄여주는 매개변수를 찾아낸다.\n",
    "\n",
    "이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다. 즉, 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는 것이다.\n",
    "\n",
    "지금까지는 데이터 하나에 대한 손실 함수만 생각해왔으니, 이제 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 고려한다.\n",
    "\n",
    "교차 엔트로피 오차부터 보자.\n",
    "\n",
    "$$E = -\\frac{1}{N} \\sum_{n} \\sum_{k}{t_{nk} log y_{nk}}$$\n",
    "\n",
    "이 때 데이터가 N개라면 $t_{nk}$는 n번째 데이터의 k번째 값을 의미한다. ($y_{nk}$는 신경망의 출력, $t_{nk}$은 정답 레이블)\n",
    "\n",
    "데이터 하나에 대한 손실 함수를 **N개의 데이터로 확장** 한 것으로, 마지막에 데이터의 개수인 N으로 나눠 정규화하면서 **평균 손실 함수**를 구하고 있다.\n",
    "\n",
    "이렇게 평균을 구해 사용하면 **훈련 데이터 개수와 관계없이 언제든 통일된 지표** 를 얻을 수 있다.\n",
    "\n",
    "그렇지만 거대한 데이터셋의 경우는 일일히 손실 함수를 계산하는 것은 시간이 많이 걸린다. 이런 경우 데이터 일부를 추려 전체의 근사치로 이용할 수 있다.\n",
    "\n",
    "신경망 학습에서도 이와 같이 **훈련 데이터로부터 일부만 골라 학습을 수행** 한다. 이것을 **미니 배치**라고 하며, 이러한 학습 방법은 미니패치 학습이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b98fe49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/choeunsol/Python'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adfa6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/choeunsol/deep-learning-from-scratch-master/ch03\")\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36380e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99e50cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776686b",
   "metadata": {},
   "source": [
    "우리의 목표는 이 train data에서 무작위로 데이터를 뽑아 미니 배치를 수행하는 것이다.\n",
    "\n",
    "넘파이의 np.random.choice() 함수를 쓰면 간단히 해결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66754560",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # R의 sample 함수와 비슷하다고 생각하면 될 듯!\n",
    "\n",
    "# np.random.choice() 함수를 통해 랜덤추출의 인덱스를 만들어주고, 배치 사이즈만큼의 배치 데이터셋을 만들어준다.\n",
    "x_batch = x_train[batch_mask] # batch_mask로 단일 인덱싱할 경우 brain_mask-1 번째의 행을 추출한다. 즉, 이미지를 뽑아낸다.\n",
    "y_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d90751",
   "metadata": {},
   "source": [
    "### (배치용) 교차 엔트로피 오차 구현하기\n",
    "\n",
    "그럼, 미니배치 같은 배치 데이터를 지원하는 교차 엔트로파 오차는 어떻게 구현할까? 아까 구현했던 함수를 조금만 바꿔주면 된다.\n",
    "\n",
    "여기에서는 데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 경우 모두를 처리할 수 있도록 구현할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dedad5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1: # 만약 y가 1차원 배열이라면\n",
    "        t = t.reshape(1, t.size) # t를 1차원 배열로 변환하고 전체 사이즈를 열로 가지면서 1행인 1차원 벡터로 변형한다.\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0] # 이미지의 전체 개수 (28*28의 데이터를 담고 있는 하나의 행)으로 이미지 당 평균을 계산한다.\n",
    "    return -np.sum(t * np.log(y+1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cadc800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47040000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## size와 shape의 활용을 주의하자.\n",
    "\n",
    "x_train.size # n*n 배열의 [전체 데이터 수]를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18666c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape # n*n 배열의 [형상], 즉 n*n을 그대로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad8accd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.ndim # 배열의 차원 수. 행과 열로 되어 있으면 2차원!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c46dd",
   "metadata": {},
   "source": [
    "<코드 해석>\n",
    "\n",
    "1. 이 코드에서 $y$는 신경망의 출력, t는 정답 레이블이다.\n",
    "\n",
    "2. y가 1차원이라면, 즉 데이터 하나 당 교차 엔트로피 오차를 구하는 경우는 reshape 함수로 데이터의 형상을 바꿔준다.\n",
    "\n",
    "3. 그리고 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다.\n",
    "\n",
    "정답 레이블이 원-핫 인코딩이 아니라 2나 7과 같은 숫자 레이블로 주어졌을 때 교차 엔트로피 오차도 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb95b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0] # y의 원소 개수로 나눠주어야 평균을 구할 수 있으므로 dataset y의 행 개수를 batch_size로 설정한다.\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size # np.arange(argu)는 0부터 argu - 1까지의 배열을 생성.\n",
    "\n",
    "# t는 정답 레이블이므로 y을 인덱싱할 때 t를 열 부분에 넣으면 정답에 해당되는 원소만 추출된다.\n",
    "# batch_size는 앞에서 정의한 것과 같이 10이므로, batch_size에 해당하는 이미지를 추출하도록 설정한다.\n",
    "# 앞에서 t가 원-핫 인코딩일 경우에는 t가 0일 때 아무런 인트로피 오차가 발생하지 않으므로 곱해줘도 됐지만, 원-핫 인코딩이 되지 않을 시에는 t를 이용해 인덱스한다.\n",
    "# 다행히도, t에 해당되지 않은 열은 인트로피가 발생하지 않으므로 t의 레이블에 해당되는 열들만 뽑아줘도 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94794ba",
   "metadata": {},
   "source": [
    "이 구현에서는 **원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다**는 것이 핵심이다.\n",
    "\n",
    "다시 말하면 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있다.\n",
    "\n",
    "그래서 원-핫 인코딩 시 t * np.log(y) 였던 부분을\n",
    "np.log(y[np.arnage(batch_size), t] + 1e-7) / batch_size\n",
    "으로 구현한다.\n",
    "\n",
    "np.arange(batch_size)는 0부터 batch_size - 1까지의 배열을 생성한다.\n",
    "\n",
    "예를 들어 batch_size가 5이면 np.arange(batch_size)는 [0,1,2,3,4]라는 배열을 생성한다.\n",
    "\n",
    "t에는 레이블이 **[2,7,0,9,4]** 와 같이 설정되어 있으므로 위의 함수는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f46099",
   "metadata": {},
   "source": [
    "### 왜 손실 함수를 설정하는가?\n",
    "\n",
    "굳이 손실 함수를 사용하는 이유가 무엇일까?\n",
    "\n",
    "이를 위해서는 신경망 학습에서의 '미분'의 역할에 주목한다면 해결된다.\n",
    "\n",
    "신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다.\n",
    "\n",
    "이 때 **매개변수의 미분(정확히는 기울기)** 을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다.\n",
    "\n",
    "가령 여기에 가상의 신경망이 있고, 그 신경망의 어느 한 가중치 매개변수에 주목한다고 할 때,\n",
    "그 가중치 매개변수의 손실 함수 미분이란 **'가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하냐'** 라는 의미이다.\n",
    "\n",
    "만일 이 미분 값이 음수이면 그 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.\n",
    "\n",
    "그러나 **<font color = red> 만일 이 미분 값이 0이면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않는다.** </font>\n",
    "\n",
    "그래서 가중치 매개변수의 갱신은 거기서 멈춘다.\n",
    "\n",
    "정확도를 지표로 삼아서는 안 되는 이유는 **미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기** 떄문임.\n",
    "\n",
    "<font color = grey> *신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.* </font>\n",
    "\n",
    "예를 들어 한 신경망이 100장의 훈련 데이터 중 32장을 올바르게 인식한다고 하자. 그렇다면 정확도는 32%이다.\n",
    "\n",
    "만약 정확도가 지표였다면 가중치 매개변수의 값을 조금 바꾼다고 해도 **정확도는 그대로 32%일 것** 이다.\n",
    "\n",
    "즉, 매개변수를 약간만 조정해서는 정확도가 개선되지 않고 일정하게 유지되고, 정확도가 개선된다고 하더라도 그 값은 연속적인 변화보다는 불연속적인 띄엄띄엄한 값으로 바뀌어버린다.\n",
    "\n",
    "정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화한다.\n",
    "\n",
    "이는 '계단 함수'를 활성화 함수로 사용하지 않는 이유와도 같다. 활성화 함수로 계단 함수를 사용하면 지금까지 설명한 것과 같은 이유로 신경망 학습이 잘 이루어지지 않는다.\n",
    "\n",
    "계단 함수의 미분은 [매끄럽지 않으므로] 대부분의 장소에서 0이다. (정확히 말하자면 0이 아닌 곳에서 0이다.)\n",
    "\n",
    "그 결과, 계단 함수를 이용하면 손실 함수를 지표로 삼는 게 아무 의미가 없게 된다. 매개변수의 작은 변화가 주는 파장을 계단 함수가 말살하여 손실 함수의 값에는 아무런 변화가 나타나지 않기 때문이다.\n",
    "\n",
    "계단 함수는 한간만 변화를 일으키지만, 시그모이드 함수의 미분은 세로축의 값이 연속적으로 변하고 따라서 곡선의 기울기도 연속적으로 변한다.\n",
    "\n",
    "즉, 시그모이드 함수의 미분은 **어느 장소라도 0이 되지 않는다**.\n",
    "\n",
    "이는 신경망 학습에서 중요한 성질로, **기울기가 0이 되지 않는 덕분에 신경망이 올바르게 학습** 할 수 있는 것이다. 우리의 목표는 결국 데이터에 구애받지 않고 미분값이 0 = 극소량의 변화에도 손실 함수의 유의미한 변화가 없음 이라는 결론을 얻고 싶기 때문이다.\n",
    "\n",
    "## 수치 미분\n",
    "\n",
    "경사법에서는 기울기(경사) 값을 기준으로 나아갈 방향을 정한다.\n",
    "\n",
    "### 미분\n",
    "\n",
    "마라톤 선수가 처음부터 10분에서 2km씩 달렸다고 해보자. 이때의 속도는 간단히 2/10 = 0.2 [km/분]이라고 계산할 수 있다.\n",
    "\n",
    "즉, 1분에 0.2km만큼의 속도(변화)로 뛰었다고 해석할 수 있다.\n",
    "\n",
    "이 마라톤 예에서는 **'달린 거리'가 '시간'에 대해서 얼마나 변화했는가** 를 계산했다. 다만 여기서 10분에 2km를 뛰었다는 것은, 정확하게는 10분 동안의 '평균 속도'를 구한 것이다.\n",
    "\n",
    "미분은 **특정 순간의 변화량**을 뜻한다. 그래서 10분이라는 시간을 가능한 한 줄여(직전 1분에 달린 거리, 직전 1초에 달린 거리, 직전 0.1초에 달린 거리, ... 식으로 갈수로 간격을 줄여) **한 순간의 변화량** 을 얻는 것이다.\n",
    "\n",
    "이처럼 미분은 한순간의 변화량을 표시한 것이다. 수식으로는 다음과 같다.\n",
    "\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h) - f(x)} {h}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ae4b3",
   "metadata": {},
   "source": [
    "위 식은 함수의 미분을 나타낸 식이다.\n",
    "\n",
    "좌변은 $f(x)$의 $x$에 대한 미분($x$에 대한 $f(x)$의 변화량)을 나타내는 기호이다.\n",
    "결국, $x$의 작은 변화가 함수 $f(x)$을 얼마나 변화시키느냐를 의미한다. 이때의 시간의 작은 변화, 즉 시간을 뜻하는 $h$를 한없이 0에 가깝게 한다는 의미를 $\\lim_{h \\to 0}$으로 나타낸다.\n",
    "\n",
    "그렇다면 파이썬으로 이를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e58a3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나쁜 구현 예\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026ae48",
   "metadata": {},
   "source": [
    "이 구현에는 두 가지의 문제가 있다.\n",
    "\n",
    "1. h에 최대한 작은 값을 대입하고 싶었기에 (가능하다면 h를 0으로 무한히 가깝게 하고 싶으니) 1e-50이라는 작은 값을 이용했다. 하지만 이 방식은 **반올림 오차 문제**를 일으킨다. 반올림 오차는 작은 값이 생략되어 최종 계산 결과에 오차가 생기게 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2841130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0fbe40",
   "metadata": {},
   "source": [
    "위와 같이 1e-50으 float32(32비트 부동소수점)으로 나타내면 0.0이 되어, 올바로 표현할 수 없다.\n",
    "    너무 작은 값을 이용하면 컴퓨터로 계산하는 데 문제가 된다는 것이다.\n",
    "    여기가 첫 번째 개선 포인트로, 미세한 값 $h$로 $10^{-4}$을 이용해보자.\n",
    "    이 값을 사용하면 좋은 결과를 얻는다고 알려져 있다.\n",
    "    \n",
    "2. 두 번재 개선은 함수 f의 차분과 관련된 것.\n",
    "    앞의 구현에서는 $x+h$와 $x$ 사이의 함수 f의 차분을 계산하고 있지만, 애당초 이 계산에는 오차가 있다는 사실에 주의해야 한다.\n",
    "    진정한 미분은 $x$ 위치의 함수의 기울기에 해당하지만, 이 구현에서의 미분은 $(x+h)$와 $x$ 사이의 기울기에 해당한다.\n",
    "    \n",
    "    그래서 진정한 미분과 이번 구현의 값은 '엄밀히 말하자면' 일치하지 않는다.\n",
    "    이 차이는 $h$을 무한히 0으로 좁히는 것이 불가능해 생기는 한계이다.\n",
    "    \n",
    "    위와 같은 수치 미분에는 오차가 포함된다. 이 오차를 줄이기 위해 $(x+h)$와 $(x-h)$ 일 때의 함수 $f$의 차분을 계산하는 방법을 쓰기도 한다.\n",
    "    이 차분은 x를 중심으로 그 전후의 차분을 계산한다는 의미에서 **중앙 차분 혹은 중앙 차분** 이라고 한다.\n",
    "    \n",
    "그렇다면 두 개선점을 이용하여 수치 미분을 다시 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "74d1839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 # 0.001\n",
    "    return ((f(x+h) - f(x-h)) / (2*h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803ec4f",
   "metadata": {},
   "source": [
    "<font color = blue> *NOTE. 여기에서 하는 것처럼 아주 작은 차분으롤 미분하는 것을 수치 미분이라 한다. 한 편, 수식을 전개해 미분하는 것은 해석적이라는 말을 이용하여 '해석적 해' 혹은 '해석적으로 미분하다' 등으로 표현한다. 가령 $y=x^2$의 미분은 해석적으로는 $/frac{dy}{dx} = 2x으로 풀어낼 수 있다. 그래서 $x=2$일 때 $y$의 미분은 4가 된다. 해석적 미분은 오차를 포함하지 않는 진정한 미분 값을 구해준다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee420009",
   "metadata": {},
   "source": [
    "### 수치 미분의 예\n",
    "\n",
    "앞 절의 수치 미분을 사용하여 간단한 함수를 미분해보자.\n",
    "\n",
    "$$y = 0.01x^2 + 0.1x $$\n",
    "\n",
    "을 파이썬으로 구현하면 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "24e4a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b45c71",
   "metadata": {},
   "source": [
    "이다.\n",
    "\n",
    "이어서 이 함수를 그려보면 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "94402ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'f(x)')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb2611",
   "metadata": {},
   "source": [
    "그럼 $x=5$일 때와 10일 때 이 함수의 미분을 계산해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c164bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83ccc038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110c7b2",
   "metadata": {},
   "source": [
    "### 편미분\n",
    "\n",
    "$$f(x_0, x_1) = {x_0}^2 + {x_1}^2$$\n",
    "\n",
    "이 식을 보자.\n",
    "\n",
    "변수가 2개라는 점에 주의해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccb6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 # 또는 return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62310c00",
   "metadata": {},
   "source": [
    "이것을 미분하려면 **\"어느 변수에 대한 미분이냐\"** 를 구분해야 한다.\n",
    "\n",
    "이와 같이 변수가 여럿인 함수에 대한 미분을 편미분이라고 한다.\n",
    "\n",
    "이 편미분을 수식으로는 $\\frac{\\partial f}{\\partial x_0}$ 나 $\\frac{\\partial f}{\\partial x_1}$ 이라고 쓴다.\n",
    "\n",
    "그렇다면 간단한 문제를 풀어보자.\n",
    "\n",
    "#### 문제 1. $x_0 = 3, x_1 = 4$일 때, $x_0$에 대한 편미분 $\\frac{\\partial f}{\\partial x_0}$을 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "57f5a83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0 ** 2.0 # x0에 대한 편미분이므로 x1 값을 넣어준다,.\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb79b1",
   "metadata": {},
   "source": [
    "#### 문제 2. $x_0 = 3, x_1 = 4$일 때, $x_0$에 대한 편미분 $\\frac{\\partial f}{\\partial x_1}$을 구하라. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ab0f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0 ** 2.0 + x1 * x1\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10777e91",
   "metadata": {},
   "source": [
    "이 문제들은 변수가 하나인 함수를 정의하고, 그 함수를 미분하는 형태로 구현해서 풀었다.\n",
    "\n",
    "이처럼 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다.\n",
    "\n",
    "단, 여러 변수 중 **목표 변수 하나에 초점을 맞추고** 다른 변수는 값을 고정한다.\n",
    "\n",
    "## 기울기\n",
    "\n",
    "앞 절의 예에서는 $x_0 \\text{와} x_1$의 편 미분을 변수별로 따로 계산했다.\n",
    "\n",
    "그럼 $x_0$과 $x_1$의 편미분을 동시에 계산하고 싶다면 어떻게 할까?\n",
    "\n",
    "가령, $x_0 = 3, x_1 = 4$일 대, $(x_0, x_1)$ 양쪽의 편미분을 묶어서 $\\left (\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1} \\right)$ 처럼 **모든 변수의 편미분을 벡터로 정리한 것을 기울기** 라고 한다.\n",
    "\n",
    "기울기는 다음과 같이 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e607dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같지만 원소가 모두 0인 배열을 생성한다.\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx] # x의 idx번째 인자를 가진 tmp_val\n",
    "    \n",
    "    # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h # tmp_val에 미세한 값(h)을 더해서 x에 집어넣는다.\n",
    "        fxh1 = f(x) # 이 집어넣은 x을 함수에 대입하여 f(x+h)의 값인 fxh1을 얻는다.\n",
    "    \n",
    "    # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h # tmp_val에 미세한 값(h)을 빼서 x에 집어넣는다.\n",
    "        fxh2 = f(x) # 이 집어넣은 x을 함수에 대입하여 f(x-h)의 값인 fxh2를 얻는다.\n",
    "    \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h) # 빈 벡터인 grad에 미분의 정의를 활용하여 미분 값을 넣는다.\n",
    "        x[idx] = tmp_val # 값 복원 -> 다시 루프가 돌 수 있도록 만들어준다.\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff2613",
   "metadata": {},
   "source": [
    "복잡해 보이지만, 동작 방식은 변수가 하나일 때의 수치 미분과 거의 같다.\n",
    "\n",
    "- np.zeros_like(x)는 x와 형상이 같고 원소가 모두 0인 배열이다.\n",
    "\n",
    "- numerical_gradient(f, x) 함수의 f는 함수이고, x는 넘파이 배열이므로 넘파이 배열 x의 각 원소에 대해서 수치 미분을 구한다.\n",
    "\n",
    "그렇다면 이 함수를 이용하여 직접 기울기를 계산해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a6db1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61fc807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8035cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae300f68",
   "metadata": {},
   "source": [
    "이처럼 $(x_0, x_1)$의 각 점에서의 기울기를 계산할 수 있다.\n",
    "\n",
    "[그림 4-9 참고]\n",
    "\n",
    "그림에서 기울기는 가장 낮은 장소를 가리키지만, 실제로는 반드시 그렇다고는 할 수 없음.\n",
    "\n",
    "사실, 기울기는 낮아지는 방향을 가리킨다.\n",
    "\n",
    "더 정확히 말하자면, <font color = red> **기울기가 가리키는 쪽은 함수의 출력 값을 가장 크게 줄이는 방향** </font> 이다.\n",
    "\n",
    "\n",
    "### 경사법(경사 하강법)\n",
    "\n",
    "기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수(가중치와 편향)을 학습 시에 찾아야 한다.\n",
    "\n",
    "여기에서 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값이다. 그러나 일반적인 문제의 손실 함수는 매우 복잡하다.\n",
    "\n",
    "매개변수 공간이 광대해서 어디가 최솟값이 되는 곳인지를 짐작할 수 없기 때문이다.\n",
    "\n",
    "이런 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법이다.\n",
    "\n",
    "여기에서 주의할 점은 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 **기울기** 라는 것이다.\n",
    "\n",
    "그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉, 그쪽이 정말로 나아갈 방향인지는 보장할 수 없다.\n",
    "\n",
    "실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다.\n",
    "\n",
    "\n",
    "<font color = red> *WARNING. 함수가 극솟값, 최솟값, 그리고 안장점이 되는 장소에서는 기울기가 0이다. 극솟값은 국소적인 최솟값, 즉 한정된 범위에서의 최솟값인 점이다. 안장점은 어느 방향에서 보면 극댓값이고, 다른 방향에서 보는 극솟값이 되는 점이다. 경사법은 기울기가 0인 장소를 찾지만 **그것이 반드시 최솟값이라고는 할 수 없다** (극솟값이나 안장점일 가능성이 있다). 또, 복잡하고 찌그러진 모양의 함수라면 평평한 곳으로 파고들면서 고원이라고 하는, 학습이 진행되지 않는 정체기에 빠질 수 있다.* </font>\n",
    "\n",
    "기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, **그 방향으로 가야 함수의 값을 줄일 수 있다**.\n",
    "\n",
    "그래서 최솟값이 되는 장소를 찾는 문제(아니면 가능한 한 작은 값이 되는 장소를 찾는 문제)에서는 기울기 정보를 단서로 **나아갈 방향**을 정해야 한다.\n",
    "\n",
    "경사법은 **현 위치에서 기울어진 방향으로 일정 거리만큼 이동** 한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다. 이렇게 해서 함수의 값을 점차 줄이는 것이 **경사법** 이다.\n",
    "\n",
    "경사법은 기계학습을 최적화하는 데 흔히 쓰는 방법이며, 특히 신경망 학습에서는 경사법을 많이 사용한다.\n",
    "\n",
    "<font color = blue> *Note. 경사법은 최솟값을 찾느냐, 최댓값을 찾느냐에 따라 이름이 다르다. 전자를 경사 하강법, 후자를 경사 상승법이라고 한다. 다만 손실 부호의 부호를 반전시키면 최솟값을 찾는 문제와 최댓값을 찾는 문제는 같은 것이니 하강이냐 상승이냐는 본질적으로 중요하지 않다. 일반적으로 신경망(딥러닝) 분야에서의 경사법은 '경사 하강법'으로 등장할 때가 많다.* </font>\n",
    "\n",
    "그럼, 경사법을 수식으로 나타내보자.\n",
    "\n",
    "\n",
    "$x_0 = x_0 - \\eta \\frac{\\partial f}{\\partial x_0} \\\\\n",
    "x_1 = x_1 - \\eta \\frac{\\partial f}{\\partial x_1}$\n",
    "\n",
    "위 식의 $\\eta$(에타)는 **갱신하는 양**을 나타낸다.\n",
    "\n",
    "이를 신경망 학습에서는 **학습률** 이라고 한다. 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 **매개변수 값을 얼마나 갱신하느냐**를 정하는 것이 학습률이다.\n",
    "\n",
    "위 식은 1회에 해당하는 갱신이고, 이 단계를 반복한다. 즉, **변수의 값을 갱신하는 단계를 여러 번 반복하면서 서서히 함수의 값을 줄이는 것**이다.\n",
    "\n",
    "여기에서는 변수가 2개인 경우를 보여주었지만, 변수의 수가 늘어도 같은 식(각 변수의 편미분 값)으로 갱신하게 된다.\n",
    "\n",
    "또한, 학습률 값은 0.01이나 0.001 등 미리 특정 값으로 정해두어야 한다.\n",
    "\n",
    "일반적으로 이 값이 너무 크거나 작으면 '좋은 장소'를 찾아갈 수 없다.\n",
    "\n",
    "신경망 학습에서는 보통 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행한다.\n",
    "\n",
    "경사 하강법은 다음과 같이 간단하게 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e741795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x # x를 갱신할 것이므로 init_x를 입력값으로 넣고, 여기서 x로 바꿔준다.\n",
    "    \n",
    "    for i in range(step_num): # step_num 동안 동작을 반복한다.\n",
    "        grad = numerical_gradient(f, x) # f는 최적화하려는 함수이므로 손실 함수\n",
    "        x -= lr * grad # x - lr * grad 값으로 조정한다.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11743ce3",
   "metadata": {},
   "source": [
    "<코드 해석>\n",
    "\n",
    "인수 f는 최적하려는 함수\n",
    "\n",
    "init_x는 초기값\n",
    "\n",
    "lr은 learning rate을 의미하는 학습률\n",
    "\n",
    "step_num은 경사법에 따른 반복 횟수를 뜻한다.\n",
    "\n",
    "이 함수를 사용하면 함수의 극솟값을 구할 수 있고, 잘하면 최솟값을 구할 수도 있다.\n",
    "\n",
    "#### 문제 : 경사법으로 $f(x_0, x_1) = {x_0}^2 + {x_1}^2$ 의 최솟값을 구하여라.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bddb02b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.11110793e-10, 8.14814391e-10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([3.0, 4.0])\n",
    "gradient_descent(function_2, init_x, lr = 0.1, step_num = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff00966",
   "metadata": {},
   "source": [
    "여기에서는 초기값을 $(-3.0, 4.0)$으로 설정한 후 경사법을 이용해 최솟값 탐색을 시작한다.\n",
    "\n",
    "최종 결과는 위와 같다. 거의 0과 가까운 결과.\n",
    "\n",
    "실제로 진정한 최솟값은 $(0,0)$으로 거의 정확한 결과를 얻은 것.\n",
    "\n",
    "\n",
    "그렇다면 학습률이 너무 크거나 너무 작으면 어떻게 될까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38f7d12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예 : lr = 10.0\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x, 10.0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a046776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 엄청난 값으로 발산한다.\n",
    "\n",
    "# 학습률이 너무 작은 예 : lr = 1e-10\n",
    "gradient_descent(function_2, init_x, 1e-10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2bde9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 초기ㅏㄱㅂㅅ인 (-3.0, 4,0)과 별 차이가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d11f9",
   "metadata": {},
   "source": [
    "이 실험 결과와 같이 학습률이 너무 크면 **큰 값으로 발산** 해 버린다.\n",
    "\n",
    "반대로 너무 작으면 **거의 갱신되지 않은** 채 끝나버린다.\n",
    "\n",
    "<font color = blue> *NOTE. 학습률 같은 매개변수를 하이퍼파라미터(초매개변수)라고 한다. 이는 가중치나 편향 같은 신경망의 매개변수와는 성질이 다른 매개변수이다. 신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의하여 **자동으로 획득되는 매개변수** 인 반면, 핛브률 같은 하이퍼퍼러미터는 **사람이 직접 설정해야 하는 매개변수** 이다. 일반적으로는 이 하이퍼퍼러미터들은 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 한다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927e244",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기\n",
    "\n",
    "기울기는 <font color = red> **가중치 매개변수에 대한 손실 함수의 기울기**</font> 이다.\n",
    "    예를 들어 형상이 2 x 3, 가중치가 W, 손실 함수가 L인 신경망을 생각해보자.\n",
    "    이 경우 경사는 $\\frac{\\partial L}{\\partial W}$ 으로 나타낼 수 있다.\n",
    "    \n",
    "수식으로는 다음과 같다.\n",
    "\n",
    "$$W = \\begin {pmatrix} \n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end {pmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{13}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\frac{\\partial L}{\\partial w_{23}}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W}$ 의 각 원소는 각각의 원소에 관한 편미분이다.\n",
    "예를 들어 1행 1번째 원소인 $\\frac{\\partial L}{\\partial w_{11}}$은 $w_{11}$을 조금 변경했을 때 **손실 함수 L이 얼마나 변화하느냐** 를 나타낸다. 여기서 중요한 점은 **$\\frac{\\partial L}{\\partial W}$의 형상이 $W$와 같다는 것.**\n",
    "\n",
    "실제로 위 식에서 $W$와 $\\frac{\\partial L}{\\partial W}$의 형상은 모두 2*3이다.\n",
    "\n",
    "그렇다면 간단한 신경망을 예로 들어 실제로 기울기를 구하는 코드를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da53a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.getcwd()\n",
    "os.chdir(\"/Users/choeunsol/deep-learning-from-scratch-master/ch03\")\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화, np.random.randn은 정규분포 float를 array로 생성한다.\n",
    "        ## np.random.randn에 인수가 없을 경우는 단일 인자로 생성한다.\n",
    "        \n",
    "    def predict(self, x): # 예측 함수\n",
    "        return np.dot(x, self.W) # x * W을 구현\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # 예측의 결과\n",
    "        y = softmax(z) # 예측 후 활성화 함수에 넣은 값. 이것으로 t(정답 레이블)과 비교할 것이다.\n",
    "        loss = cross_entropy_error(y, t) # 손실 함수 계산 값\n",
    "        \n",
    "        return loss # loss를 return하여 손실 함수의 값을 보여준다.\n",
    "    \n",
    "    # 클래스를 정의할 때 self에 항상 주의하기.\n",
    "    # 안에서 정의해야 할 변수들은 self를 넣어주기.\n",
    "    # 클래스 내부의 함수를 정의할 때에도 함수 첫 인자에 self를 넣어주자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0d7d4",
   "metadata": {},
   "source": [
    "<코드 해석>\n",
    "\n",
    "1. 여기에서는 common/functions.py에 정의된 softmax 함수와 cross_entropy_error 메서드를 이용한다. 또한, common/gradient.py에 정의한 numerical_gradient 메서드도 이용한다.\n",
    "\n",
    "2. simpleNet 클래스는 형상이 2 * 3인 가중치 매개변수 하나를 인스턴스 변수로 갖는다.\n",
    "\n",
    "3. 메서드는 2개\n",
    "- 하나는 예측을 수행하는 predict(x)\n",
    "- 다른 하나는 손실 함수의 값을 구하는 loss(x, t)\n",
    "    - 여기에서 인수 x는 입력 데이터, t는 정답 레이블이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a69483e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25120191 -0.50856341 -0.2153448 ]\n",
      " [ 1.29597426 -1.70291697 -0.03984895]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W) # 가중치 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a1e92e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.31709798 -1.83776332 -0.16507094]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62ab1c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df275a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23885046895444992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([1,0,0]) # 정답 레이블\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58eac6",
   "metadata": {},
   "source": [
    "이어서 기울기를 구해보자.\n",
    "\n",
    "지금까지처럼 numerical_gradient(f, x)를 써서 구하면 된다.\n",
    "\n",
    "아래에서 정의한 $f(W)$ 함수의 인수 W은 더미로 만든 것이다.\n",
    "\n",
    "numerical_gradient(f,x) 내부에서 $f(x)$을 실행하는데, 그와의 일관성을 위해서 $f(W)$을 정의한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "376c0bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12748046  0.02015027  0.10733019]\n",
      " [-0.1912207   0.0302254   0.16099529]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W) # 매개변수의 변화에 따른 손실함수의 변화를 측정하기 위함이므로 f는 손실함수, W은 가중치를 넣어준다.\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbd914",
   "metadata": {},
   "source": [
    "numerical_gradient(f,x)의 인수 f는 함수, x는 함수 f의 인수이다.\n",
    "\n",
    "여기에서는 **net.W을 인수로 받아 손실 함수를 계산하는 새로운 함수 f를 정의** 했다.\n",
    "\n",
    "그리고 이 새로 정의된 함수를 numerical_gradient(f,x)에 넘긴다.\n",
    "\n",
    "dW은 numerical_gradient(f, net.W)의 결과로, 그 형상은 2 * 3의 2차원 배열이다.\n",
    "\n",
    "dW의 내용을 보자. 예를 들어 $\\frac{\\partial L}{\\partial W} \\text{의} \\frac{\\partial L}{\\partial w_{11}}$은 대략 -0.13이다.\n",
    "\n",
    "이는 $w_{11}$을 $h$만큼 늘리면 손실 함수의 값은 $0.13h$만큼 감소한다는 것이다.\n",
    "\n",
    "따라서 **손실 함수를 줄인다는 관점에서는 $\\frac{\\partial L}{\\partial w_{11}}$은 양의 방향으로 갱신해야 함을 알 수 있음**\n",
    "\n",
    "\n",
    "이 구현에서는 새로운 함수를 정의하는 데 \"def f(x):...\"의 문법을 썼는데,\n",
    "\n",
    "파이썬에서는 간단한 함수라면 **람다(lambda) 기법을 쓰면** 더 편하다. 가령 lambda를 쓰면 다음과 같이 구현할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aae2078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t) # 매개변수 w을 뒤의 함수로 정의한다. 매개변수 w을 사용하여 다음과 같이 정의한다. 정도로 기억하면 될 듯.\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169cab6",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 구현하기\n",
    "\n",
    "### 복습\n",
    "\n",
    "**전체**\n",
    "\n",
    "    신경망에는 적용 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라 한다.\n",
    "    신경망 학습은 다음과 같이 4단계로 수행된다.\n",
    "    \n",
    "**1단계 미니배치**\n",
    "    \n",
    "    훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별된 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.\n",
    "    \n",
    "**2단계 기울기 산출**\n",
    "    \n",
    "    미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "    \n",
    "**3단계 매개변수 갱신**\n",
    "    \n",
    "    가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "    \n",
    "**4단계 반복**\n",
    "\n",
    "    1~3단계를 반복한다.\n",
    "    \n",
    "이것이 신경망 학습이 이뤄지는 순서이다.\n",
    "\n",
    "이는 경사 하강법으로 매개변수를 갱신하는 방법이며, 이때 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법**이라고 한다. \n",
    "\n",
    "### 2층 신경망 클래스 구현하기\n",
    "\n",
    "실제로 손글씨 숫자를 학습하는 신경망을 구현해보자. 여기에서는 2층 신경망(은닉층이 1개인 네트워크)을 대상으로 MNIST 데이터셋을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0bccec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f39ca327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/choeunsol/deep-learning-from-scratch-master/ch03'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd744904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size) # hidden_size 크기의 제로 벡터를 생성한다.\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size) # 위에서와 같이 뒤 층 크기의 제로 벡터를 생성\n",
    "        \n",
    "    # 네트워크 만들기\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y= softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # 손실함수 계산\n",
    "    ## x: 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1) # 행 중에서 가장 높은 원소의 index를 산출한다.\n",
    "        t = np.argmax(t, axis = 1) # t도 마찬가지로 가장 높은 원소[정답]의 인덱스를 산출한다.\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 기울기 계산\n",
    "    ## x : 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311cea3e",
   "metadata": {},
   "source": [
    "TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 갖는다.\n",
    "\n",
    "- params 변수 : 가중치 매개변수가 저장 ex) 첫 번째 층의 가중치 매개변수는 params['W1'] 키에 넘파이 배열로 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92cca86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "net.params['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "039ffda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6433234c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['W2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f75fdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['b2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ca62ec",
   "metadata": {},
   "source": [
    "    이와 같이 params 변수에는 이 신경망에 필요한 매개변수가 모두 저장된다.\n",
    "    params 변수에 저장된 가중치 매개변수가 순방향 처리(예측 처리)에서 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "286b42aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량), 28*28의 그림 데이터이므로 열은 784개\n",
    "t = np.random.rand(100, 10) # 더미 입력 데이터(10장 분량), 10개의 분류이므로 열은 10개\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 기울기 계산\n",
    "\n",
    "grads['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "030005d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "148ccfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['W2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c3cab1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['b2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c72a2c",
   "metadata": {},
   "source": [
    "    이어서 TwoLayerNet의 메서드를 살펴보자.\n",
    "\n",
    "    우선 __init__(self, input_size, hidden_size, output_size) 메서드는 클래스를 초기화한다.\n",
    "    \n",
    "    인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력 층의 뉴런 수이다.\n",
    "    예를 들어 손글씨 숫자 인식에서는 크기가 28 * 28 = 784개이고, 출력이 10개이므로\n",
    "    input_size = 784, output_size = 10이고 hidden_size는 적당한 숫자로 설정하면 된다.\n",
    "    \n",
    "    이 초기화 메서드에서는 가중치 매개변수도 초기화된다.\n",
    "    가중치 매개변수의 초깃값을 무엇으로 설정하냐가 학습의 성공을 좌우하기도 한다. 가중치 매개변수 초기화에 대한 내용은 나중에 살펴볼 것임.\n",
    "    당장은 정규분포를 따르는 난수로, 편향은 0으로 초기화한다.\n",
    "    \n",
    "    predict(self, x)와 accuracy(self, x, t)의 구현은 앞에서의 구현과 같다.\n",
    "    \n",
    "    loss(self, x, t)는 손실 함수의 값을 계산하는 메서드이다.\n",
    "    이 메서드는 predict()의 결과와 정답 레이블을 바탕으로 교차 엔트로피 오차를 구하도록 구현했다.\n",
    "    \n",
    "    남은 numerical_gradient(self, x, t) 메서드는 각 매개변수의 기울기를 계산한다.\n",
    "    수치 미분 방식으로 각 매개변수의 손실 함수에 대한 기울기를 계산한다.\n",
    "    \n",
    "    마지막 gradient(self, x, t)는 다음 장에서 구현할 메서드이다.\n",
    "    이 메서드는 오차역전파법을 사용하여 기울기를 효율적이고 빠르게 계산한다.\n",
    "    \n",
    "<font color = blue> *NOTE. numerical_gradient(self, x, t)는 수치 미분 방식으로 매개변수의 기울기를 계산한다. 다음 장에서는 이 기울기 계산을 고속으로 수행하는 기법인 오차역전파법을 설명한다. 오차역전파법을 사용하면 수치 미분을 쓸 때보다 더욱 빠르게 결과를 얻을 수 있다. 신경망 학습은 시간이 오래 걸리니 시간이 빠른 gradient(self, x, t)를 쓰는 것이 좋다.* </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1e717",
   "metadata": {},
   "source": [
    "### 미니배치 학습 구현하기\n",
    "\n",
    "미니배치 학습을 활용하여 신경망 학습을 구현해보자.\n",
    "\n",
    "미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고(미니배치), 그 미니배치에 대해서 경사법으로 매개변수를 갱신한다.\n",
    "\n",
    "    그러면 TwoLayerNet 클래스와 MNIST 데이터셋을 이용하여 학습을 수행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "train_loss_list = {}\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니 배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 계산\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    ## numerical_gradient는 입력값 x에서 predict 함수를 통해 각 매개변수(W, b)의 기울기 값을 계산한다.\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 성능 개선판!\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c919c",
   "metadata": {},
   "source": [
    "### 시험 데이터로 평가하기\n",
    "\n",
    "손실 함수의 값이란, **정확히는 '훈련 데이터의 미니배치에 대한 손실 함수'의 값** 이다.\n",
    "\n",
    "훈련 데이터의 손실 함수 값이 작아지는 것은 신경망이 잘 학습하고 있단느 방증. 그렇지만 **이 결과만으로는 다른 데이터셋에서도 비슷한 실력을 발휘** 할지는 확실하지 않다.\n",
    "\n",
    "신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해야 한다. 다른 말로 **'오버피팅' 문제가 발생하지 않는지** 확인해야 한다는 것.\n",
    "오버피팅되었다는 것은, 예를 들어 훈련 데이터에 포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻이다.\n",
    "\n",
    "신경망 학습의 원래 목표는 범용적인 능력을 익히는 것.\n",
    "\n",
    "따라서 이를 평가하기 위해 다음 구현에서는 학습 도중 정기적으로 훈련 데이터와 시험 데이터를 대상으로 정확도를 기록한다. 여기에서는 1에폭 별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록할 것이다.\n",
    "\n",
    "<font color = blue> *NOTE. 에폭은 하나의 단위이다. 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당된다. 예컨대 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사하강법을 100회 반복하면 모든 훈련 데이터를 소진한 게 된다. 이 경우 100회가 1에폭이 된다.* </font>\n",
    "\n",
    "평가가 제대로 이뤄질 수 있도록 앞의 구현에서 조금 수정해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147fcf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir(\"/Users/choeunsol/deep-learning-from-scratch-master/ch04\")\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭 당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1) \n",
    "\n",
    "for i in range(iters_num): # iters_num만큼 배치를 반복\n",
    "# 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "# 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 성능 개선판!\n",
    "    \n",
    "# 매개변수 계산\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "# 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "# 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0: # %은 나머지를 뜻하는 연산자. i를 iter_per_epoch으로 나눴을 때의 나머지가 0이면... 이라고 해석하자.\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc : \" + str(train_acc) + \",\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76775f50",
   "metadata": {},
   "source": [
    "<코드 해석>\n",
    "\n",
    "    1. iter_per_epoch은 1에폭 당 반복 횟수이다. 보통 1에폭 당 train_size / batch_size만큼 반복을 하므로 (train_size가 모두 소진 될 때까지) 그 값을 넣어준다.\n",
    "    \n",
    "    2. 1에폭 당 정확도를 계산하는 조건문 if i % iter_per_epoch == 0은 반복 횟수가 1에폭에 도달했을 때 기록하기 위해 넣어준다.\n",
    "    i가 train_size/batch_size에 도달했을 때 나머지가 0이 되므로 이 때를 기록한다!\n",
    "    \n",
    "    3. 정확도를 계산하는 코드는 network.accuracy를 통해 정확도를 넣어준다.\n",
    "    \n",
    "    4. append는 앞에서 만들어놓은 train_acc_list에 변수를 할당해주는 역할.\n",
    "       * append는 dictionary에는 적용할 수 없다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
